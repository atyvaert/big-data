{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c0693c",
   "metadata": {},
   "source": [
    "# Initialize pyspark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "#path = \"/Users/konstantinlazarov/Desktop/Big_Data/PySpark/Week_5/spark\"\n",
    "path = \"/Users/Artur/spark\"\n",
    "findspark.init(path) \n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b1cb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Voor Konstantin \\n# import pyspark\\nimport pyspark\\n# create spark context\\nsc = pyspark.SparkContext()\\n# create spark session \\nspark = pyspark.sql.SparkSession(sc)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Voor Konstantin \n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a6ad9",
   "metadata": {},
   "source": [
    "# Import necessary packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1fed3",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5968b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pytz\n",
    "import emojis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from pandas.tseries.holiday import nearest_workday, \\\n",
    "    AbstractHolidayCalendar, Holiday, \\\n",
    "    USMartinLutherKingJr, USPresidentsDay, GoodFriday, \\\n",
    "    USMemorialDay, USLaborDay, USThanksgivingDay\n",
    "from datetime import date\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b37bb",
   "metadata": {},
   "source": [
    "#### Import the twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03afc613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a25396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46530a1c",
   "metadata": {},
   "source": [
    "# Predict the engagement rate of a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2304e11",
   "metadata": {},
   "source": [
    "## 1. Goal of our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00b73b",
   "metadata": {},
   "source": [
    "In this notebook, we are going to predict the engagement rate of tweets. Further, it will be interesting to see the driving factors behind the engagement rate. This can be valuable information when creating an own social media brand or when you want to increase the reach of your tweets.\n",
    "\n",
    "We start by selecting the interesting variables for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849d5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the interesting variables\n",
    "basetable_engr = df_json.select(F.col('created_at').alias('tweet_created'), \\\n",
    "                                   F.col('entities.symbols').alias('symbols'), \\\n",
    "                                   F.col('display_text_range').alias('text_range'), \\\n",
    "                                   F.col('extended_entities.media.type').alias('media_type'), \\\n",
    "                                   F.col('favorite_count'), \\\n",
    "                                   F.col('full_text'), \\\n",
    "                                   F.col('is_quote_status').alias('quoted'), \\\n",
    "                                   F.col('lang').alias('language'), \\\n",
    "                                   F.col('retweet_count'),\\\n",
    "                                   F.col('user.created_at').alias('user_created'), \\\n",
    "                                   F.col('user.followers_count').alias('user_followers'), \\\n",
    "                                   F.col('user.friends_count').alias('user_following'), \\\n",
    "                                   F.col('user.verified').alias('user_verified'), \\\n",
    "                                   F.col(\"user.screen_name\"), \\\n",
    "                                   F.col('user.statuses_count').alias('nr_tweets_by_user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68aafc",
   "metadata": {},
   "source": [
    "Next, we perform some basic preprocessing steps on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e6a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we  convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "basetable_engr = basetable_engr.withColumn('tweet_created', F.to_utc_timestamp(date_udf(\"tweet_created\"), \"UTC\"))\n",
    "basetable_engr = basetable_engr.withColumn('user_created', F.to_utc_timestamp(date_udf(\"user_created\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772d6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "basetable_engr = basetable_engr.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "basetable_engr = basetable_engr.sort(\"tweet_created\", ascending=False)\n",
    "\n",
    "#removing spam accounts \n",
    "basetable_engr = basetable_engr.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1259f",
   "metadata": {},
   "source": [
    "Before we start the feature engineering, we need to filter our data. As we will use the vader package to determine the sentiment later in this notebook, we will only work with English tweets. As our insights in the data will be primarily for European and American companies and most of our tweets are in English, we do not see this as a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b73aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data on language\n",
    "basetable_engr = basetable_engr.filter(F.col(\"language\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447587fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4699"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing if we can compile the model for a smaller data set \n",
    "basetable_engr = basetable_engr.sample(0.01) #takiong 10 procent of the data \n",
    "basetable_engr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b26ba",
   "metadata": {},
   "source": [
    "# 2. Create the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca291115",
   "metadata": {},
   "source": [
    "AAN TE PASSEN\n",
    "\n",
    "First, we start by defining our dependent variable. The engagement rate has already been discussed in the data exploration section. We will use the same definition in order to create a model to predict the engagement rate. Below, we repeat this definition:\n",
    "\n",
    "Engagement on Twitter is measured by the number of retweets, follows, replies, favorites, and other peopleâ€™s reactions to your tweets, including the clicks on the links and hashtags in those tweets. Your Twitter engagement rate is your engagement figure divided by the number of impressions on the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add engagement rate to the dataframe\n",
    "basetable_engr = basetable_engr.withColumn('eng_rate', ((basetable_engr['favorite_count'] + basetable_engr['retweet_count'])/basetable_engr['user_followers']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c3c97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb924520",
   "metadata": {},
   "source": [
    "Check if the dependent variable has no null values, this could be the case if the user has no followers. In this case, we will set the value of the engagement rate to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075c3acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng_rate\n",
       "0        47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values for the dependent variable\n",
    "df = basetable_engr.select(F.col('eng_rate'))\n",
    "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a98544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca485a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng_rate\n",
       "0         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handle the null values in the dependent variable for the created dataframe\n",
    "df = df.fillna(0)\n",
    "\n",
    "# inspect\n",
    "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6ec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9466d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the dependent variable in the basetable\n",
    "basetable_engr = basetable_engr.fillna({'eng_rate': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4ab37",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88cde2",
   "metadata": {},
   "source": [
    "In order to predict the engagement of a tweet, we will create some additional features in order to improve the performance of our model. For most of these features, we will first creata a function. The following features will be created:\n",
    "\n",
    "    1) number of words\n",
    "    2) number of hashtags\n",
    "    3) number of tags\n",
    "    4) number of emojis\n",
    "    5) get the number of exclamation marks\n",
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "    10) The number of upper case words\n",
    "    11) tweeted quote\n",
    "    12) presence of a symbol\n",
    "    13) The age of the account\n",
    "    14) The number of media elements\n",
    "    15) The media type present\n",
    "    16) The number of text characters in the tweet\n",
    "    17) Indicator if the account is verified\n",
    "\n",
    "These features will be used next to some variables that are already present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675584",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfebde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "671e09af",
   "metadata": {},
   "source": [
    "    1) For the number of words, we can just use the function F.size() and apply it to the tokenized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e58af41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define function to count hashtags\n",
    "def get_hashtags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"#\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419c458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define function to count tags\n",
    "def get_tags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"@\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Define a function to get the number of emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94c3885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Define function to count exclamation marks\n",
    "def get_exclamation_marks(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter ==  \"!\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609aa6b",
   "metadata": {},
   "source": [
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "\n",
    "We saw how to create these variables when solving the questions for this assignment. The same code will be used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e807c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e25698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Define number of upper case words\n",
    "def get_upper_case_words(text):\n",
    "    counter = 0\n",
    "    \n",
    "    ## Tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    ## Check for uppercase words\n",
    "    for word in word_tokens:\n",
    "        if word.isupper():\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37602e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Define a function that indicates if the tweet was a quote\n",
    "def tweeted_quote_indicator(quoted):\n",
    "    quote = 0\n",
    "    if quoted == True:\n",
    "        quote = 1\n",
    "    return quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7987bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) define a function that indicates the presence of a symbol\n",
    "def symbol_indicator(symbols):\n",
    "    symbol = 0\n",
    "    if(symbols > 0):\n",
    "        symbol = 1\n",
    "    return symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea625a6",
   "metadata": {},
   "source": [
    "    13) Define the age of the account. This is defined as the number of days since the account has been created and the last day of scraping (2022-10-11). The last day of scraping was calculated in the exploration phase of the data. For this variable, we will use the function datediff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c208ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) define a function to get help get the number of media types included in the tweet\n",
    "def adjust_nr_media(number):\n",
    "    if number == -1:\n",
    "        number = 0\n",
    "        \n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "650393c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) define a function to get the first media element\n",
    "def get_media_type(media):\n",
    "    if media == None:\n",
    "        media = 'no_media'\n",
    "    else:\n",
    "        media = media[0]\n",
    "       \n",
    "    return media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0210dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) define a function to get the first media element\n",
    "def get_nr_text_characters(text_range):\n",
    "    number = text_range[1] - text_range[0]  \n",
    "    return number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d61bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Look if the user is a verified user\n",
    "def verified_ind(verified):\n",
    "    indicator = 0\n",
    "    if verified == True:\n",
    "        indicator = 1\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "591c78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the functions as an udf\n",
    "get_upper_case_words_UDF = F.udf(get_upper_case_words, IntegerType()) \n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n",
    "get_hashtags_udf = F.udf(get_hashtags, IntegerType())\n",
    "get_tags_udf = F.udf(get_tags, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "tweeted_quote_indicator_UDF = F.udf(tweeted_quote_indicator, IntegerType())\n",
    "symbol_indicator_udf = F.udf(symbol_indicator, IntegerType())\n",
    "adjust_nr_media_udf = F.udf(adjust_nr_media, IntegerType())\n",
    "get_media_type_udf = F.udf(get_media_type, StringType())\n",
    "get_nr_text_characters_udf = F.udf(get_nr_text_characters, IntegerType())\n",
    "verified_ind_udf = F.udf(verified_ind, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ac2b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already define some text cleaning steps in order to define the correct number of words.\n",
    "\n",
    "# define puncutation and stopwords\n",
    "PUNCTUATION = [char for char in punctuation if char not in [\"!\", \"@\", \"#\"]]\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "# define function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    ## Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in PUNCTUATION])\n",
    "    return(text)\n",
    "\n",
    "# define function to remove stopwords\n",
    "def remove_stops(text_tokenized):\n",
    "    # remove stopwords\n",
    "    text_tokenized = [word for word in text_tokenized if word not in STOPWORDS]\n",
    "    return(text_tokenized)\n",
    "\n",
    "# register as udf\n",
    "remove_punct_UDF = F.udf(remove_punct, StringType())\n",
    "remove_stops_UDF = F.udf(remove_stops, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02854ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr.withColumn(\"num_emojis\", emoji_counter_udf(F.col(\"full_text\")))\\\n",
    "                            .withColumn('upper_case_words', get_upper_case_words_UDF('full_text'))\\\n",
    "                            .withColumn(\"text_lower\", F.lower(\"full_text\")) \\\n",
    "                            .withColumn(\"text_cleaned\", remove_punct_UDF(\"text_lower\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"text_cleaned\", \" \")) \\\n",
    "                            .withColumn(\"text_tokenized_no_stops\", remove_stops_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_tags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn('nr_exlcamations', get_exclamation_marks_UDF('text_tokenized_no_stops'))\\\n",
    "                            .withColumn(\"week_day\", F.date_format(F.col(\"tweet_created\"), \"E\"))\\\n",
    "                            .withColumn(\"hour\", F.date_format(F.col(\"tweet_created\"), \"H\").cast('string'))\\\n",
    "                            .withColumn(\"month\", F.date_format(F.col(\"tweet_created\"), \"M\"))\\\n",
    "                            .withColumn(\"day_month\", F.date_format(F.col(\"tweet_created\"), \"d\"))\\\n",
    "                            .withColumn('quoted_ind', tweeted_quote_indicator_UDF('quoted'))\\\n",
    "                            .withColumn('symbol_ind', F.size('symbols'))\\\n",
    "                            .withColumn('symbol_ind', symbol_indicator_udf('symbol_ind'))\\\n",
    "                            .withColumn('user_age_days', F.datediff(F.lit(\"2022-10-11\"), F.col(\"user_created\")))\\\n",
    "                            .withColumn('verified', verified_ind_udf('user_verified'))\\\n",
    "                            .withColumn(\"nr_media_elements\", F.size(\"media_type\"))\\\n",
    "                            .withColumn(\"nr_media_elements\", adjust_nr_media_udf(\"nr_media_elements\"))\\\n",
    "                            .withColumn(\"media_type\", get_media_type_udf('media_type'))\\\n",
    "                            .withColumn(\"nr_text_char\", get_nr_text_characters_udf('text_range'))\\\n",
    "                            .drop('tweet_created')\\\n",
    "                            .drop('quoted')\\\n",
    "                            .drop('symbols')\\\n",
    "                            .drop('user_created')\\\n",
    "                            .drop('user_verified')\\\n",
    "                            .drop('display_text_range')\\\n",
    "                            .drop('text_lower')\\\n",
    "                            .drop('text_cleaned')\\\n",
    "                            .drop('text_tokenized')\\\n",
    "                            .drop('text_tokenized_no_stops')\\\n",
    "                            .drop('text_range')\\\n",
    "                            .filter(\"num_words > 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7733a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f2a7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+---------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|media_type|favorite_count|           full_text|language|retweet_count|user_followers|user_following|    screen_name|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+---------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|  no_media|             1|\"Why do vegans ma...|      en|            0|          5197|          4972|   BlesiRebekah|             5733|1.924187030979411...|         0|               2|       13|           0|           0|              0|     Thu|  15|    5|       12|         0|         0|         1601|       0|                0|         135|\n",
      "|     video|             1|#BrittFit50ðŸŒ± Ann...|      en|            1|          2336|          1303|          brijh|           152769|8.561643835616438E-4|         7|               0|       15|           0|           0|              0|     Tue|  15|    1|       25|         0|         0|         4977|       0|                1|         218|\n",
      "|  no_media|             0|#Clearwater HotBa...|      en|            0|          2199|          2366|NatureFoodPatch|            21027|                 0.0|         0|               0|       21|           0|           0|              0|     Fri|  14|    9|        2|         0|         0|         4152|       0|                0|         152|\n",
      "|  no_media|             2|#FoodShortage #Ve...|      en|            0|           211|            92|lynnmariecunli1|            10741|0.009478672985781991|         0|               0|       28|           0|           0|              0|     Mon|   9|    6|       20|         0|         0|         1575|       0|                0|         271|\n",
      "|     photo|             5|#Grapefruit â€˜Star...|      en|            2|           337|           892|    GospaCitrus|             2867|0.020771513353115726|         1|               0|       23|           0|           0|              0|     Fri|  17|    3|       25|         0|         0|          731|       0|                1|         278|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+---------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95e2cc",
   "metadata": {},
   "source": [
    "# 3. Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bba8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6790c",
   "metadata": {},
   "source": [
    "Now, we are going to clean the text of the twitter data. Then, we can use this cleaned text to extract features about the sensitivity of the tweet.\n",
    "\n",
    "Here, we remove numbers, punctuation, urls... Further, we transform the emojis to words. Emojis are at the very core of communication over social channels. One small image can completely describe one or more human emotions. A naive thing to do during pre-processing would be to remove all emojis. This could result in significant loss of meaning.\n",
    "A good way to achieve this is to replace the emoji with corresponding text explaining the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30919066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emoji.demojize(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5a6dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to udf\n",
    "clean_text_udf = F.udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "137d917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c70e267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basetable_engr_final.select(\"full_text\", \"cleaned_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951b7b",
   "metadata": {},
   "source": [
    "Next, we tokenize the text. Then, we remove stop words. Finally, we use a spelling correction library to correct the spelling of the tweet data. This library from the textblob package is based on the Levenshtein distance. To correct the spelling, we first define a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "081c910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "# define helper function for spelling\n",
    "#correct_spelling_udf = F.udf(lambda tokens: [TextBlob(token).correct() for token in tokens], ArrayType(StringType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd55c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the cleaned_text variable \n",
    "#tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
    "#basetable_engr_final = tokenizer.transform(basetable_engr_final)\n",
    "\n",
    "#remove stop words \n",
    "#remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\")\n",
    "#basetable_engr_final = remover.transform(basetable_engr_final)\n",
    "#basetable_engr_final.select('tokens', 'clean_tokens').show()\n",
    "\n",
    "# correct spelling\n",
    "#basetable_engr_final = basetable_engr_final.withColumn(\"tokens_stemmed\", correct_spelling_udf(\"clean_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edc95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.select('clean_tokens', 'tokens_stemmed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecacd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fa5eb",
   "metadata": {},
   "source": [
    "Now that the text is cleaned, we can derive the sentiment of the text. In this next section, we derive the sentiment, the subjectivity and the polarity. These are the tree final features we add to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21641bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)\n",
    "\n",
    "# define function to get polarity score of text \n",
    "def get_polarity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[0]\n",
    "\n",
    "# define function to get subjectivity score of text \n",
    "def get_subjectivity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[1]\n",
    "\n",
    "\n",
    "# register the functions as udf\n",
    "get_sentiment_udf = F.udf(get_sentiment, DoubleType())\n",
    "get_polarity_udf = F.udf(get_polarity, DoubleType())\n",
    "get_subjectivity_udf = F.udf(get_subjectivity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c764d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"sentiment\", get_sentiment_udf(F.col(\"cleaned_text\")))\\\n",
    "                                .withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "                                .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d1e0f",
   "metadata": {},
   "source": [
    "# 4. Basetable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd4d1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- user_followers: long (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = false)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the basetable:\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a4803",
   "metadata": {},
   "source": [
    "    - Check for missing values. If there are some - handle them (delete, impute,..). In this exercise: write code to handle them even if they aren't present.\n",
    "    - Adjust datatypes where needed\n",
    "    - Remove unnecessary columns\n",
    "    - Add data pre-processing steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be68364",
   "metadata": {},
   "source": [
    "## 4.1 Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "289f4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop (the 3 last variables were used to define the dependent variable)\n",
    "basetable_engr_final = basetable_engr_final.drop('full_text')\\\n",
    "                                .drop('screen_name')\\\n",
    "                                .drop('language')\\\n",
    "                                .drop('cleaned_text')\\\n",
    "                                .drop('retweet_count')\\\n",
    "                                .drop('tokens_stemmed')\\\n",
    "                                .drop('favorite_count')\\\n",
    "                                .drop('user_followers')\n",
    "\n",
    "#.drop('tokens')\\\n",
    "#.drop('clean_tokens')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1da23c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = false)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f169a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|media_type|user_following|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|sentiment|           polarity|      subjectivity|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|     photo|          2497|            15906|0.008379888268156424|         0|               6|       34|           0|           0|              0|     Mon|   9|   11|        1|         1|         0|         2566|       0|                1|         308|    0.237| 0.2397959183673469| 0.573469387755102|\n",
      "|  no_media|          1217|             1986|0.002159827213822...|         0|               0|       18|           0|           0|              0|     Fri|  18|    4|       29|         1|         0|          943|       0|                0|         224|    0.183|0.09999999999999999|0.3666666666666667|\n",
      "|  no_media|          1789|            17254|0.002072538860103627|         0|               0|       28|           0|           0|              0|     Mon|   3|    8|       29|         0|         0|         4630|       0|                0|         171|    0.167|                0.0|               0.0|\n",
      "|  no_media|           328|             3338|0.010526315789473684|         0|               1|       21|           0|           0|              0|     Wed|  10|    7|        6|         0|         0|         1107|       0|                0|         217|    0.089|                0.0|               0.0|\n",
      "|  no_media|           728|             4603|                 0.0|         0|               2|       15|           0|           0|              0|     Thu|  18|    6|       30|         0|         0|         1107|       0|                0|         141|      0.0|                0.0|               0.0|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4f1f8",
   "metadata": {},
   "source": [
    "## 4.2 Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37067394",
   "metadata": {},
   "source": [
    "We see that there are no missing values in our dataset, so this step is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cea47faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values for variable media_type : 0\n",
      "Number of null values for variable user_following : 0\n",
      "Number of null values for variable nr_tweets_by_user : 0\n",
      "Number of null values for variable eng_rate : 0\n",
      "Number of null values for variable num_emojis : 0\n",
      "Number of null values for variable upper_case_words : 0\n",
      "Number of null values for variable num_words : 0\n",
      "Number of null values for variable num_hashtags : 0\n",
      "Number of null values for variable num_mentions : 0\n",
      "Number of null values for variable nr_exlcamations : 0\n",
      "Number of null values for variable week_day : 0\n",
      "Number of null values for variable hour : 0\n",
      "Number of null values for variable month : 0\n",
      "Number of null values for variable day_month : 0\n",
      "Number of null values for variable quoted_ind : 0\n",
      "Number of null values for variable symbol_ind : 0\n",
      "Number of null values for variable user_age_days : 0\n",
      "Number of null values for variable verified : 0\n",
      "Number of null values for variable nr_media_elements : 0\n",
      "Number of null values for variable nr_text_char : 0\n",
      "Number of null values for variable sentiment : 0\n",
      "Number of null values for variable polarity : 0\n",
      "Number of null values for variable subjectivity : 0\n"
     ]
    }
   ],
   "source": [
    "# check number of missing values per column\n",
    "for col in basetable_engr_final.columns:\n",
    "    \n",
    "    # look at the perecentage of null values\n",
    "    print(\"Number of null values for variable\", col,\":\", basetable_engr_final.filter(F.col(col).isNull()).count())\n",
    "\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cd53b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_type</th>\n",
       "      <th>user_following</th>\n",
       "      <th>nr_tweets_by_user</th>\n",
       "      <th>eng_rate</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>upper_case_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>nr_exlcamations</th>\n",
       "      <th>...</th>\n",
       "      <th>day_month</th>\n",
       "      <th>quoted_ind</th>\n",
       "      <th>symbol_ind</th>\n",
       "      <th>user_age_days</th>\n",
       "      <th>verified</th>\n",
       "      <th>nr_media_elements</th>\n",
       "      <th>nr_text_char</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   media_type  user_following  nr_tweets_by_user  eng_rate  num_emojis  \\\n",
       "0           0               0                  0         0           0   \n",
       "\n",
       "   upper_case_words  num_words  num_hashtags  num_mentions  nr_exlcamations  \\\n",
       "0                 0          0             0             0                0   \n",
       "\n",
       "   ...  day_month  quoted_ind  symbol_ind  user_age_days  verified  \\\n",
       "0  ...          0           0           0              0         0   \n",
       "\n",
       "   nr_media_elements  nr_text_char  sentiment  polarity  subjectivity  \n",
       "0                  0             0          0         0             0  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "basetable_engr_final.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in basetable_engr_final.columns]).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616eef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37062864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8006276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c7c45c",
   "metadata": {},
   "source": [
    "Further, we see that our variables all have the correct datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0557",
   "metadata": {},
   "source": [
    "# 5. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be551e3f",
   "metadata": {},
   "source": [
    "In this part, we are going to train a random forest model on the training data. This is an advanced machine learning model of which we expect good performance. After we train this model, we will evaluate it. As the goal of our analysis was to find the driving factors behind the engagement rate, we will use the shap values to determine variable importance. This way, we know in which direction the variable affects the engagement rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f313007",
   "metadata": {},
   "source": [
    "## 5.1 Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450db7",
   "metadata": {},
   "source": [
    "**Import required transformers and estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a18c486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark ml packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, StandardScaler, Word2Vec\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "\n",
    "# import models and evaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f28f38",
   "metadata": {},
   "source": [
    "**Create a vector for each type of features (numeric, categorical)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60accc3d",
   "metadata": {},
   "source": [
    "#### handle numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "676921ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the numeric variables\n",
    "num_cols = ['user_following', 'nr_tweets_by_user', 'num_emojis', 'upper_case_words',\n",
    "           'num_words', 'num_hashtags', 'num_mentions', 'nr_exlcamations',  'user_age_days', \n",
    "           'nr_media_elements', 'nr_text_char']\n",
    "\n",
    "# define the assembler\n",
    "num_vec_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a33263",
   "metadata": {},
   "source": [
    "#### handle categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe911a",
   "metadata": {},
   "source": [
    "First, we transform the text variables in our dataset into numerical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dc251be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables\n",
    "cat_cols = ['media_type', 'hour', 'week_day', 'month', 'day_month', 'quoted_ind',\n",
    "           'symbol_ind', 'verified']\n",
    "\n",
    "# create an object of StringIndexer class for each categorical variable\n",
    "SI_media = StringIndexer(inputCol= 'media_type', outputCol= 'media_type_index').setHandleInvalid(\"skip\")\n",
    "SI_hour = StringIndexer(inputCol= 'hour', outputCol= 'hour_index').setHandleInvalid(\"skip\")\n",
    "SI_week_day = StringIndexer(inputCol= 'week_day', outputCol= 'week_day_index').setHandleInvalid(\"skip\")\n",
    "SI_month = StringIndexer(inputCol= 'month', outputCol= 'month_index').setHandleInvalid(\"skip\")\n",
    "SI_day_month = StringIndexer(inputCol= 'day_month', outputCol= 'day_month_index').setHandleInvalid(\"skip\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39b48d",
   "metadata": {},
   "source": [
    "Next, we use a VectorAssembler to assemble all indexed variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5429a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the assembler\n",
    "cat_cols_indexed = ['media_type_index', 'hour_index', 'week_day_index', 'month_index', \n",
    "                    'day_month_index','quoted_ind', 'symbol_ind', 'verified']\n",
    "cat_vec_assembler = VectorAssembler(inputCols= cat_cols_indexed, outputCol=\"cat_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a188e",
   "metadata": {},
   "source": [
    "Finally ,we use the VectorIndexer that helps index categorical features in datasets of vectors. This is required for tree based methods, which we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79e5f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexer\n",
    "indexer = VectorIndexer(inputCol= \"cat_features\", outputCol=\"cat_features_indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a2688",
   "metadata": {},
   "source": [
    "**Create final feature vector containing the indexed categorical features and numeric features**\n",
    "\n",
    "Most ML algorithms can only handle one vector as input, so we add all preprocessed columns (here two vectors) into one vector, using the ``VectorAssembler`` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "696e6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector assembler\n",
    "VA_all = VectorAssembler(inputCols=[\"num_features\", \"cat_features_indexed\", \"polarity\", \"subjectivity\", \"sentiment\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7d00f",
   "metadata": {},
   "source": [
    "**Define, fit and apply Pipeline on data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a44a52",
   "metadata": {},
   "source": [
    "Now, we create preprocessed data that we will use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b865e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [num_vec_assembler, SI_media, SI_hour, SI_week_day, SI_month, SI_day_month,\n",
    "          cat_vec_assembler, indexer, VA_all]\n",
    "\n",
    "# define pipeline model and fit on data\n",
    "pipeline_model_preprocess = Pipeline().setStages(stages).fit(basetable_engr_final)\n",
    "\n",
    "# transform data by applying pipeline model on data\n",
    "preprocessed_data = pipeline_model_preprocess.transform(basetable_engr_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a3065",
   "metadata": {},
   "source": [
    "**Select features and label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a864c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "preprocessed_data = preprocessed_data.select([\"features\", \"eng_rate\"])\n",
    "#preprocessed_data = preprocessed_data.select([\"num_features\", \"eng_rate\"])\n",
    "\n",
    "# rename engagement rate to label\n",
    "preprocessed_data = preprocessed_data.withColumnRenamed(\"eng_rate\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9be8c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|               label|\n",
      "+--------------------+--------------------+\n",
      "|(22,[0,1,3,4,8,10...|1.924187030979411...|\n",
      "|[1303.0,152769.0,...|8.561643835616438E-4|\n",
      "|(22,[0,1,4,8,10,1...|                 0.0|\n",
      "|(22,[0,1,4,8,10,1...|0.009478672985781991|\n",
      "|(22,[0,1,2,4,8,9,...|0.020771513353115726|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the data\n",
    "preprocessed_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d8370",
   "metadata": {},
   "source": [
    "## 5.2 Train random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df96fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = preprocessed_data.randomSplit([0.7, 0.3], seed= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42c5b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of observations in both datasets\n",
    "#print(\"Number of observations in the training set: %s \" % train.count())\n",
    "#print(\"Number of observations in the test set: %s \" %test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "336b3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rf model\n",
    "RF = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", numTrees=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4af5d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training set\n",
    "rf_model = RF.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82b4d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "rf_preds = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6483bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluator object\n",
    "rfEvaluator = RegressionEvaluator(labelCol = 'label', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "148dc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get different metrics using your created evaluator object\n",
    "rfsq = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'r2'})\n",
    "rfmae = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'mae'})\n",
    "rfrmse = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'rmse'})\n",
    "rfmse = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb9f658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2  : -0.349711\n",
      "MAE  : 0.0594825\n",
      "RMSE : 0.17006\n",
      "MSE  : 0.0352987\n"
     ]
    }
   ],
   "source": [
    "# inspect evaluation metrics\n",
    "print('R^2  : %g' % rfsq)\n",
    "print('MAE  : %g' % rfmae)\n",
    "print('RMSE : %g' % rfrmse)\n",
    "print('MSE  : %g' % rfmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550228c",
   "metadata": {},
   "source": [
    "#### Use cross validation to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7f5eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter space\n",
    "rfparamGrid = (ParamGridBuilder().addGrid(RF.maxDepth, [3, 7, 10])\n",
    "                                   .addGrid(RF.maxBins, [15, 25, 40])\n",
    "                                   .addGrid(RF.numTrees, [5, 25, 60])\n",
    "                                   .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb801858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross validation\n",
    "rfcv_model = CrossValidator(estimator=RF, #random forest model we created before\n",
    "                          estimatorParamMaps=rfparamGrid, \n",
    "                          evaluator=rfEvaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cross validation on trainig set\n",
    "rfcv_model = rfcv_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect best params\n",
    "print(\"best max depth: %s\" %rfcv_model.bestModel._java_obj.getMaxDepth())\n",
    "print(\"best max bins: %s\" %rfcv_model.bestModel._java_obj.getMaxBins())\n",
    "print(\"best num trees: %s\" %rfcv_model.bestModel._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of best model of on test set (cv_model automatically uses best model)\n",
    "rfcv_preds = rfcv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "rfcv_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca17e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ee57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"RANDOM FOREST WITHOUT CV:\")\n",
    "print('  R^2  : %g' % rfsq)\n",
    "print('  MAE  : %g' % rfmae)\n",
    "print('  RMSE : %g' % rfrmse)\n",
    "print('  MSE  : %g' % rfmse)\n",
    "print(\"------------------\")\n",
    "print(\"RANDOM FOREST WITH CV:\")\n",
    "print('  R^2  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'r2'}))\n",
    "print('  MAE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mae'}))\n",
    "print('  RMSE : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'rmse'}))\n",
    "print('  MSE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mse'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189e975",
   "metadata": {},
   "source": [
    "# 6. Interpretation Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee61ec",
   "metadata": {},
   "source": [
    "As we want use the random forest model, we would also like some insights into the drivers behind the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97aa28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (0.40.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: numpy in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (1.23.5)\n",
      "Requirement already satisfied: scipy in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (1.9.3)\n",
      "Requirement already satisfied: slicer==0.0.7 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (0.53.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (0.24.1)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (4.59.0)\n",
      "Requirement already satisfied: packaging>20.9 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (21.3)\n",
      "Requirement already satisfied: pandas in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from shap) (1.5.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from packaging>20.9->shap) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from numba->shap) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from numba->shap) (65.6.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from pandas->shap) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->shap) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->shap) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Artur/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: RandomForestRegressionModel: uid=RandomForestRegressor_7244211c5cfe, numTrees=500, numFeatures=22",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-c732ad5bc558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# explain the model's predictions using SHAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# if we get here then we don't know how to handle what was given to us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# build the right subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: RandomForestRegressionModel: uid=RandomForestRegressor_7244211c5cfe, numTrees=500, numFeatures=22"
     ]
    }
   ],
   "source": [
    "!pip3 install shap\n",
    "#!pip3 install numpy\n",
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values,  X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0c040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31414fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44474fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70addd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d80d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
