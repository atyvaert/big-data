{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c0693c",
   "metadata": {},
   "source": [
    "# Initialize pyspark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "#path = \"/Users/konstantinlazarov/Desktop/Big_Data/PySpark/Week_5/spark\"\n",
    "path = \"/Users/Artur/spark\"\n",
    "findspark.init(path) \n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b1cb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Voor Konstantin \\n# import pyspark\\nimport pyspark\\n# create spark context\\nsc = pyspark.SparkContext()\\n# create spark session \\nspark = pyspark.sql.SparkSession(sc)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Voor Konstantin \n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a6ad9",
   "metadata": {},
   "source": [
    "# Import necessary packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1fed3",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5968b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pytz\n",
    "import emojis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from pandas.tseries.holiday import nearest_workday, \\\n",
    "    AbstractHolidayCalendar, Holiday, \\\n",
    "    USMartinLutherKingJr, USPresidentsDay, GoodFriday, \\\n",
    "    USMemorialDay, USLaborDay, USThanksgivingDay\n",
    "from datetime import date\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b37bb",
   "metadata": {},
   "source": [
    "#### Import the twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03afc613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a25396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46530a1c",
   "metadata": {},
   "source": [
    "# Predict the engagement rate of a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2304e11",
   "metadata": {},
   "source": [
    "## 1. Goal of our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00b73b",
   "metadata": {},
   "source": [
    "In this notebook, we are going to predict the engagement rate of tweets. Further, it will be interesting to see the driving factors behind the engagement rate. This can be valuable information when creating an own social media brand or when you want to increase the reach of your tweets.\n",
    "\n",
    "We start by selecting the interesting variables for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849d5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the interesting variables\n",
    "basetable_engr = df_json.select(F.col('created_at').alias('tweet_created'), \\\n",
    "                                   F.col('entities.symbols').alias('symbols'), \\\n",
    "                                   F.col('display_text_range').alias('text_range'), \\\n",
    "                                   F.col('extended_entities.media.type').alias('media_type'), \\\n",
    "                                   F.col('favorite_count'), \\\n",
    "                                   F.col('full_text'), \\\n",
    "                                   F.col('is_quote_status').alias('quoted'), \\\n",
    "                                   F.col('lang').alias('language'), \\\n",
    "                                   F.col('retweet_count'),\\\n",
    "                                   F.col('user.created_at').alias('user_created'), \\\n",
    "                                   F.col('user.followers_count').alias('user_followers'), \\\n",
    "                                   F.col('user.friends_count').alias('user_following'), \\\n",
    "                                   F.col('user.verified').alias('user_verified'), \\\n",
    "                                   F.col(\"user.screen_name\"), \\\n",
    "                                   F.col('user.statuses_count').alias('nr_tweets_by_user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68aafc",
   "metadata": {},
   "source": [
    "Next, we perform some basic preprocessing steps on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e6a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we  convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "basetable_engr = basetable_engr.withColumn('tweet_created', F.to_utc_timestamp(date_udf(\"tweet_created\"), \"UTC\"))\n",
    "basetable_engr = basetable_engr.withColumn('user_created', F.to_utc_timestamp(date_udf(\"user_created\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772d6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "basetable_engr = basetable_engr.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "basetable_engr = basetable_engr.sort(\"tweet_created\", ascending=False)\n",
    "\n",
    "#removing spam accounts \n",
    "basetable_engr = basetable_engr.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1259f",
   "metadata": {},
   "source": [
    "Before we start the feature engineering, we need to filter our data. As we will use the vader package to determine the sentiment later in this notebook, we will only work with English tweets. As our insights in the data will be primarily for European and American companies and most of our tweets are in English, we do not see this as a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b73aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data on language\n",
    "basetable_engr = basetable_engr.filter(F.col(\"language\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447587fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4731"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing if we can compile the model for a smaller data set \n",
    "basetable_engr = basetable_engr.sample(0.01) #taking 1 procent of the data \n",
    "basetable_engr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b26ba",
   "metadata": {},
   "source": [
    "# 2. Create the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca291115",
   "metadata": {},
   "source": [
    "AAN TE PASSEN\n",
    "\n",
    "First, we start by defining our dependent variable. The engagement rate has already been discussed in the data exploration section. We will use the same definition in order to create a model to predict the engagement rate. Below, we repeat this definition:\n",
    "\n",
    "Engagement on Twitter is measured by the number of retweets, follows, replies, favorites, and other people’s reactions to your tweets, including the clicks on the links and hashtags in those tweets. Your Twitter engagement rate is your engagement figure divided by the number of impressions on the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add engagement rate to the dataframe\n",
    "basetable_engr = basetable_engr.withColumn('eng_rate', ((basetable_engr['favorite_count'] + basetable_engr['retweet_count'])/basetable_engr['user_followers']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c3c97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc68c022",
   "metadata": {},
   "source": [
    "Check if the dependent variable has no null values, this could be the case if the user has no followers. In this case, we will set the value of the engagement rate to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac74dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng_rate\n",
       "0        55"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values for the dependent variable\n",
    "df = basetable_engr.select(F.col('eng_rate'))\n",
    "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62d647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6736b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng_rate\n",
       "0         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handle the null values in the dependent variable for the created dataframe\n",
    "df = df.fillna(0)\n",
    "\n",
    "# inspect\n",
    "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1726f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f5d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the dependent variable in the basetable\n",
    "basetable_engr = basetable_engr.fillna({'eng_rate': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4ab37",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88cde2",
   "metadata": {},
   "source": [
    "In order to predict the engagement of a tweet, we will create some additional features in order to improve the performance of our model. For most of these features, we will first creata a function. The following features will be created:\n",
    "\n",
    "    1) number of words\n",
    "    2) number of hashtags\n",
    "    3) number of tags\n",
    "    4) number of emojis\n",
    "    5) get the number of exclamation marks\n",
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "    10) The number of upper case words\n",
    "    11) tweeted quote\n",
    "    12) presence of a symbol\n",
    "    13) The age of the account\n",
    "    14) The number of media elements\n",
    "    15) The media type present\n",
    "    16) The number of text characters in the tweet\n",
    "    17) Indicator if the account is verified\n",
    "\n",
    "These features will be used next to some variables that are already present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675584",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfebde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "671e09af",
   "metadata": {},
   "source": [
    "    1) For the number of words, we can just use the function F.size() and apply it to the tokenized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e58af41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define function to count hashtags\n",
    "def get_hashtags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"#\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419c458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define function to count tags\n",
    "def get_tags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"@\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Define a function to get the number of emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94c3885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Define function to count exclamation marks\n",
    "def get_exclamation_marks(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter ==  \"!\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609aa6b",
   "metadata": {},
   "source": [
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "\n",
    "We saw how to create these variables when solving the questions for this assignment. The same code will be used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e807c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e25698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Define number of upper case words\n",
    "def get_upper_case_words(text):\n",
    "    counter = 0\n",
    "    \n",
    "    ## Tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    ## Check for uppercase words\n",
    "    for word in word_tokens:\n",
    "        if word.isupper():\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37602e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Define a function that indicates if the tweet was a quote\n",
    "def tweeted_quote_indicator(quoted):\n",
    "    quote = 0\n",
    "    if quoted == True:\n",
    "        quote = 1\n",
    "    return quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7987bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) define a function that indicates the presence of a symbol\n",
    "def symbol_indicator(symbols):\n",
    "    symbol = 0\n",
    "    if(symbols > 0):\n",
    "        symbol = 1\n",
    "    return symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea625a6",
   "metadata": {},
   "source": [
    "    13) Define the age of the account. This is defined as the number of days since the account has been created and the last day of scraping (2022-10-11). The last day of scraping was calculated in the exploration phase of the data. For this variable, we will use the function datediff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c208ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) define a function to get help get the number of media types included in the tweet\n",
    "def adjust_nr_media(number):\n",
    "    if number == -1:\n",
    "        number = 0\n",
    "        \n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "650393c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) define a function to get the first media element\n",
    "def get_media_type(media):\n",
    "    if media == None:\n",
    "        media = 'no_media'\n",
    "    else:\n",
    "        media = media[0]\n",
    "       \n",
    "    return media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0210dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) define a function to get the first media element\n",
    "def get_nr_text_characters(text_range):\n",
    "    number = text_range[1] - text_range[0]  \n",
    "    return number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d61bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Look if the user is a verified user\n",
    "def verified_ind(verified):\n",
    "    indicator = 0\n",
    "    if verified == True:\n",
    "        indicator = 1\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "591c78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the functions as an udf\n",
    "get_upper_case_words_UDF = F.udf(get_upper_case_words, IntegerType()) \n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n",
    "get_hashtags_udf = F.udf(get_hashtags, IntegerType())\n",
    "get_tags_udf = F.udf(get_tags, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "tweeted_quote_indicator_UDF = F.udf(tweeted_quote_indicator, IntegerType())\n",
    "symbol_indicator_udf = F.udf(symbol_indicator, IntegerType())\n",
    "adjust_nr_media_udf = F.udf(adjust_nr_media, IntegerType())\n",
    "get_media_type_udf = F.udf(get_media_type, StringType())\n",
    "get_nr_text_characters_udf = F.udf(get_nr_text_characters, IntegerType())\n",
    "verified_ind_udf = F.udf(verified_ind, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ac2b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already define some text cleaning steps in order to define the correct number of words.\n",
    "\n",
    "# define puncutation and stopwords\n",
    "PUNCTUATION = [char for char in punctuation if char not in [\"!\", \"@\", \"#\"]]\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "# define function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    ## Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in PUNCTUATION])\n",
    "    return(text)\n",
    "\n",
    "# define function to remove stopwords\n",
    "def remove_stops(text_tokenized):\n",
    "    # remove stopwords\n",
    "    text_tokenized = [word for word in text_tokenized if word not in STOPWORDS]\n",
    "    return(text_tokenized)\n",
    "\n",
    "# register as udf\n",
    "remove_punct_UDF = F.udf(remove_punct, StringType())\n",
    "remove_stops_UDF = F.udf(remove_stops, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02854ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr.withColumn(\"num_emojis\", emoji_counter_udf(F.col(\"full_text\")))\\\n",
    "                            .withColumn('upper_case_words', get_upper_case_words_UDF('full_text'))\\\n",
    "                            .withColumn(\"text_lower\", F.lower(\"full_text\")) \\\n",
    "                            .withColumn(\"text_cleaned\", remove_punct_UDF(\"text_lower\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"text_cleaned\", \" \")) \\\n",
    "                            .withColumn(\"text_tokenized_no_stops\", remove_stops_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_tags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn('nr_exlcamations', get_exclamation_marks_UDF('text_tokenized_no_stops'))\\\n",
    "                            .withColumn(\"week_day\", F.date_format(F.col(\"tweet_created\"), \"E\"))\\\n",
    "                            .withColumn(\"hour\", F.date_format(F.col(\"tweet_created\"), \"H\").cast('string'))\\\n",
    "                            .withColumn(\"month\", F.date_format(F.col(\"tweet_created\"), \"M\"))\\\n",
    "                            .withColumn(\"day_month\", F.date_format(F.col(\"tweet_created\"), \"d\"))\\\n",
    "                            .withColumn('quoted_ind', tweeted_quote_indicator_UDF('quoted'))\\\n",
    "                            .withColumn('symbol_ind', F.size('symbols'))\\\n",
    "                            .withColumn('symbol_ind', symbol_indicator_udf('symbol_ind'))\\\n",
    "                            .withColumn('user_age_days', F.datediff(F.lit(\"2022-10-11\"), F.col(\"user_created\")))\\\n",
    "                            .withColumn('verified', verified_ind_udf('user_verified'))\\\n",
    "                            .withColumn(\"nr_media_elements\", F.size(\"media_type\"))\\\n",
    "                            .withColumn(\"nr_media_elements\", adjust_nr_media_udf(\"nr_media_elements\"))\\\n",
    "                            .withColumn(\"media_type\", get_media_type_udf('media_type'))\\\n",
    "                            .withColumn(\"nr_text_char\", get_nr_text_characters_udf('text_range'))\\\n",
    "                            .drop('tweet_created')\\\n",
    "                            .drop('quoted')\\\n",
    "                            .drop('symbols')\\\n",
    "                            .drop('user_created')\\\n",
    "                            .drop('user_verified')\\\n",
    "                            .drop('display_text_range')\\\n",
    "                            .drop('text_lower')\\\n",
    "                            .drop('text_cleaned')\\\n",
    "                            .drop('text_tokenized')\\\n",
    "                            .drop('text_tokenized_no_stops')\\\n",
    "                            .drop('text_range')\\\n",
    "                            .filter(\"num_words > 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7733a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f2a7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+---------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|media_type|favorite_count|           full_text|language|retweet_count|user_followers|user_following|    screen_name|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+---------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|  no_media|             1|\"Why do vegans ma...|      en|            0|          5197|          4972|   BlesiRebekah|             5733|1.924187030979411...|         0|               2|       13|           0|           0|              0|     Thu|  15|    5|       12|         0|         0|         1601|       0|                0|         135|\n",
      "|     video|             1|#BrittFit50🌱 Ann...|      en|            1|          2336|          1303|          brijh|           152769|8.561643835616438E-4|         7|               0|       15|           0|           0|              0|     Tue|  15|    1|       25|         0|         0|         4977|       0|                1|         218|\n",
      "|  no_media|             0|#Clearwater HotBa...|      en|            0|          2199|          2366|NatureFoodPatch|            21027|                 0.0|         0|               0|       21|           0|           0|              0|     Fri|  14|    9|        2|         0|         0|         4152|       0|                0|         152|\n",
      "|  no_media|             2|#FoodShortage #Ve...|      en|            0|           211|            92|lynnmariecunli1|            10741|0.009478672985781991|         0|               0|       28|           0|           0|              0|     Mon|   9|    6|       20|         0|         0|         1575|       0|                0|         271|\n",
      "|     photo|             5|#Grapefruit ‘Star...|      en|            2|           337|           892|    GospaCitrus|             2867|0.020771513353115726|         1|               0|       23|           0|           0|              0|     Fri|  17|    3|       25|         0|         0|          731|       0|                1|         278|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+---------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95e2cc",
   "metadata": {},
   "source": [
    "# 3. Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bba8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6790c",
   "metadata": {},
   "source": [
    "Now, we are going to clean the text of the twitter data. Then, we can use this cleaned text to extract features about the sensitivity of the tweet.\n",
    "\n",
    "Here, we remove numbers, punctuation, urls... Further, we transform the emojis to words. Emojis are at the very core of communication over social channels. One small image can completely describe one or more human emotions. A naive thing to do during pre-processing would be to remove all emojis. This could result in significant loss of meaning.\n",
    "A good way to achieve this is to replace the emoji with corresponding text explaining the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30919066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emoji.demojize(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5a6dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to udf\n",
    "clean_text_udf = F.udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "137d917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c70e267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basetable_engr_final.select(\"full_text\", \"cleaned_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951b7b",
   "metadata": {},
   "source": [
    "Next, we tokenize the text. Then, we remove stop words. Finally, we use a spelling correction library to correct the spelling of the tweet data. This library from the textblob package is based on the Levenshtein distance. To correct the spelling, we first define a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "081c910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "# define helper function for spelling\n",
    "#correct_spelling_udf = F.udf(lambda tokens: [TextBlob(token).correct() for token in tokens], ArrayType(StringType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd55c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the cleaned_text variable \n",
    "#tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
    "#basetable_engr_final = tokenizer.transform(basetable_engr_final)\n",
    "\n",
    "#remove stop words \n",
    "#remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\")\n",
    "#basetable_engr_final = remover.transform(basetable_engr_final)\n",
    "#basetable_engr_final.select('tokens', 'clean_tokens').show()\n",
    "\n",
    "# correct spelling\n",
    "#basetable_engr_final = basetable_engr_final.withColumn(\"tokens_stemmed\", correct_spelling_udf(\"clean_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edc95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.select('clean_tokens', 'tokens_stemmed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecacd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fa5eb",
   "metadata": {},
   "source": [
    "Now that the text is cleaned, we can derive the sentiment of the text. In this next section, we derive the sentiment, the subjectivity and the polarity. These are the tree final features we add to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21641bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)\n",
    "\n",
    "# define function to get polarity score of text \n",
    "def get_polarity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[0]\n",
    "\n",
    "# define function to get subjectivity score of text \n",
    "def get_subjectivity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[1]\n",
    "\n",
    "\n",
    "# register the functions as udf\n",
    "get_sentiment_udf = F.udf(get_sentiment, DoubleType())\n",
    "get_polarity_udf = F.udf(get_polarity, DoubleType())\n",
    "get_subjectivity_udf = F.udf(get_subjectivity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c764d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"sentiment\", get_sentiment_udf(F.col(\"cleaned_text\")))\\\n",
    "                                .withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "                                .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d1e0f",
   "metadata": {},
   "source": [
    "# 4. Basetable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd4d1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- user_followers: long (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = false)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the basetable:\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a4803",
   "metadata": {},
   "source": [
    "    - Check for missing values. If there are some - handle them (delete, impute,..). In this exercise: write code to handle them even if they aren't present.\n",
    "    - Adjust datatypes where needed\n",
    "    - Remove unnecessary columns\n",
    "    - Add data pre-processing steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be68364",
   "metadata": {},
   "source": [
    "## 4.1 Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "289f4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop (the 3 last variables were used to define the dependent variable)\n",
    "basetable_engr_final = basetable_engr_final.drop('full_text')\\\n",
    "                                .drop('screen_name')\\\n",
    "                                .drop('language')\\\n",
    "                                .drop('cleaned_text')\\\n",
    "                                .drop('retweet_count')\\\n",
    "                                .drop('tokens_stemmed')\\\n",
    "                                .drop('favorite_count')\\\n",
    "                                .drop('user_followers')\n",
    "\n",
    "#.drop('tokens')\\\n",
    "#.drop('clean_tokens')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1da23c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = false)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f169a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|media_type|user_following|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|sentiment|           polarity|      subjectivity|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|     photo|          2497|            15906|0.008379888268156424|         0|               6|       34|           0|           0|              0|     Mon|   9|   11|        1|         1|         0|         2566|       0|                1|         308|    0.237| 0.2397959183673469| 0.573469387755102|\n",
      "|  no_media|          1217|             1986|0.002159827213822...|         0|               0|       18|           0|           0|              0|     Fri|  18|    4|       29|         1|         0|          943|       0|                0|         224|    0.183|0.09999999999999999|0.3666666666666667|\n",
      "|  no_media|          1789|            17254|0.002072538860103627|         0|               0|       28|           0|           0|              0|     Mon|   3|    8|       29|         0|         0|         4630|       0|                0|         171|    0.167|                0.0|               0.0|\n",
      "|  no_media|           328|             3338|0.010526315789473684|         0|               1|       21|           0|           0|              0|     Wed|  10|    7|        6|         0|         0|         1107|       0|                0|         217|    0.089|                0.0|               0.0|\n",
      "|  no_media|           728|             4603|                 0.0|         0|               2|       15|           0|           0|              0|     Thu|  18|    6|       30|         0|         0|         1107|       0|                0|         141|      0.0|                0.0|               0.0|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4f1f8",
   "metadata": {},
   "source": [
    "## 4.2 Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37067394",
   "metadata": {},
   "source": [
    "We see that there are no missing values in our dataset, so this step is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cea47faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values for variable media_type : 0\n",
      "Number of null values for variable user_following : 0\n",
      "Number of null values for variable nr_tweets_by_user : 0\n",
      "Number of null values for variable eng_rate : 0\n",
      "Number of null values for variable num_emojis : 0\n",
      "Number of null values for variable upper_case_words : 0\n",
      "Number of null values for variable num_words : 0\n",
      "Number of null values for variable num_hashtags : 0\n",
      "Number of null values for variable num_mentions : 0\n",
      "Number of null values for variable nr_exlcamations : 0\n",
      "Number of null values for variable week_day : 0\n",
      "Number of null values for variable hour : 0\n",
      "Number of null values for variable month : 0\n",
      "Number of null values for variable day_month : 0\n",
      "Number of null values for variable quoted_ind : 0\n",
      "Number of null values for variable symbol_ind : 0\n",
      "Number of null values for variable user_age_days : 0\n",
      "Number of null values for variable verified : 0\n",
      "Number of null values for variable nr_media_elements : 0\n",
      "Number of null values for variable nr_text_char : 0\n",
      "Number of null values for variable sentiment : 0\n",
      "Number of null values for variable polarity : 0\n",
      "Number of null values for variable subjectivity : 0\n"
     ]
    }
   ],
   "source": [
    "# check number of missing values per column\n",
    "for col in basetable_engr_final.columns:\n",
    "    \n",
    "    # look at the perecentage of null values\n",
    "    print(\"Number of null values for variable\", col,\":\", basetable_engr_final.filter(F.col(col).isNull()).count())\n",
    "\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cd53b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_type</th>\n",
       "      <th>user_following</th>\n",
       "      <th>nr_tweets_by_user</th>\n",
       "      <th>eng_rate</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>upper_case_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>nr_exlcamations</th>\n",
       "      <th>...</th>\n",
       "      <th>day_month</th>\n",
       "      <th>quoted_ind</th>\n",
       "      <th>symbol_ind</th>\n",
       "      <th>user_age_days</th>\n",
       "      <th>verified</th>\n",
       "      <th>nr_media_elements</th>\n",
       "      <th>nr_text_char</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   media_type  user_following  nr_tweets_by_user  eng_rate  num_emojis  \\\n",
       "0           0               0                  0         0           0   \n",
       "\n",
       "   upper_case_words  num_words  num_hashtags  num_mentions  nr_exlcamations  \\\n",
       "0                 0          0             0             0                0   \n",
       "\n",
       "   ...  day_month  quoted_ind  symbol_ind  user_age_days  verified  \\\n",
       "0  ...          0           0           0              0         0   \n",
       "\n",
       "   nr_media_elements  nr_text_char  sentiment  polarity  subjectivity  \n",
       "0                  0             0          0         0             0  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "basetable_engr_final.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in basetable_engr_final.columns]).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616eef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37062864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8006276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c7c45c",
   "metadata": {},
   "source": [
    "Further, we see that our variables all have the correct datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0557",
   "metadata": {},
   "source": [
    "# 5. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f02011",
   "metadata": {},
   "source": [
    "In this part, we are going to train a random forest classification model on the training data. This is an advanced machine learning model of which we expect good performance. After we train this model, we will evaluate it. As the goal of our analysis was to find the driving factors behind the engagement rate, we will use the shap values to determine variable importance. This way, we know in which direction the variable affects the engagement rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd181e4",
   "metadata": {},
   "source": [
    "We start by handling our dependent variable, which we make categorical. Then, we use the StringIndexer to create the right input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e33abcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to categorize our dependent variable in order to make it categorical\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "basetable_engr_final = Bucketizer(\n",
    "    splits=[-float('inf'), 0.015, 0.05, float('inf')],\n",
    "    inputCol='eng_rate',\n",
    "    outputCol='label_cat'\n",
    ").transform(basetable_engr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77a31f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|label_cat|count|\n",
      "+---------+-----+\n",
      "|      0.0| 4056|\n",
      "|      1.0|  368|\n",
      "|      2.0|  305|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect\n",
    "basetable_engr_final.groupBy('label_cat').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08969c59",
   "metadata": {},
   "source": [
    "## 5.1 Split the data in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "690752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = basetable_engr_final.randomSplit([0.7, 0.3], seed= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b9272ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of observations in both datasets\n",
    "#print(\"Number of observations in the training set: %s \" % train.count())\n",
    "#print(\"Number of observations in the test set: %s \" %test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b969f3",
   "metadata": {},
   "source": [
    "#### handle class imbalance in dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2d91fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define the number of training observations\n",
    "nr_train_obs = train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf4b0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define ratios of labels\n",
    "ratio_0 = train.filter(F.col('label_cat') == 0.0).count()/nr_train_obs\n",
    "ratio_1 = train.filter(F.col('label_cat') == 1.0).count()/nr_train_obs\n",
    "ratio_2 = train.filter(F.col('label_cat') == 2.0).count()/nr_train_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb698fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define weights of labels\n",
    "weight_0 = 1 - ratio_0\n",
    "weight_1 = 1 - ratio_1\n",
    "weight_2 = 1 - ratio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a5e4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert weight column into training and test set\n",
    "train = train.withColumn(\"weight\", F.when(F.col(\"label_cat\") == 1, weight_1)\\\n",
    "                         .otherwise(F.when(F.col(\"label_cat\") == 2, weight_2).otherwise(weight_0)))\n",
    "test = test.withColumn(\"weight\", F.when(F.col(\"label_cat\") == 1, weight_1)\\\n",
    "                         .otherwise(F.when(F.col(\"label_cat\") == 2, weight_2).otherwise(weight_0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30af3738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             weight|\n",
      "+-------------------+\n",
      "|0.14771322620519156|\n",
      "|  0.919344870210136|\n",
      "|0.14771322620519156|\n",
      "|0.14771322620519156|\n",
      "|0.14771322620519156|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the weights\n",
    "train.select(F.col('weight')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e09ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b896f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f761cd78",
   "metadata": {},
   "source": [
    "## 5.2 Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450db7",
   "metadata": {},
   "source": [
    "**Import required transformers and estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a18c486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark ml packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, StandardScaler, Word2Vec\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "# import models and evaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f28f38",
   "metadata": {},
   "source": [
    "#### Define the different pipeline stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05ef3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string indexer for the dependent variable\n",
    "IDX = StringIndexer(inputCol = 'label_cat', outputCol = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60accc3d",
   "metadata": {},
   "source": [
    "#### handle numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "676921ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the numeric variables\n",
    "num_cols = ['user_following', 'nr_tweets_by_user', 'num_emojis', 'upper_case_words',\n",
    "           'num_words', 'num_hashtags', 'num_mentions', 'nr_exlcamations',  'user_age_days', \n",
    "           'nr_media_elements', 'nr_text_char']\n",
    "\n",
    "# define the assembler\n",
    "num_VA = VectorAssembler(inputCols=num_cols, outputCol=\"num_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02847d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scaler\n",
    "SS = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a33263",
   "metadata": {},
   "source": [
    "#### handle categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe911a",
   "metadata": {},
   "source": [
    "First, we transform the text variables in our dataset into numerical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8dc251be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables\n",
    "cat_cols = ['media_type', 'hour', 'week_day', 'month', 'day_month', 'quoted_ind',\n",
    "           'symbol_ind', 'verified']\n",
    "\n",
    "# create an object of StringIndexer class for each categorical variable\n",
    "SI_media = StringIndexer(inputCol= 'media_type', outputCol= 'media_type_index')\n",
    "SI_hour = StringIndexer(inputCol= 'hour', outputCol= 'hour_index')\n",
    "SI_week_day = StringIndexer(inputCol= 'week_day', outputCol= 'week_day_index')\n",
    "SI_month = StringIndexer(inputCol= 'month', outputCol= 'month_index')\n",
    "SI_day_month = StringIndexer(inputCol= 'day_month', outputCol= 'day_month_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239d54f",
   "metadata": {},
   "source": [
    "Next, we One Hot Encode categorical variables in numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5460b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables that are already in NUMERIC format\n",
    "cat_cols_num = ['media_type_index', 'hour_index', 'week_day_index', 'month_index', \n",
    "                    'day_month_index','quoted_ind', 'symbol_ind', 'verified']\n",
    "\n",
    "# define new names of encoded categorical variables\n",
    "catColumnsOHE = [name  + \"_OHE\" for name in cat_cols_num]\n",
    "\n",
    "# define one hot encoder\n",
    "encoder = OneHotEncoder(inputCols = cat_cols_num, outputCols = catColumnsOHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5429a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables that are already in NUMERIC format\n",
    "cat_VA = VectorAssembler(inputCols= catColumnsOHE, outputCol=\"cat_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befe1f5",
   "metadata": {},
   "source": [
    "Next, we use a VectorAssembler to assemble all encoded variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f35ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector assembler\n",
    "VA_all = VectorAssembler(inputCols=[\"num_features\", \"cat_features\", \"polarity\", \"subjectivity\", \"sentiment\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea3b9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest we don't scale the features \n",
    "VA_RF = VectorAssembler(inputCols=[\"num_features\", \"cat_features\", \"polarity\", \"subjectivity\", \"sentiment\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f2046d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logistic regression model\n",
    "LR = LogisticRegression(featuresCol = \"features\", labelCol = \"label\", weightCol= \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ab7c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest model\n",
    "RF = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\", weightCol= \"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495928f4",
   "metadata": {},
   "source": [
    "## 5.3 Train a logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed9a25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75aa4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training data\n",
    "LR_pipeline = Pipeline().setStages([IDX, num_VA, SS,SI_media, SI_hour, SI_week_day,\n",
    "                                    SI_month, SI_day_month, encoder, cat_VA, VA_all, LR])\\\n",
    "                        .fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e98abb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "LR_preds = LR_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012b10a",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0ab378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# define evaluator (for other metrics)\n",
    "evaluator_mc = MulticlassClassificationEvaluator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28706780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for LR model\n",
    "lr_f1 = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"f1\"})\n",
    "lr_accuracy = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"accuracy\"})\n",
    "lr_recall = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"weightedRecall\"})\n",
    "lr_precision = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"weightedPrecision\"})\n",
    "lr_weighted_TPR = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"weightedTruePositiveRate\"})\n",
    "lr_weighted_FPR = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"weightedFalsePositiveRate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb358ff",
   "metadata": {},
   "source": [
    "Let's refresh the definition of the different metrics:\n",
    "\n",
    "    1)The F-measure is calculated as the harmonic mean of precision and recall, giving each the same weighting. It allows a model to be evaluated taking both the precision and recall into account using a single score, which is helpful when describing the performance of the model and in comparing models.\n",
    "    2) Accuracy = Number of correct predictions of the total number of predictions\n",
    "    3) Recall = the number of correct positive predictions made out of all positive predictions that could have been made\n",
    "    4)  Precision = to the number of true positives divided by the total number of positive predictions \n",
    "    5) Weighted true positive rate = gives the weighted mean of class TPR with weights equal to class probability. TPR is the probability that an actual positive will test positive.\n",
    "    6) The Weighted False Positive Rate gives the weighted mean of class False Positive Rate (FPR) with weights equal to class probability. The FPR is the proportion of negative cases incorrectly identified as positive cases in the data.\n",
    "    \n",
    "For more inforamtion: https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a4340f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION:\n",
      "  F1       : 0.728145\n",
      "  ACCURACY : 0.673128\n",
      "  RECALL   : 0.664879\n",
      "  PRECISION   : 0.798029\n",
      "  WEIGHTED_TPR   : 0.675839\n",
      "  WEIGHTED_FPR   : 0.31146\n"
     ]
    }
   ],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"LOGISTIC REGRESSION:\")\n",
    "print('  F1       : %g' % lr_f1)\n",
    "print('  ACCURACY : %g' % lr_accuracy)\n",
    "print('  RECALL   : %g' % lr_recall)\n",
    "print('  PRECISION   : %g' % lr_precision)\n",
    "print('  WEIGHTED_TPR   : %g' % lr_weighted_TPR)\n",
    "print('  WEIGHTED_FPR   : %g' % lr_weighted_FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60338c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d20b0b36",
   "metadata": {},
   "source": [
    "## 5.4 Train random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e453d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training data\n",
    "RF_pipeline = Pipeline().setStages([IDX, num_VA,SI_media, SI_hour, SI_week_day,\n",
    "                                    SI_month, SI_day_month, encoder, cat_VA, VA_RF, RF])\\\n",
    "                        .fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42c5b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "rf_preds = RF_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30e6f0",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf7d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "148dc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for RF model\n",
    "rf_f1 = evaluator_mc.evaluate(rf_preds, {evaluator_mc.metricName: \"f1\"})\n",
    "rf_accuracy = evaluator_mc.evaluate(rf_preds, {evaluator_mc.metricName: \"accuracy\"})\n",
    "rf_recall = evaluator_mc.evaluate(rf_preds, {evaluator_mc.metricName: \"weightedRecall\"})\n",
    "rf_precision = evaluator_mc.evaluate(rf_preds, {evaluator_mc.metricName: \"weightedPrecision\"})\n",
    "rf_weighted_TPR = evaluator_mc.evaluate(rf_preds, {evaluator_mc.metricName: \"weightedTruePositiveRate\"})\n",
    "rf_weighted_FPR = evaluator_mc.evaluate(rf_preds, {evaluator_mc.metricName: \"weightedFalsePositiveRate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7616752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  F1       : 0.803417\n",
      "  ACCURACY : 0.807641\n",
      "  RECALL   : 0.795165\n",
      "  PRECISION   : 0.786446\n",
      "  WEIGHTED_TPR   : 0.8063\n",
      "  WEIGHTED_FPR   : 0.567836\n"
     ]
    }
   ],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"Random Forest:\")\n",
    "print('  F1       : %g' % rf_f1)\n",
    "print('  ACCURACY : %g' % rf_accuracy)\n",
    "print('  RECALL   : %g' % rf_recall)\n",
    "print('  PRECISION   : %g' % rf_precision)\n",
    "print('  WEIGHTED_TPR   : %g' % rf_weighted_TPR)\n",
    "print('  WEIGHTED_FPR   : %g' % rf_weighted_FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c973e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ec04482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8197596795727636"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the AUC of the model\n",
    "eval_auc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "auc = eval_auc.evaluate(rf_preds)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38cd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c550228c",
   "metadata": {},
   "source": [
    "#### Use cross validation to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7f5eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter space\n",
    "rfparamGrid = (ParamGridBuilder().addGrid(RF.maxDepth, [3, 7, 10])\n",
    "                                   .addGrid(RF.maxBins, [15, 25, 40])\n",
    "                                   .addGrid(RF.numTrees, [5, 25, 60])\n",
    "                                   .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb801858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross validation\n",
    "rfcv_model = CrossValidator(estimator=RF, #random forest model we created before\n",
    "                          estimatorParamMaps=rfparamGrid, \n",
    "                          evaluator=rfEvaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cross validation on trainig set\n",
    "rfcv_model = rfcv_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect best params\n",
    "print(\"best max depth: %s\" %rfcv_model.bestModel._java_obj.getMaxDepth())\n",
    "print(\"best max bins: %s\" %rfcv_model.bestModel._java_obj.getMaxBins())\n",
    "print(\"best num trees: %s\" %rfcv_model.bestModel._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of best model of on test set (cv_model automatically uses best model)\n",
    "rfcv_preds = rfcv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "rfcv_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca17e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ee57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"RANDOM FOREST WITHOUT CV:\")\n",
    "print('  R^2  : %g' % rfsq)\n",
    "print('  MAE  : %g' % rfmae)\n",
    "print('  RMSE : %g' % rfrmse)\n",
    "print('  MSE  : %g' % rfmse)\n",
    "print(\"------------------\")\n",
    "print(\"RANDOM FOREST WITH CV:\")\n",
    "print('  R^2  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'r2'}))\n",
    "print('  MAE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mae'}))\n",
    "print('  RMSE : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'rmse'}))\n",
    "print('  MSE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mse'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f6be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6189e975",
   "metadata": {},
   "source": [
    "# 6. Interpretation Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee61ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "preds_and_labels = rf_preds.select(['prediction','label'])\n",
    "preds_and_labels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b85b045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ffb102755b0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# get predictions and labels\n",
    "preds_and_labels = rf_preds.select(['prediction','label']) \\\n",
    "                                  .withColumn('label', F.col('label').cast(FloatType())) \\\n",
    "                                  .orderBy('prediction') \\\n",
    "                                  .toPandas()\n",
    "\n",
    "# get confusion matrix\n",
    "cm = confusion_matrix(preds_and_labels[\"label\"], preds_and_labels[\"prediction\"], labels=[0, 1, 2])\n",
    "# get confusion matrix figure\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "# plot figure\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44474fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sensitivity and specificity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect sensitivity and specificity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddacc1",
   "metadata": {},
   "source": [
    "## 6.2 Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde60b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect feature importance\n",
    "feature_importance = RF_pipeline.stages[-1].featureImportances.toArray()\n",
    "# define all the features\n",
    "all_feature_names = numFeatureCols + [\"avg_polarity\", \"avg_subjectivity\"] \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(x=range(len(feature_importance)), height=feature_importance)\n",
    "plt.xticks(range(len(feature_importance)), all_feature_names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e20c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f50ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19bfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5cc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae75066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
