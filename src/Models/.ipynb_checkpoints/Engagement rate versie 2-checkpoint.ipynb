{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c0693c",
   "metadata": {},
   "source": [
    "# Initialize pyspark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "#path = \"/Users/konstantinlazarov/Desktop/Big_Data/PySpark/Week_5/spark\"\n",
    "path = \"/Users/Artur/spark\"\n",
    "findspark.init(path) \n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a6ad9",
   "metadata": {},
   "source": [
    "# Import necessary packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1fed3",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5968b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pytz\n",
    "import emojis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from pandas.tseries.holiday import nearest_workday, \\\n",
    "    AbstractHolidayCalendar, Holiday, \\\n",
    "    USMartinLutherKingJr, USPresidentsDay, GoodFriday, \\\n",
    "    USMemorialDay, USLaborDay, USThanksgivingDay\n",
    "from datetime import date\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b37bb",
   "metadata": {},
   "source": [
    "#### Import the twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03afc613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a25396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46530a1c",
   "metadata": {},
   "source": [
    "# Predict the engagement rate of a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2304e11",
   "metadata": {},
   "source": [
    "## 1. Goal of our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00b73b",
   "metadata": {},
   "source": [
    "In this notebook, we are going to predict the engagement rate of tweets. Further, it will be interesting to see the driving factors behind the engagement rate. This can be valuable information when creating an own social media brand or when you want to increase the reach of your tweets.\n",
    "\n",
    "We start by selecting the interesting variables for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849d5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the interesting variables\n",
    "basetable_engr = df_json.select(F.col('created_at').alias('tweet_created'), \\\n",
    "                                   F.col('entities.symbols').alias('symbols'), \\\n",
    "                                   F.col('display_text_range').alias('text_range'), \\\n",
    "                                   F.col('extended_entities.media.type').alias('media_type'), \\\n",
    "                                   F.col('favorite_count'), \\\n",
    "                                   F.col('full_text'), \\\n",
    "                                   F.col('is_quote_status').alias('quoted'), \\\n",
    "                                   F.col('lang').alias('language'), \\\n",
    "                                   F.col('retweet_count'),\\\n",
    "                                   F.col('user.created_at').alias('user_created'), \\\n",
    "                                   F.col('user.followers_count').alias('user_followers'), \\\n",
    "                                   F.col('user.friends_count').alias('user_following'), \\\n",
    "                                   F.col('user.verified').alias('user_verified'), \\\n",
    "                                   F.col(\"user.screen_name\"), \\\n",
    "                                   F.col('user.statuses_count').alias('nr_tweets_by_user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68aafc",
   "metadata": {},
   "source": [
    "Next, we perform some basic preprocessing steps on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e6a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we  convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "basetable_engr = basetable_engr.withColumn('tweet_created', F.to_utc_timestamp(date_udf(\"tweet_created\"), \"UTC\"))\n",
    "basetable_engr = basetable_engr.withColumn('user_created', F.to_utc_timestamp(date_udf(\"user_created\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772d6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "basetable_engr = basetable_engr.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "basetable_engr = basetable_engr.sort(\"tweet_created\", ascending=False)\n",
    "\n",
    "#removing spam accounts \n",
    "basetable_engr = basetable_engr.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1259f",
   "metadata": {},
   "source": [
    "Before we start the feature engineering, we need to filter our data. As we will use the vader package to determine the sentiment later in this notebook, we will only work with English tweets. As our insights in the data will be primarily for European and American companies and most of our tweets are in English, we do not see this as a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b73aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data on language\n",
    "basetable_engr = basetable_engr.filter(F.col(\"language\") == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b26ba",
   "metadata": {},
   "source": [
    "# 2. Create the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca291115",
   "metadata": {},
   "source": [
    "AAN TE PASSEN\n",
    "\n",
    "First, we start by defining our dependent variable. The engagement rate has already been discussed in the data exploration section. We will use the same definition in order to create a model to predict the engagement rate. Below, we repeat this definition:\n",
    "\n",
    "Engagement on Twitter is measured by the number of retweets, follows, replies, favorites, and other people’s reactions to your tweets, including the clicks on the links and hashtags in those tweets. Your Twitter engagement rate is your engagement figure divided by the number of impressions on the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add engagement rate to the dataframe\n",
    "basetable_engr = basetable_engr.withColumn('eng_rate', ((basetable_engr['favorite_count'] + basetable_engr['retweet_count'])/basetable_engr['user_followers']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c3c97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146953f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e4ab37",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88cde2",
   "metadata": {},
   "source": [
    "In order to predict the engagement of a tweet, we will create some additional features in order to improve the performance of our model. For most of these features, we will first creata a function. The following features will be created:\n",
    "\n",
    "    1) number of words\n",
    "    2) number of hashtags\n",
    "    3) number of tags\n",
    "    4) number of emojis\n",
    "    5) get the number of exclamation marks\n",
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "    10) The number of upper case words\n",
    "    11) tweeted quote\n",
    "    12) presence of a symbol\n",
    "    13) The age of the account\n",
    "    14) The number of media elements\n",
    "    15) The media type present\n",
    "    16) The number of text characters in the tweet\n",
    "    17) Indicator if the account is verified\n",
    "\n",
    "These features will be used next to some variables that are already present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675584",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfebde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "671e09af",
   "metadata": {},
   "source": [
    "    1) For the number of words, we can just use the function F.size() and apply it to the tokenized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58af41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define function to count hashtags\n",
    "def get_hashtags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"#\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419c458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define function to count tags\n",
    "def get_tags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"@\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Define a function to get the number of emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c3885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Define function to count exclamation marks\n",
    "def get_exclamation_marks(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter ==  \"!\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609aa6b",
   "metadata": {},
   "source": [
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "\n",
    "We saw how to create these variables when solving the questions for this assignment. The same code will be used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e807c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e25698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Define number of upper case words\n",
    "def get_upper_case_words(text):\n",
    "    counter = 0\n",
    "    \n",
    "    ## Tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    ## Check for uppercase words\n",
    "    for word in word_tokens:\n",
    "        if word.isupper():\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37602e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Define a function that indicates if the tweet was a quote\n",
    "def tweeted_quote_indicator(quoted):\n",
    "    quote = 0\n",
    "    if quoted == True:\n",
    "        quote = 1\n",
    "    return quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7987bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) define a function that indicates the presence of a symbol\n",
    "def symbol_indicator(symbols):\n",
    "    symbol = 0\n",
    "    if(symbols > 0):\n",
    "        symbol = 1\n",
    "    return symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea625a6",
   "metadata": {},
   "source": [
    "    13) Define the age of the account. This is defined as the number of days since the account has been created and the last day of scraping (2022-10-11). The last day of scraping was calculated in the exploration phase of the data. For this variable, we will use the function datediff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c208ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) define a function to get help get the number of media types included in the tweet\n",
    "def adjust_nr_media(number):\n",
    "    if number == -1:\n",
    "        number = 0\n",
    "        \n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "650393c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) define a function to get the first media element\n",
    "def get_media_type(media):\n",
    "    if media == None:\n",
    "        media = 'no_media'\n",
    "    else:\n",
    "        media = media[0]\n",
    "       \n",
    "    return media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0210dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) define a function to get the first media element\n",
    "def get_nr_text_characters(text_range):\n",
    "    number = text_range[1] - text_range[0]  \n",
    "    return number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d61bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Look if the user is a verified user\n",
    "def verified_ind(verified):\n",
    "    indicator = 0\n",
    "    if verified == True:\n",
    "        indicator = 1\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "591c78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the functions as an udf\n",
    "get_upper_case_words_UDF = F.udf(get_upper_case_words, IntegerType()) \n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n",
    "get_hashtags_udf = F.udf(get_hashtags, IntegerType())\n",
    "get_tags_udf = F.udf(get_tags, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "tweeted_quote_indicator_UDF = F.udf(tweeted_quote_indicator, IntegerType())\n",
    "symbol_indicator_udf = F.udf(symbol_indicator, IntegerType())\n",
    "adjust_nr_media_udf = F.udf(adjust_nr_media, IntegerType())\n",
    "get_media_type_udf = F.udf(get_media_type, StringType())\n",
    "get_nr_text_characters_udf = F.udf(get_nr_text_characters, IntegerType())\n",
    "verified_ind_udf = F.udf(verified_ind, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac2b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already define some text cleaning steps in order to define the correct number of words.\n",
    "\n",
    "# define puncutation and stopwords\n",
    "PUNCTUATION = [char for char in punctuation if char not in [\"!\", \"@\", \"#\"]]\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "# define function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    ## Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in PUNCTUATION])\n",
    "    return(text)\n",
    "\n",
    "# define function to remove stopwords\n",
    "def remove_stops(text_tokenized):\n",
    "    # remove stopwords\n",
    "    text_tokenized = [word for word in text_tokenized if word not in STOPWORDS]\n",
    "    return(text_tokenized)\n",
    "\n",
    "# register as udf\n",
    "remove_punct_UDF = F.udf(remove_punct, StringType())\n",
    "remove_stops_UDF = F.udf(remove_stops, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02854ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr.withColumn(\"num_emojis\", emoji_counter_udf(F.col(\"full_text\")))\\\n",
    "                            .withColumn('upper_case_words', get_upper_case_words_UDF('full_text'))\\\n",
    "                            .withColumn(\"text_lower\", F.lower(\"full_text\")) \\\n",
    "                            .withColumn(\"text_cleaned\", remove_punct_UDF(\"text_lower\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"text_cleaned\", \" \")) \\\n",
    "                            .withColumn(\"text_tokenized_no_stops\", remove_stops_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_tags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn('nr_exlcamations', get_exclamation_marks_UDF('text_tokenized_no_stops'))\\\n",
    "                            .withColumn(\"week_day\", F.date_format(F.col(\"tweet_created\"), \"E\"))\\\n",
    "                            .withColumn(\"hour\", F.date_format(F.col(\"tweet_created\"), \"H\").cast('string'))\\\n",
    "                            .withColumn(\"month\", F.date_format(F.col(\"tweet_created\"), \"M\"))\\\n",
    "                            .withColumn(\"day_month\", F.date_format(F.col(\"tweet_created\"), \"d\"))\\\n",
    "                            .withColumn('quoted_ind', tweeted_quote_indicator_UDF('quoted'))\\\n",
    "                            .withColumn('symbol_ind', F.size('symbols'))\\\n",
    "                            .withColumn('symbol_ind', symbol_indicator_udf('symbol_ind'))\\\n",
    "                            .withColumn('user_age_days', F.datediff(F.lit(\"2022-10-11\"), F.col(\"user_created\")))\\\n",
    "                            .withColumn('verified', verified_ind_udf('user_verified'))\\\n",
    "                            .withColumn(\"nr_media_elements\", F.size(\"media_type\"))\\\n",
    "                            .withColumn(\"nr_media_elements\", adjust_nr_media_udf(\"nr_media_elements\"))\\\n",
    "                            .withColumn(\"media_type\", get_media_type_udf('media_type'))\\\n",
    "                            .withColumn(\"nr_text_char\", get_nr_text_characters_udf('text_range'))\\\n",
    "                            .drop('tweet_created')\\\n",
    "                            .drop('quoted')\\\n",
    "                            .drop('symbols')\\\n",
    "                            .drop('user_created')\\\n",
    "                            .drop('user_verified')\\\n",
    "                            .drop('display_text_range')\\\n",
    "                            .drop('text_lower')\\\n",
    "                            .drop('text_cleaned')\\\n",
    "                            .drop('text_tokenized')\\\n",
    "                            .drop('text_tokenized_no_stops')\\\n",
    "                            .drop('text_range')\\\n",
    "                            .filter(\"num_words > 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2a7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|media_type|favorite_count|           full_text|language|retweet_count|user_followers|user_following|   screen_name|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|  no_media|             0|! We will be open...|      en|            0|           947|          2057|veginoutmarket|             1719|                 0.0|         0|               0|       30|           0|           0|              1|     Sat|   7|    1|       15|         0|         0|         1846|       0|                0|         280|\n",
      "|     photo|             8|!! Daily Updates ...|      en|            3|         10745|          2812|      Mix938FM|            16386|0.001023731968357...|         0|               1|       23|           0|           0|              0|     Wed|   9|    9|        7|         0|         0|         4016|       0|                1|         216|\n",
      "|  no_media|             2|!love !iq !waddup...|      en|            3|           181|           267|      Bugateax|              202|0.027624309392265192|         0|               3|       29|           0|           0|              0|     Sun|  13|    2|        6|         0|         0|         1704|       0|                0|         280|\n",
      "|     photo|             0|\" DID YOU KNOW ?\"...|      en|            1|             2|             5|senorita_amigo|               24|                 0.5|         0|               3|       16|           0|           0|              0|     Sat|   4|    9|        3|         0|         0|          119|       0|                1|         184|\n",
      "|     video|             0|\" Nature's Interc...|      en|            0|            87|           567|dorsett_tattoo|              104|                 0.0|         0|               0|       28|           0|           0|              0|     Mon|  20|    7|       18|         0|         0|         2521|       0|                1|         273|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95e2cc",
   "metadata": {},
   "source": [
    "# 3. Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bba8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6790c",
   "metadata": {},
   "source": [
    "Now, we are going to clean the text of the twitter data. Then, we can use this cleaned text to extract features about the sensitivity of the tweet.\n",
    "\n",
    "Here, we remove numbers, punctuation, urls... Further, we transform the emojis to words. Emojis are at the very core of communication over social channels. One small image can completely describe one or more human emotions. A naive thing to do during pre-processing would be to remove all emojis. This could result in significant loss of meaning.\n",
    "A good way to achieve this is to replace the emoji with corresponding text explaining the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30919066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emoji.demojize(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a6dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to udf\n",
    "clean_text_udf = F.udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "137d917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c70e267f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! We will be open 10am-9pm today and tomorrow!...</td>\n",
       "      <td>we will be open ampm today and tomorrow much l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!! Daily Updates !!\\n\\nA taste of Vegan Food t...</td>\n",
       "      <td>daily updates taste of vegan food today on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!love !iq !waddup Building time on the Sketch ...</td>\n",
       "      <td>love iq waddup building time on the sketch smp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" DID YOU KNOW ?\"\\n\\n#senoritacosmetics #senor...</td>\n",
       "      <td>did you know senoritacosmetics senoritacosmeti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" Nature's Interconnection \" \\n\\nCommission fo...</td>\n",
       "      <td>natures interconnection commission for stefano...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  \\\n",
       "0  ! We will be open 10am-9pm today and tomorrow!...   \n",
       "1  !! Daily Updates !!\\n\\nA taste of Vegan Food t...   \n",
       "2  !love !iq !waddup Building time on the Sketch ...   \n",
       "3  \" DID YOU KNOW ?\"\\n\\n#senoritacosmetics #senor...   \n",
       "4  \" Nature's Interconnection \" \\n\\nCommission fo...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  we will be open ampm today and tomorrow much l...  \n",
       "1  daily updates taste of vegan food today on the...  \n",
       "2  love iq waddup building time on the sketch smp...  \n",
       "3  did you know senoritacosmetics senoritacosmeti...  \n",
       "4  natures interconnection commission for stefano...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_engr_final.select(\"full_text\", \"cleaned_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951b7b",
   "metadata": {},
   "source": [
    "Next, we tokenize the text. Then, we remove stop words. Finally, we use a spelling correction library to correct the spelling of the tweet data. This library from the textblob package is based on the Levenshtein distance. To correct the spelling, we first define a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081c910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "# define helper function for spelling\n",
    "#correct_spelling_udf = F.udf(lambda tokens: [TextBlob(token).correct() for token in tokens], ArrayType(StringType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd55c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              tokens|        clean_tokens|\n",
      "+--------------------+--------------------+\n",
      "|[we, will, be, op...|[open, ampm, toda...|\n",
      "|[daily, updates, ...|[daily, updates, ...|\n",
      "|[love, iq, waddup...|[love, iq, waddup...|\n",
      "|[did, you, know, ...|[know, senoritaco...|\n",
      "|[natures, interco...|[natures, interco...|\n",
      "|[scarlet, punica,...|[scarlet, punica,...|\n",
      "|[dairy, milk, no,...|[dairy, milk, lon...|\n",
      "|[greenwashing, is...|[greenwashing, ri...|\n",
      "|[greenwashing, is...|[greenwashing, ri...|\n",
      "|[newyorkcity, sch...|[newyorkcity, sch...|\n",
      "|[vegan, food, is,...|[vegan, food, dis...|\n",
      "|[vegan, food’s, a...|[vegan, food’s, a...|\n",
      "|[my, bad, cholest...|[bad, cholesterol...|\n",
      "|[vegan, festival,...|[vegan, festival,...|\n",
      "|[dont, be, too, h...|[dont, hard, does...|\n",
      "|[steve, brexit, s...|[steve, brexit, s...|\n",
      "|[it, becomes, que...|[becomes, questio...|\n",
      "|[or, vegan, ones,...|[vegan, ones, bra...|\n",
      "|[vita, russia, an...|[vita, russia, an...|\n",
      "|[of, surveyed, sw...|[surveyed, swine,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenize the cleaned_text variable \n",
    "#tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
    "#basetable_engr_final = tokenizer.transform(basetable_engr_final)\n",
    "\n",
    "#remove stop words \n",
    "#remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\")\n",
    "#basetable_engr_final = remover.transform(basetable_engr_final)\n",
    "#basetable_engr_final.select('tokens', 'clean_tokens').show()\n",
    "\n",
    "# correct spelling\n",
    "#basetable_engr_final = basetable_engr_final.withColumn(\"tokens_stemmed\", correct_spelling_udf(\"clean_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edc95cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        clean_tokens|      tokens_stemmed|\n",
      "+--------------------+--------------------+\n",
      "|[open, ampm, toda...|[{parser={__class...|\n",
      "|[daily, updates, ...|[{parser={__class...|\n",
      "|[love, iq, waddup...|[{parser={__class...|\n",
      "|[know, senoritaco...|[{parser={__class...|\n",
      "|[natures, interco...|[{parser={__class...|\n",
      "|[scarlet, punica,...|[{parser={__class...|\n",
      "|[dairy, milk, lon...|[{parser={__class...|\n",
      "|[greenwashing, ri...|[{parser={__class...|\n",
      "|[greenwashing, ri...|[{parser={__class...|\n",
      "|[newyorkcity, sch...|[{parser={__class...|\n",
      "|[vegan, food, dis...|[{parser={__class...|\n",
      "|[vegan, food’s, a...|[{parser={__class...|\n",
      "|[bad, cholesterol...|[{parser={__class...|\n",
      "|[vegan, festival,...|[{parser={__class...|\n",
      "|[dont, hard, does...|[{parser={__class...|\n",
      "|[steve, brexit, s...|[{parser={__class...|\n",
      "|[becomes, questio...|[{parser={__class...|\n",
      "|[vegan, ones, bra...|[{parser={__class...|\n",
      "|[vita, russia, an...|[{parser={__class...|\n",
      "|[surveyed, swine,...|[{parser={__class...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.select('clean_tokens', 'tokens_stemmed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecacd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fa5eb",
   "metadata": {},
   "source": [
    "Now that the text is cleaned, we can derive the sentiment of the text. In this next section, we derive the sentiment, the subjectivity and the polarity. These are the tree final features we add to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21641bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)\n",
    "\n",
    "# define function to get polarity score of text \n",
    "def get_polarity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[0]\n",
    "\n",
    "# define function to get subjectivity score of text \n",
    "def get_subjectivity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[1]\n",
    "\n",
    "\n",
    "# register the functions as udf\n",
    "get_sentiment_udf = F.udf(get_sentiment, DoubleType())\n",
    "get_polarity_udf = F.udf(get_polarity, DoubleType())\n",
    "get_subjectivity_udf = F.udf(get_subjectivity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c764d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"sentiment\", get_sentiment_udf(F.col(\"cleaned_text\")))\\\n",
    "                                .withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "                                .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d1e0f",
   "metadata": {},
   "source": [
    "# 4. Basetable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd4d1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- user_followers: long (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = true)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the basetable:\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a4803",
   "metadata": {},
   "source": [
    "    - Check for missing values. If there are some - handle them (delete, impute,..). In this exercise: write code to handle them even if they aren't present.\n",
    "    - Adjust datatypes where needed\n",
    "    - Remove unnecessary columns\n",
    "    - Add data pre-processing steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be68364",
   "metadata": {},
   "source": [
    "## 4.1 Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "289f4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop (the 3 last variables were used to define the dependent variable)\n",
    "basetable_engr_final = basetable_engr_final.drop('full_text')\\\n",
    "                                .drop('screen_name')\\\n",
    "                                .drop('language')\\\n",
    "                                .drop('cleaned_text')\\\n",
    "                                .drop('retweet_count')\\\n",
    "                                .drop('tokens_stemmed')\\\n",
    "                                .drop('favorite_count')\\\n",
    "                                .drop('user_followers')\n",
    "\n",
    "#.drop('tokens')\\\n",
    "#.drop('clean_tokens')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1da23c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = true)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f169a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|media_type|user_following|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|sentiment|           polarity|      subjectivity|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|  no_media|          2057|             1719|                 0.0|         0|               0|       30|           0|           0|              1|     Sat|   7|    1|       15|         0|         0|         1846|       0|                0|         280|     0.27|              0.375|              0.55|\n",
      "|     photo|          2812|            16386|0.001023731968357...|         0|               1|       23|           0|           0|              0|     Wed|   9|    9|        7|         0|         0|         4016|       0|                1|         216|      0.0|               -0.1|              0.15|\n",
      "|  no_media|           267|              202|0.027624309392265192|         0|               3|       29|           0|           0|              0|     Sun|  13|    2|        6|         0|         0|         1704|       0|                0|         280|    0.127|0.14545454545454548|0.6166666666666667|\n",
      "|     photo|             5|               24|                 0.5|         0|               3|       16|           0|           0|              0|     Sat|   4|    9|        3|         0|         0|          119|       0|                1|         184|      0.0|                0.0|               0.0|\n",
      "|     video|           567|              104|                 0.0|         0|               0|       28|           0|           0|              0|     Mon|  20|    7|       18|         0|         0|         2521|       0|                1|         273|      0.0|                0.0|               0.0|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4f1f8",
   "metadata": {},
   "source": [
    "## 4.2 Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37067394",
   "metadata": {},
   "source": [
    "We see that there are no missing values in our dataset, so this step is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cea47faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values for variable media_type : 0\n",
      "Number of null values for variable user_following : 0\n",
      "Number of null values for variable nr_tweets_by_user : 0\n",
      "Number of null values for variable eng_rate : 0\n",
      "Number of null values for variable num_emojis : 0\n",
      "Number of null values for variable upper_case_words : 0\n",
      "Number of null values for variable num_words : 0\n",
      "Number of null values for variable num_hashtags : 0\n",
      "Number of null values for variable num_mentions : 0\n",
      "Number of null values for variable nr_exlcamations : 0\n",
      "Number of null values for variable week_day : 0\n",
      "Number of null values for variable hour : 0\n",
      "Number of null values for variable month : 0\n",
      "Number of null values for variable day_month : 0\n",
      "Number of null values for variable quoted_ind : 0\n",
      "Number of null values for variable symbol_ind : 0\n",
      "Number of null values for variable user_age_days : 0\n",
      "Number of null values for variable verified : 0\n",
      "Number of null values for variable nr_media_elements : 0\n",
      "Number of null values for variable nr_text_char : 0\n",
      "Number of null values for variable sentiment : 0\n",
      "Number of null values for variable polarity : 0\n",
      "Number of null values for variable subjectivity : 0\n"
     ]
    }
   ],
   "source": [
    "# check number of missing values per column\n",
    "for col in basetable_engr_final.columns:\n",
    "    \n",
    "    # look at the perecentage of null values\n",
    "    print(\"Number of null values for variable\", col,\":\", basetable_engr_final.filter(F.col(col) == None).count())\n",
    "\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2cd53b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_type</th>\n",
       "      <th>user_following</th>\n",
       "      <th>nr_tweets_by_user</th>\n",
       "      <th>eng_rate</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>upper_case_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>nr_exlcamations</th>\n",
       "      <th>...</th>\n",
       "      <th>day_month</th>\n",
       "      <th>quoted_ind</th>\n",
       "      <th>symbol_ind</th>\n",
       "      <th>user_age_days</th>\n",
       "      <th>verified</th>\n",
       "      <th>nr_media_elements</th>\n",
       "      <th>nr_text_char</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   media_type  user_following  nr_tweets_by_user  eng_rate  num_emojis  \\\n",
       "0           0               0                  0         0           0   \n",
       "\n",
       "   upper_case_words  num_words  num_hashtags  num_mentions  nr_exlcamations  \\\n",
       "0                 0          0             0             0                0   \n",
       "\n",
       "   ...  day_month  quoted_ind  symbol_ind  user_age_days  verified  \\\n",
       "0  ...          0           0           0              0         0   \n",
       "\n",
       "   nr_media_elements  nr_text_char  sentiment  polarity  subjectivity  \n",
       "0                  0             0          0         0             0  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "basetable_engr_final.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in basetable_engr_final.columns]).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616eef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37062864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8006276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c7c45c",
   "metadata": {},
   "source": [
    "Further, we see that our variables all have the correct datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0557",
   "metadata": {},
   "source": [
    "# 4.3 Add pre-processing steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450db7",
   "metadata": {},
   "source": [
    "**Import required transformers and estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a18c486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark ml packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, StandardScaler, Word2Vec\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "766099c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stages of preprocessing pipeline\n",
    "stagesPreprocessingPipeline = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f28f38",
   "metadata": {},
   "source": [
    "**Create a vector for each type of features (numeric, categorical)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60accc3d",
   "metadata": {},
   "source": [
    "#### handle numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "676921ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the numeric variables\n",
    "num_cols = ['user_following', 'nr_tweets_by_user', 'num_emojis', 'upper_case_words',\n",
    "           'num_words', 'num_hashtags', 'num_mentions', 'nr_exlcamations', 'quoted_ind',\n",
    "           'symbol_ind', 'user_age_days', 'verified', 'nr_media_elements', 'nr_text_char', \n",
    "           'sentiment', 'polarity', 'subjectivity']\n",
    "\n",
    "# define the assembler\n",
    "num_vec_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(num_vec_assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42297141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a33263",
   "metadata": {},
   "source": [
    "#### handle categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe911a",
   "metadata": {},
   "source": [
    "First, we transform the text variables in our dataset into numerical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef799e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dc251be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the categorical variables\n",
    "cat_cols = ['media_type', 'hour', 'week_day', 'month', 'day_month']\n",
    "\n",
    "# create an object of StringIndexer class for each categorical variable\n",
    "SI_media = StringIndexer(inputCol= 'media_type', outputCol= 'media_type_index').setHandleInvalid(\"skip\")\n",
    "SI_hour = StringIndexer(inputCol= 'hour', outputCol= 'hour_index').setHandleInvalid(\"skip\")\n",
    "SI_week_day = StringIndexer(inputCol= 'week_day', outputCol= 'week_day_index').setHandleInvalid(\"skip\")\n",
    "SI_month = StringIndexer(inputCol= 'month', outputCol= 'month_index').setHandleInvalid(\"skip\")\n",
    "SI_day_month = StringIndexer(inputCol= 'day_month', outputCol= 'day_month_index').setHandleInvalid(\"skip\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(SI_media)\n",
    "stagesPreprocessingPipeline.append(SI_hour)\n",
    "stagesPreprocessingPipeline.append(SI_week_day)\n",
    "stagesPreprocessingPipeline.append(SI_month)\n",
    "stagesPreprocessingPipeline.append(SI_day_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39b48d",
   "metadata": {},
   "source": [
    "Next, we use a VectorAssembler to assemble all indexed variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5429a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the assembler\n",
    "cat_cols_indexed = ['media_type_index', 'hour_index', 'week_day_index', 'month_index', 'day_month_index']\n",
    "cat_vec_assembler = VectorAssembler(inputCols= cat_cols_indexed, outputCol=\"cat_features\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(cat_vec_assembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a188e",
   "metadata": {},
   "source": [
    "VRAAG KONSTANTIN: VECTOR INDEXER NOG NODIG NA STRINGINDEXER?\n",
    "Finally ,we use the VectorIndexer that helps index categorical features in datasets of vectors. This is required for tree based methods, which we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79e5f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexer\n",
    "indexer = VectorIndexer(inputCol= \"cat_features\", outputCol=\"cat_features_indexed\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcdf780",
   "metadata": {},
   "source": [
    "We will also look at the words that were used in the text with the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33999f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word2vec model\n",
    "#W2V = Word2Vec(inputCol = 'tokens_stemmed', outputCol = 'text_features')\n",
    "\n",
    "# add to pipeline stage\n",
    "#stagesPreprocessingPipeline.append(W2V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7d00f",
   "metadata": {},
   "source": [
    "**Define, fit and apply Pipeline on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b865e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on data\n",
    "pipeline_model_preprocess = Pipeline().setStages(stagesPreprocessingPipeline).fit(basetable_engr_final)\n",
    "\n",
    "# transform data by applying pipeline model on data\n",
    "preprocessed_data = pipeline_model_preprocess.transform(basetable_engr_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a3065",
   "metadata": {},
   "source": [
    "**Select features and label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a864c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "preprocessed_data = preprocessed_data.select([\"num_features\", \"cat_features_indexed\", \"eng_rate\"])\n",
    "\n",
    "# rename engagement rate to label\n",
    "preprocessed_data = preprocessed_data.withColumnRenamed(\"eng_rate\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9be8c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|        num_features|cat_features_indexed|               label|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(17,[0,1,4,7,10,1...|[0.0,20.0,5.0,9.0...|                 0.0|\n",
      "|(17,[0,1,3,4,10,1...|[1.0,16.0,1.0,11....|0.001023731968357...|\n",
      "|(17,[0,1,3,4,10,1...|[0.0,7.0,4.0,10.0...|0.027624309392265192|\n",
      "|(17,[0,1,3,4,10,1...|[1.0,21.0,5.0,11....|                 0.5|\n",
      "|(17,[0,1,4,10,12,...|[2.0,5.0,6.0,2.0,...|                 0.0|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the data\n",
    "preprocessed_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a60e7a",
   "metadata": {},
   "source": [
    "## 4.5 Split dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2257052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = preprocessed_data.randomSplit([0.7, 0.3], seed= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of observations in both datasets\n",
    "print(\"Number of observations in the training set: %s \" % train.count())\n",
    "print(\"Number of observations in the test set: %s \" %test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e5336",
   "metadata": {},
   "source": [
    "**Define scaler to standardize numeric features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c787472",
   "metadata": {},
   "source": [
    "The `StandardScaler` should only be performed on the trainingset, because an equal `mean` and `standard deviation` between the training- and testset need to be assumed to avoid methodological mistakes. However, as we will only train a random forest model it is not needed to standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148dc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5abfb88",
   "metadata": {},
   "source": [
    "**Create final feature vector containing the indexed categorical features and scaled numeric features**\n",
    "\n",
    "Most ML algorithms can only handle one vector as input, so we add all preprocessed columns (here two vectors) into one vector, using the ``VectorAssembler`` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2885feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define assembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"cat_features_indexed\", \"num_features\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550228c",
   "metadata": {},
   "source": [
    "**Define, fit and apply Pipeline on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7f5eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model stages\n",
    "stages = [vec_assembler]\n",
    "\n",
    "# define pipeline model\n",
    "pipeline_model_split = Pipeline().setStages(stages)\n",
    "\n",
    "# fit pipeline model on training data\n",
    "pipeline_model_split = pipeline_model_split.fit(train)\n",
    "\n",
    "# transform training set\n",
    "train_final = pipeline_model_split.transform(train).select([\"label\", \"features\"])\n",
    "\n",
    "# transform test set\n",
    "test_final = pipeline_model_split.transform(test).select([\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb801858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               label|            features|\n",
      "+--------------------+--------------------+\n",
      "|                 0.0|[1.0,11.0,6.0,1.0...|\n",
      "|                 0.0|(22,[1,2,3,4,5,6,...|\n",
      "|1.080452127659574...|[1.0,2.0,3.0,0.0,...|\n",
      "|                 0.0|(22,[1,2,3,4,5,6,...|\n",
      "|8.748906386701663E-4|(22,[1,2,4,5,6,7,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the final data\n",
    "train_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff619af6",
   "metadata": {},
   "source": [
    "# 5. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cea52",
   "metadata": {},
   "source": [
    "In this part, we are going to train a random forest model on the training data. This is an advanced machine learning model of which we expect good performance. After we train this model, we will evaluate it. As the goal of our analysis was to find the driving factors behind the engagement rate, we will use the shap values to determine variable importance. This way, we know in which direction the variable affects the engagement rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af18d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and evaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3413885",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8759b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rf model\n",
    "RF = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", numTrees=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3920129a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o987.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 224.0 failed 1 times, most recent failure: Lost task 7.0 in stage 224.0 (TID 4251) (192.168.0.127 executor driver): scala.MatchError: [null,1.0,(22,[0,2,4,5,6,7,8,9,15,17,18,20,21],[1.0,3.0,4.0,1.0,91.0,7.0,4.0,29.0,388.0,1.0,258.0,0.08461538461538462,0.4807692307692308])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: scala.MatchError: [null,1.0,(22,[0,2,4,5,6,7,8,9,15,17,18,20,21],[1.0,3.0,4.0,1.0,91.0,7.0,4.0,29.0,388.0,1.0,258.0,0.08461538461538462,0.4807692307692308])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-589b2564a521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o987.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 224.0 failed 1 times, most recent failure: Lost task 7.0 in stage 224.0 (TID 4251) (192.168.0.127 executor driver): scala.MatchError: [null,1.0,(22,[0,2,4,5,6,7,8,9,15,17,18,20,21],[1.0,3.0,4.0,1.0,91.0,7.0,4.0,29.0,388.0,1.0,258.0,0.08461538461538462,0.4807692307692308])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: scala.MatchError: [null,1.0,(22,[0,2,4,5,6,7,8,9,15,17,18,20,21],[1.0,3.0,4.0,1.0,91.0,7.0,4.0,29.0,388.0,1.0,258.0,0.08461538461538462,0.4807692307692308])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# fit model on training set\n",
    "rf_model = RF.fit(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "rf_preds = rf_model.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluator object\n",
    "rfEvaluator = RegressionEvaluator(labelCol = 'label', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get different metrics using your created evaluator object\n",
    "rfsq = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'r2'})\n",
    "rfmae = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'mae'})\n",
    "rfrmse = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'rmse'})\n",
    "rfmse = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30252ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect evaluation metrics\n",
    "print('R^2  : %g' % rfsq)\n",
    "print('MAE  : %g' % rfmae)\n",
    "print('RMSE : %g' % rfrmse)\n",
    "print('MSE  : %g' % rfmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263b4fb",
   "metadata": {},
   "source": [
    "#### Use cross validation to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter space\n",
    "rfparamGrid = (ParamGridBuilder().addGrid(RF.maxDepth, [3, 7, 10])\n",
    "                                   .addGrid(RF.maxBins, [15, 25, 40])\n",
    "                                   .addGrid(RF.numTrees, [5, 25, 60])\n",
    "                                   .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ab519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross validation\n",
    "rfcv_model = CrossValidator(estimator=RF, #random forest model we created before\n",
    "                          estimatorParamMaps=rfparamGrid, \n",
    "                          evaluator=rfEvaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cross validation on trainig set\n",
    "rfcv_model = rfcv_model.fit(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect best params\n",
    "print(\"best max depth: %s\" %rfcv_model.bestModel._java_obj.getMaxDepth())\n",
    "print(\"best max bins: %s\" %rfcv_model.bestModel._java_obj.getMaxBins())\n",
    "print(\"best num trees: %s\" %rfcv_model.bestModel._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of best model of on test set (cv_model automatically uses best model)\n",
    "rfcv_preds = rfcv_model.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "rfcv_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca17e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ee57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"RANDOM FOREST WITHOUT CV:\")\n",
    "print('  R^2  : %g' % rfsq)\n",
    "print('  MAE  : %g' % rfmae)\n",
    "print('  RMSE : %g' % rfrmse)\n",
    "print('  MSE  : %g' % rfmse)\n",
    "print(\"------------------\")\n",
    "print(\"RANDOM FOREST WITH CV:\")\n",
    "print('  R^2  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'r2'}))\n",
    "print('  MAE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mae'}))\n",
    "print('  RMSE : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'rmse'}))\n",
    "print('  MSE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mse'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189e975",
   "metadata": {},
   "source": [
    "# 6. Interpretation Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee61ec",
   "metadata": {},
   "source": [
    "As we want use the random forest model, we would also like some insights into the drivers behind the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install shap\n",
    "#!pip3 install numpy\n",
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values,  X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0c040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31414fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44474fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70addd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d80d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
