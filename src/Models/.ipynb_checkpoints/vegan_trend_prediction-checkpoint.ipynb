{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76508cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "# initialize findspark with spark directory\n",
    "findspark.init(\"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\")\n",
    "#findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99192d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import emojis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "import matplotlib.pyplot as plt \n",
    "import emojis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import plotly.express as px\n",
    "from pandas.tseries.holiday import nearest_workday, \\\n",
    "    AbstractHolidayCalendar, Holiday, \\\n",
    "    USMartinLutherKingJr, USPresidentsDay, GoodFriday, \\\n",
    "    USMemorialDay, USLaborDay, USThanksgivingDay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dafa38",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dcdae",
   "metadata": {},
   "source": [
    "In this notebook we will buid a model that predicts if the trend of a certain topic goes up or down on a certain day based on Twitter data of that day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1943e2",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa5e6",
   "metadata": {},
   "source": [
    "### 1.1 Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read trend data \n",
    "trend = spark.read.csv(\".././../data/Google_trends/daily_trends.csv\", header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be47e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy().orderBy(col(\"date\"))\n",
    "trend.withColumn(\"dependent_vegan\", lag(\"dependent_vegan\", -1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb226e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "trend.createOrReplaceTempView(\"trendSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b9d77",
   "metadata": {},
   "source": [
    "The binary variable indicates if the trend goes up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aff149",
   "metadata": {},
   "source": [
    "### 1.2 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795db881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data dir\n",
    "data_dir = \"../../data/Topic/\"\n",
    "\n",
    "# get all twitter files\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2953fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter data \n",
    "#twitter_df = spark.read.json(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hashtags = [\"vegan\"]\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "files_hashtags = [file for file in tweet_files if (file.find(list_hashtags[0]) != -1)]             \n",
    "twitter_df = spark.read.option(\"multiline\",\"true\").json(files_hashtags) \n",
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select interesting features\n",
    "twitter_df = twitter_df.select(F.col('user.name'),\n",
    "                                F.col('user.screen_name'),\n",
    "                                F.col('user.followers_count'),\n",
    "                                F.col('user.following'),\n",
    "                                F.col('user.statuses_count'),\n",
    "                                F.col('user.listed_count'),\n",
    "                                F.col('created_at'),\n",
    "                                F.col('full_text'),\n",
    "                                F.col('entities.hashtags'),\n",
    "                                F.col('favorite_count'),\n",
    "                                F.col('retweet_count'),\n",
    "                                F.col('user.friends_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad4622",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9dd70",
   "metadata": {},
   "source": [
    "### 2.1 Check time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96607a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "twitter_df = twitter_df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first post\n",
    "first_post = F.min('post_created_at').alias('earliest')\n",
    "# get latest post\n",
    "latest_post = F.max('post_created_at').alias('latest')\n",
    "# show tweet period in our dataset\n",
    "twitter_df.select(first_post, latest_post).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f4e9b",
   "metadata": {},
   "source": [
    "### 2.2 Remove retweets and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all retweets from dataset\n",
    "no_retweets_df = twitter_df.filter(~F.col(\"full_text\").startswith(\"RT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sort no_retweets_df based on date in chronological order (most recent ones on top)\n",
    "no_retweets_sorted_df = no_retweets_df.sort(\"post_created_at\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations before dropping duplicates\n",
    "no_retweets_sorted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates based on tweet text and the profile it was posted from\n",
    "final_no_duplicates_df = no_retweets_sorted_df.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a33594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations after dropping duplicates\n",
    "final_no_duplicates_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908138e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename dataframe\n",
    "final_twitter_df = final_no_duplicates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6316",
   "metadata": {},
   "source": [
    "## 3. Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165908d",
   "metadata": {},
   "source": [
    "For our independent variables we need to design a pipeline that transforms the data into the desired aggregated metrics per day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15c1f5",
   "metadata": {},
   "source": [
    "### 3.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39fa87",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050976b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d141323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count number of emojis used\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3079f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate engagement rate\n",
    "def engagement_rate(favorite_count, retweet_count, followers_count):\n",
    "    if(followers_count == 0):\n",
    "        eng_rate = 0\n",
    "    else:\n",
    "        eng_rate = (favorite_count + retweet_count)/followers_count\n",
    "    \n",
    "    return eng_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_UDF = F.udf(emoji_counter, IntegerType())\n",
    "engagement_rate_UDF = F.udf(engagement_rate, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions to create new features\n",
    "final_twitter_df = final_twitter_df.withColumn(\"emoji_count\", emoji_counter_UDF(\"full_text\")) \\\n",
    "        .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "        .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "        .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "        .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "        .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\")) \\\n",
    "        .withColumn(\"engagement_rate\", engagement_rate_UDF(\"favorite_count\", \"retweet_count\", \"followers_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7219ef0",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b483df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english tweets (NOTE: for the assignment you can translate non-english tweets using an API)\n",
    "final_twitter_df = final_twitter_df.filter(F.col(\"lang\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72531af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check number of observations\n",
    "final_twitter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21463c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to udf\n",
    "clean_text_udf = F.udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string\n",
    "final_twitter_df = final_twitter_df.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c608f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "final_twitter_df.select(\"full_text\", \"cleaned_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bdf59",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1457b",
   "metadata": {},
   "source": [
    "VADER sentimental analysis relies on a dictionary that maps lexical features to emotion intensities known as sentiment scores. The sentiment score of a text can be obtained by summing up the intensity of each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c817a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using the vaderSentiment package \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ff1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)\n",
    "\n",
    "get_sentiment_udf = udf(get_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_twitter_df = final_twitter_df.withColumn(\"sentiment_vader\", get_sentiment_udf(F.col(\"cleaned_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304a09d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_twitter_df.select(\"cleaned_text\", \"sentiment_vader\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d493ac0",
   "metadata": {},
   "source": [
    "#### TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b22e97",
   "metadata": {},
   "source": [
    "TextBlob returns polarity and subjectivity of a sentence. \n",
    "\n",
    "**Polarity** lies between [-1,1],  -1 defines a negative sentiment and 1 defines a positive sentiment.  \n",
    "\n",
    "**Subjectivity** quantifies the amount of personal opinion and factual information contained in the text. Subjectivity lies between [0,1]. The higher subjectivity means that the text contains personal opinion rather than factual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad40a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use polarity and subjectivity from TextBlob \n",
    "#https://textblob.readthedocs.io/en/dev/\n",
    "from textblob import TextBlob\n",
    "\n",
    "# define function to get polarity score of text document\n",
    "def get_polarity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[0]\n",
    "# define function to get subjectivity score of text document\n",
    "def get_subjectivity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[1]\n",
    "get_polarity_udf = F.udf(get_polarity, DoubleType())\n",
    "get_subjectivity_udf = F.udf(get_subjectivity, DoubleType())\n",
    "\n",
    "final_twitter_df = final_twitter_df.withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "        .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))\n",
    "\n",
    "final_twitter_df.select(\"cleaned_text\", \"sentiment_vader\", \"polarity\", \"subjectivity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60590543",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_twitter_df = final_twitter_df.withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "        .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa86eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_twitter_df.select(\"cleaned_text\", \"sentiment_vader\", \"polarity\", \"subjectivity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "final_twitter_df.createOrReplaceTempView(\"twitterSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cf7e6",
   "metadata": {},
   "source": [
    "### 3.1 Volume of tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "volume = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, COUNT(*) as volume \\\n",
    "                                    FROM twitterSQL \\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7816d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "volume.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "volume.createOrReplaceTempView(\"volumeSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb81a7",
   "metadata": {},
   "source": [
    "### 3.2 Average likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e49f2",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28099a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_likes = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(favorite_count) as avg_likes \\\n",
    "                           FROM twitterSQL \\\n",
    "                           WHERE favorite_count > 0 \\\n",
    "                           GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                           ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edf65d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_likes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6003b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_likes.createOrReplaceTempView(\"avg_likesSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa4e0b",
   "metadata": {},
   "source": [
    "### 3.3 Average Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff829814",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b17a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_retweets = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(retweet_count) as avg_retweets \\\n",
    "                          FROM twitterSQL \\\n",
    "                          WHERE retweet_count > 0 \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6f578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_retweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_retweets.createOrReplaceTempView(\"avg_retweetsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c11de2",
   "metadata": {},
   "source": [
    "### 3.4 Average Engagement rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271c4c1",
   "metadata": {},
   "source": [
    "We define engagement rate of a tweet as the sum of likes and retweets divided by the amount of followers of the account that sent out the tweet. For our purpose we will take the avergage engagement rate per day. We exclude accounts who have no followers and we only take tweets into account which are liked and retweeted at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_engagement_rate = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(engagement_rate) as avg_engagement_rate \\\n",
    "                                     FROM twitterSQL \\\n",
    "                                     GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                     ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ebc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_engagement_rate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2dfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_engagement_rate.createOrReplaceTempView(\"avg_engagement_rateSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea74252",
   "metadata": {},
   "source": [
    "### 3.5 Number of influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77904a",
   "metadata": {},
   "source": [
    "We will calculate how many influencers actively tweeted a certain day. We define an influencer as someone with:\n",
    "- followers > 1000 \n",
    "- engagement_rate > 0.20 \n",
    "- weekly tweet frequency > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d92a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influencers(follower_count_tresh, eng_rate_tresh, freq_week_tresh, data):\n",
    "\n",
    "    #df\n",
    "    df = data\n",
    "    \n",
    "    # get all users with their amount of followers\n",
    "    influencers = df.groupBy(\"screen_name\") \\\n",
    "                    .agg(first(\"followers_count\").alias(\"followers_count\"))\n",
    "\n",
    "    # average engagement rate for each user\n",
    "    eng_rate = df.withColumn('eng_rate', ((df['favorite_count'] + df['retweet_count'])/df['followers_count']))\n",
    "\n",
    "    eng_rate_user = eng_rate.groupBy(\"screen_name\") \\\n",
    "                            .agg(avg(\"eng_rate\").alias(\"eng_rate\"))\n",
    "\n",
    "    # average freq_weekly per user\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('screen_name', 'year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('screen_name', 'year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select('screen_name', 'freq')\n",
    "\n",
    "    freq_week = freq_week.groupby(\"screen_name\").agg(avg(freq_week.freq).alias('freq'))\n",
    "\n",
    "    # put the data together\n",
    "    data_joined = eng_rate_user.join(influencers, \"screen_name\").join(freq_week, \"screen_name\")\n",
    "\n",
    "    # filter the data\n",
    "    data_joined = data_joined.filter((data_joined.followers_count > follower_count_tresh) & (data_joined.eng_rate > eng_rate_tresh) & (data_joined.freq > freq_week_tresh))\n",
    "    \n",
    "    # show the data\n",
    "    data_joined.show()\n",
    "    return data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df2e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "influencers = get_influencers(1000, 0.002, 2, final_twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acae7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "influencers.createOrReplaceTempView(\"influencersSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "number_of_influencers = spark.sql(\" SELECT DATE_FORMAT(a.post_created_at, 'Y-M-dd') as date, COUNT(b.screen_name) as influencers \\\n",
    "                                    FROM twitterSQL a \\\n",
    "                                    RIGHT OUTER JOIN influencersSQL b ON a.screen_name = b.screen_name\\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c3a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "number_of_influencers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd978455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"number_of_influencersSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac84d0b6",
   "metadata": {},
   "source": [
    "### 3.6 Average Followers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_followers = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(followers_count) as avg_followers \\\n",
    "                          FROM twitterSQL \\\n",
    "                          WHERE followers_count > 0 \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbef568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_followers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecbe410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"followersSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1886a",
   "metadata": {},
   "source": [
    "### 3.7 Average Emoji Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ae563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_emoji = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(emoji_count) as avg_emojis \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_emoji.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"emojiSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8e7d9",
   "metadata": {},
   "source": [
    "### 3.8 Avergae Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de782c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_words = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_words) as avg_words \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_words.createOrReplaceTempView(\"wordsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d277253",
   "metadata": {},
   "source": [
    "### 3.9 Avergae Number of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_hashtags = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_hashtags) as avg_hashtags \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf351dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207697b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_hashtags.createOrReplaceTempView(\"hashtagsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ef6de",
   "metadata": {},
   "source": [
    "### 3.10 Average Number of Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8408634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_mentions = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_mentions) as avg_mentions \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_mentions.createOrReplaceTempView(\"mentionsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58726060",
   "metadata": {},
   "source": [
    "### 3.11 Average Number of Exclamation Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_marks = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_exclamation_marks) as avg_exclamation_marks \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6abfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_marks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c522446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_marks.createOrReplaceTempView(\"marksSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95f264",
   "metadata": {},
   "source": [
    "### 3.12 Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_sentiment = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(sentiment_vader) as avg_sentiment \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_sentiment.createOrReplaceTempView(\"sentimentSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237688b5",
   "metadata": {},
   "source": [
    "### 3.13 Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1778ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_polarity = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(polarity) as avg_polarity \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9874c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_polarity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_polarity.createOrReplaceTempView(\"polaritySQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb6f32",
   "metadata": {},
   "source": [
    "### 3.13 Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d11cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_subjectivity = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(subjectivity) as avg_subjectivity \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bcadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_subjectivity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_subjectivity.createOrReplaceTempView(\"subjectivitySQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b60dd",
   "metadata": {},
   "source": [
    "## 4. Basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basetable\n",
    "basetable = volume.join(avg_likes, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_retweets, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_engagement_rate, \"date\", how=\"inner\") \\\n",
    "                    .join(number_of_influencers, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_followers, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_emoji, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_words, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_hashtags, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_mentions, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_marks, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_sentiment, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_polarity, \"date\", how=\"inner\") \\\n",
    "                    .join(avg_subjectivity, \"date\", how=\"inner\") \\\n",
    "                    .join(trend, \"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the basetable\n",
    "basetable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable = basetable.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41acbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export basetable as a .json file\n",
    "basetable.to_json(\"./../../data/basetable_vegan_trend_prediction.json\", orient=\"records\", force_ascii=False, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b2a65",
   "metadata": {},
   "source": [
    "#### Read table in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426afab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the saved basetable (.json)\n",
    "basetable_df = spark.read.json(\"./../../data/basetable_vegan_trend_prediction.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7cdbd3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+---------------------+----------------+------------+-------------+------------+------------+-------------+-------------+----------------+-------------+----------+---------------+-----------+------+\n",
      "|  avg_emojis|avg_engagement_rate|avg_exclamation_marks|   avg_followers|avg_hashtags|    avg_likes|avg_mentions|avg_polarity| avg_retweets|avg_sentiment|avg_subjectivity|    avg_words|      date|dependent_vegan|influencers|volume|\n",
      "+------------+-------------------+---------------------+----------------+------------+-------------+------------+------------+-------------+-------------+----------------+-------------+----------+---------------+-----------+------+\n",
      "|0.4280180762|       0.0303423202|         0.2091672046| 7811.1375488918| 1.506132989| 9.5102286402|1.2246610717|0.1127719889| 3.3629343629| 0.1322898644|    0.4252520269|25.9561007101|2021-11-03|            1.0|        120|  1549|\n",
      "|0.8352165725|       0.0330735083|         0.3549905838| 3640.0143403442|6.1186440678|18.8860544218| 0.565913371|0.1794901069|10.7292307692| 0.1655160075|     0.415197219|25.7542372881| 2022-8-15|            0.0|        119|  1062|\n",
      "|0.5795454545|       0.0090354097|         0.3522727273|  6038.816091954|6.6363636364| 5.2222222222|0.2613636364|0.2407288701|      2.59375| 0.1910340909|    0.4031603386|24.8068181818| 2022-6-13|            0.0|          4|    88|\n",
      "|0.9856512141|       0.0808854463|         0.3498896247| 6134.9597315436|6.0794701987|14.1910946197| 0.614790287|0.1695837324| 6.1367781155| 0.1527108168|    0.4074819656|25.4150110375| 2022-8-14|            1.0|         89|   906|\n",
      "|0.8306828812|       0.0479265599|         0.4237605239| 4375.8903591682| 6.506080449|12.4626086957|  0.40505145|0.2141369687| 6.7068062827| 0.1783002806|    0.4308005243|25.5098222638| 2022-7-07|            0.0|         76|  1069|\n",
      "| 0.737394958|        0.025709073|         0.3865546218| 5702.7358892439|5.6575630252|12.0820512821|0.4978991597|0.2091006325| 6.2695924765| 0.1690987395|    0.4259372933|24.4957983193| 2022-5-26|            1.0|         78|   952|\n",
      "|0.4393939394|       0.0213452919|         0.2095959596| 3704.7251308901|1.6767676768|14.8789237668|1.0934343434|0.1270217157| 7.6951219512| 0.1265151515|     0.460761597|29.3484848485|2021-10-27|            0.0|         32|   396|\n",
      "| 0.724847561|       0.0332267214|         0.3208841463| 5130.6095089634| 4.293445122|19.2364394993|0.5861280488|0.1551534023|11.4625668449| 0.1496036585|    0.4115203241|     25.21875| 2022-3-13|            1.0|        129|  1312|\n",
      "|0.7159804606|       0.0280713247|         0.3914863922| 9470.1829096045|5.8457780879|15.8010012516|0.5150034892|0.2072232474| 5.5292929293|  0.154718074|    0.4430629996|25.9902302861| 2022-1-10|            0.0|         93|  1433|\n",
      "|  0.73657289|       0.0203645056|         0.3631713555| 4784.1841432225|5.9488491049|13.6784140969|0.3708439898| 0.194723533|       10.312|  0.161826087|    0.4216710856| 25.631713555| 2022-7-15|            1.0|         40|   391|\n",
      "|0.6647058824|       0.0381538045|         0.3008403361| 5410.1367965368|6.4268907563|14.4456140351|0.3487394958|0.1841718318| 6.7661290323| 0.1661428571|    0.4244741044|25.5344537815| 2022-4-16|            1.0|         81|  1190|\n",
      "|0.8772678762|       0.0323760468|         0.3468516542| 3448.8548387097|5.7097118463|21.7274305556|  0.37886873|0.1865421411| 7.6457680251| 0.1656638207|    0.4088177264|23.9786552828| 2022-5-01|            1.0|        100|   937|\n",
      "|0.4488696263|       0.0345402634|         0.2179922689|14339.8916508088|0.9578306197|12.6662473795|0.9076959119|0.1185524159| 6.7667757774| 0.1361419702|    0.4165406415| 23.792432939| 2022-8-03|            1.0|        356|  8537|\n",
      "|0.6615776081|       0.0342467786|         0.3702290076| 6531.6858974359|5.6857506361|17.1735159817|0.4541984733|0.2004823474| 9.9288537549| 0.1699516539|    0.4254932817|24.3117048346| 2022-5-29|            1.0|         57|   786|\n",
      "| 0.781512605|       0.0182628516|         0.3538748833| 8110.8289224953|5.8394024276|12.9023569024|0.5116713352|0.1872391473| 8.1531531532| 0.1654939309|    0.4153168586|24.9234360411| 2022-8-31|            1.0|         90|  1071|\n",
      "|1.1111111111|       0.0156431646|         0.3055555556|14294.6388888889|6.8333333333|          5.0|0.7777777778| 0.235408235|        3.875| 0.2154166667|    0.4191615509|23.6666666667|2021-12-06|            0.0|          2|    36|\n",
      "| 0.764604811|        0.031361015|         0.3771477663|15252.2026086957|5.6984536082|14.5280373832| 0.631443299|0.1800931746|  5.389380531| 0.1546709622|    0.4198680733|25.1305841924| 2022-9-07|            0.0|         93|  1164|\n",
      "|0.7833333333|       0.0357992132|         0.3392156863| 5466.7223880597|5.8333333333|21.1176470588|0.5392156863| 0.192187921| 7.3236151603| 0.1549637255|    0.4176970414|24.1401960784| 2022-9-03|            1.0|         79|  1020|\n",
      "|1.0416666667|       0.0219217233|                 0.25| 2650.4166666667|6.1666666667|          1.2|        0.25|0.1327958847|          1.5| 0.1022916667|    0.4484988102|22.1666666667| 2022-6-03|            1.0|          1|    24|\n",
      "|      0.7025|       0.0263312531|         0.2691666667| 5702.8761583825|5.1641666667|         18.0|0.6066666667|0.1758793604| 7.7543859649| 0.1590216667|    0.4124599683|      25.5125| 2022-6-26|            1.0|        107|  1200|\n",
      "+------------+-------------------+---------------------+----------------+------------+-------------+------------+------------+-------------+-------------+----------------+-------------+----------+---------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basetable_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fc7ca",
   "metadata": {},
   "source": [
    "Look at the total number of observations in the basetable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58343ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9612cd2",
   "metadata": {},
   "source": [
    "## 5. split train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e147e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_emojis</th>\n",
       "      <th>avg_engagement_rate</th>\n",
       "      <th>avg_exclamation_marks</th>\n",
       "      <th>avg_followers</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_likes</th>\n",
       "      <th>avg_mentions</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_retweets</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>avg_words</th>\n",
       "      <th>date</th>\n",
       "      <th>dependent_vegan</th>\n",
       "      <th>influencers</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428018</td>\n",
       "      <td>0.030342</td>\n",
       "      <td>0.209167</td>\n",
       "      <td>7811.137549</td>\n",
       "      <td>1.506133</td>\n",
       "      <td>9.510229</td>\n",
       "      <td>1.224661</td>\n",
       "      <td>0.112772</td>\n",
       "      <td>3.362934</td>\n",
       "      <td>0.132290</td>\n",
       "      <td>0.425252</td>\n",
       "      <td>25.956101</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835217</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.354991</td>\n",
       "      <td>3640.014340</td>\n",
       "      <td>6.118644</td>\n",
       "      <td>18.886054</td>\n",
       "      <td>0.565913</td>\n",
       "      <td>0.179490</td>\n",
       "      <td>10.729231</td>\n",
       "      <td>0.165516</td>\n",
       "      <td>0.415197</td>\n",
       "      <td>25.754237</td>\n",
       "      <td>2022-8-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.352273</td>\n",
       "      <td>6038.816092</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.240729</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>0.191034</td>\n",
       "      <td>0.403160</td>\n",
       "      <td>24.806818</td>\n",
       "      <td>2022-6-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985651</td>\n",
       "      <td>0.080885</td>\n",
       "      <td>0.349890</td>\n",
       "      <td>6134.959732</td>\n",
       "      <td>6.079470</td>\n",
       "      <td>14.191095</td>\n",
       "      <td>0.614790</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>6.136778</td>\n",
       "      <td>0.152711</td>\n",
       "      <td>0.407482</td>\n",
       "      <td>25.415011</td>\n",
       "      <td>2022-8-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.830683</td>\n",
       "      <td>0.047927</td>\n",
       "      <td>0.423761</td>\n",
       "      <td>4375.890359</td>\n",
       "      <td>6.506080</td>\n",
       "      <td>12.462609</td>\n",
       "      <td>0.405051</td>\n",
       "      <td>0.214137</td>\n",
       "      <td>6.706806</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.430801</td>\n",
       "      <td>25.509822</td>\n",
       "      <td>2022-7-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_emojis  avg_engagement_rate  avg_exclamation_marks  avg_followers  \\\n",
       "0    0.428018             0.030342               0.209167    7811.137549   \n",
       "1    0.835217             0.033074               0.354991    3640.014340   \n",
       "2    0.579545             0.009035               0.352273    6038.816092   \n",
       "3    0.985651             0.080885               0.349890    6134.959732   \n",
       "4    0.830683             0.047927               0.423761    4375.890359   \n",
       "\n",
       "   avg_hashtags  avg_likes  avg_mentions  avg_polarity  avg_retweets  \\\n",
       "0      1.506133   9.510229      1.224661      0.112772      3.362934   \n",
       "1      6.118644  18.886054      0.565913      0.179490     10.729231   \n",
       "2      6.636364   5.222222      0.261364      0.240729      2.593750   \n",
       "3      6.079470  14.191095      0.614790      0.169584      6.136778   \n",
       "4      6.506080  12.462609      0.405051      0.214137      6.706806   \n",
       "\n",
       "   avg_sentiment  avg_subjectivity  avg_words        date  dependent_vegan  \\\n",
       "0       0.132290          0.425252  25.956101  2021-11-03              1.0   \n",
       "1       0.165516          0.415197  25.754237   2022-8-15              0.0   \n",
       "2       0.191034          0.403160  24.806818   2022-6-13              0.0   \n",
       "3       0.152711          0.407482  25.415011   2022-8-14              1.0   \n",
       "4       0.178300          0.430801  25.509822   2022-7-07              0.0   \n",
       "\n",
       "   influencers  volume  \n",
       "0          120    1549  \n",
       "1          119    1062  \n",
       "2            4      88  \n",
       "3           89     906  \n",
       "4           76    1069  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b32b2",
   "metadata": {},
   "source": [
    "Look at the total number of observations in the basetable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa6192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a3a93",
   "metadata": {},
   "source": [
    "We cannot use the randomsplit function, because we have time series data, so we have to use another approach\n",
    "https://towardsdatascience.com/time-series-from-scratch-train-test-splits-and-evaluation-metrics-4fd654de1b37#:~:text=Train%2Ftest%20splits%20in%20time%20series%20In%20machine%20learning%2C,dataset%20for%20testing%20and%20everything%20else%20for%20training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8ec34",
   "metadata": {},
   "source": [
    "First we look at the amount of observations that will be assigned to the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dece8543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_train = int(basetable_df.count()*0.7)\n",
    "nr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5eb92",
   "metadata": {},
   "source": [
    "convert the final basetable to a pandas dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52568ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_emojis</th>\n",
       "      <th>avg_engagement_rate</th>\n",
       "      <th>avg_exclamation_marks</th>\n",
       "      <th>avg_followers</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_likes</th>\n",
       "      <th>avg_mentions</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_retweets</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>avg_words</th>\n",
       "      <th>date</th>\n",
       "      <th>dependent_vegan</th>\n",
       "      <th>influencers</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428018</td>\n",
       "      <td>0.030342</td>\n",
       "      <td>0.209167</td>\n",
       "      <td>7811.137549</td>\n",
       "      <td>1.506133</td>\n",
       "      <td>9.510229</td>\n",
       "      <td>1.224661</td>\n",
       "      <td>0.112772</td>\n",
       "      <td>3.362934</td>\n",
       "      <td>0.132290</td>\n",
       "      <td>0.425252</td>\n",
       "      <td>25.956101</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835217</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.354991</td>\n",
       "      <td>3640.014340</td>\n",
       "      <td>6.118644</td>\n",
       "      <td>18.886054</td>\n",
       "      <td>0.565913</td>\n",
       "      <td>0.179490</td>\n",
       "      <td>10.729231</td>\n",
       "      <td>0.165516</td>\n",
       "      <td>0.415197</td>\n",
       "      <td>25.754237</td>\n",
       "      <td>2022-8-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.352273</td>\n",
       "      <td>6038.816092</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.240729</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>0.191034</td>\n",
       "      <td>0.403160</td>\n",
       "      <td>24.806818</td>\n",
       "      <td>2022-6-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985651</td>\n",
       "      <td>0.080885</td>\n",
       "      <td>0.349890</td>\n",
       "      <td>6134.959732</td>\n",
       "      <td>6.079470</td>\n",
       "      <td>14.191095</td>\n",
       "      <td>0.614790</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>6.136778</td>\n",
       "      <td>0.152711</td>\n",
       "      <td>0.407482</td>\n",
       "      <td>25.415011</td>\n",
       "      <td>2022-8-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.830683</td>\n",
       "      <td>0.047927</td>\n",
       "      <td>0.423761</td>\n",
       "      <td>4375.890359</td>\n",
       "      <td>6.506080</td>\n",
       "      <td>12.462609</td>\n",
       "      <td>0.405051</td>\n",
       "      <td>0.214137</td>\n",
       "      <td>6.706806</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.430801</td>\n",
       "      <td>25.509822</td>\n",
       "      <td>2022-7-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_emojis  avg_engagement_rate  avg_exclamation_marks  avg_followers  \\\n",
       "0    0.428018             0.030342               0.209167    7811.137549   \n",
       "1    0.835217             0.033074               0.354991    3640.014340   \n",
       "2    0.579545             0.009035               0.352273    6038.816092   \n",
       "3    0.985651             0.080885               0.349890    6134.959732   \n",
       "4    0.830683             0.047927               0.423761    4375.890359   \n",
       "\n",
       "   avg_hashtags  avg_likes  avg_mentions  avg_polarity  avg_retweets  \\\n",
       "0      1.506133   9.510229      1.224661      0.112772      3.362934   \n",
       "1      6.118644  18.886054      0.565913      0.179490     10.729231   \n",
       "2      6.636364   5.222222      0.261364      0.240729      2.593750   \n",
       "3      6.079470  14.191095      0.614790      0.169584      6.136778   \n",
       "4      6.506080  12.462609      0.405051      0.214137      6.706806   \n",
       "\n",
       "   avg_sentiment  avg_subjectivity  avg_words        date  dependent_vegan  \\\n",
       "0       0.132290          0.425252  25.956101  2021-11-03              1.0   \n",
       "1       0.165516          0.415197  25.754237   2022-8-15              0.0   \n",
       "2       0.191034          0.403160  24.806818   2022-6-13              0.0   \n",
       "3       0.152711          0.407482  25.415011   2022-8-14              1.0   \n",
       "4       0.178300          0.430801  25.509822   2022-7-07              0.0   \n",
       "\n",
       "   influencers  volume  \n",
       "0          120    1549  \n",
       "1          119    1062  \n",
       "2            4      88  \n",
       "3           89     906  \n",
       "4           76    1069  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_pd = basetable_df.toPandas()\n",
    "basetable_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cbb6a",
   "metadata": {},
   "source": [
    "Removing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f886731",
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable_pd = basetable_pd.drop(['date'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaf526",
   "metadata": {},
   "source": [
    "Split the dataframe into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be927ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = basetable_pd.iloc[:nr_train,:]\n",
    "test_pd = basetable_pd.iloc[nr_train:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a577dee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_emojis</th>\n",
       "      <th>avg_engagement_rate</th>\n",
       "      <th>avg_exclamation_marks</th>\n",
       "      <th>avg_followers</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_likes</th>\n",
       "      <th>avg_mentions</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_retweets</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>avg_words</th>\n",
       "      <th>dependent_vegan</th>\n",
       "      <th>influencers</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428018</td>\n",
       "      <td>0.030342</td>\n",
       "      <td>0.209167</td>\n",
       "      <td>7811.137549</td>\n",
       "      <td>1.506133</td>\n",
       "      <td>9.510229</td>\n",
       "      <td>1.224661</td>\n",
       "      <td>0.112772</td>\n",
       "      <td>3.362934</td>\n",
       "      <td>0.132290</td>\n",
       "      <td>0.425252</td>\n",
       "      <td>25.956101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835217</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.354991</td>\n",
       "      <td>3640.014340</td>\n",
       "      <td>6.118644</td>\n",
       "      <td>18.886054</td>\n",
       "      <td>0.565913</td>\n",
       "      <td>0.179490</td>\n",
       "      <td>10.729231</td>\n",
       "      <td>0.165516</td>\n",
       "      <td>0.415197</td>\n",
       "      <td>25.754237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.352273</td>\n",
       "      <td>6038.816092</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.240729</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>0.191034</td>\n",
       "      <td>0.403160</td>\n",
       "      <td>24.806818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985651</td>\n",
       "      <td>0.080885</td>\n",
       "      <td>0.349890</td>\n",
       "      <td>6134.959732</td>\n",
       "      <td>6.079470</td>\n",
       "      <td>14.191095</td>\n",
       "      <td>0.614790</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>6.136778</td>\n",
       "      <td>0.152711</td>\n",
       "      <td>0.407482</td>\n",
       "      <td>25.415011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.830683</td>\n",
       "      <td>0.047927</td>\n",
       "      <td>0.423761</td>\n",
       "      <td>4375.890359</td>\n",
       "      <td>6.506080</td>\n",
       "      <td>12.462609</td>\n",
       "      <td>0.405051</td>\n",
       "      <td>0.214137</td>\n",
       "      <td>6.706806</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.430801</td>\n",
       "      <td>25.509822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_emojis  avg_engagement_rate  avg_exclamation_marks  avg_followers  \\\n",
       "0    0.428018             0.030342               0.209167    7811.137549   \n",
       "1    0.835217             0.033074               0.354991    3640.014340   \n",
       "2    0.579545             0.009035               0.352273    6038.816092   \n",
       "3    0.985651             0.080885               0.349890    6134.959732   \n",
       "4    0.830683             0.047927               0.423761    4375.890359   \n",
       "\n",
       "   avg_hashtags  avg_likes  avg_mentions  avg_polarity  avg_retweets  \\\n",
       "0      1.506133   9.510229      1.224661      0.112772      3.362934   \n",
       "1      6.118644  18.886054      0.565913      0.179490     10.729231   \n",
       "2      6.636364   5.222222      0.261364      0.240729      2.593750   \n",
       "3      6.079470  14.191095      0.614790      0.169584      6.136778   \n",
       "4      6.506080  12.462609      0.405051      0.214137      6.706806   \n",
       "\n",
       "   avg_sentiment  avg_subjectivity  avg_words  dependent_vegan  influencers  \\\n",
       "0       0.132290          0.425252  25.956101              1.0          120   \n",
       "1       0.165516          0.415197  25.754237              0.0          119   \n",
       "2       0.191034          0.403160  24.806818              0.0            4   \n",
       "3       0.152711          0.407482  25.415011              1.0           89   \n",
       "4       0.178300          0.430801  25.509822              0.0           76   \n",
       "\n",
       "   volume  \n",
       "0    1549  \n",
       "1    1062  \n",
       "2      88  \n",
       "3     906  \n",
       "4    1069  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19a50f",
   "metadata": {},
   "source": [
    "Convert the pandas dataframe back to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c37906",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o50.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\session.py:1161\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1165\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m )\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\session.py:1206\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\session.py:853\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    850\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 853\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    855\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\session.py:725\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    724\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m--> 725\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacyInferArrayTypeFromFirstElement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m    727\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[0;32m    728\u001b[0m     _merge_type,\n\u001b[0;32m    729\u001b[0m     (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     ),\n\u001b[0;32m    739\u001b[0m )\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:199\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    201\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling o50.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n\n"
     ]
    }
   ],
   "source": [
    "train = spark.createDataFrame(train_pd)\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56110f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = spark.createDataFrame(test_pd)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690b548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get number in observations in each set\n",
    "print(\"Number of observations train: %s\" %train.count())\n",
    "print(\"Number of observations test: %s\" %test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2dfd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get distribution of dependent variable within each set\n",
    "train.groupBy(\"dependent\").count().show()\n",
    "test.groupBy(\"dependent\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32404f2",
   "metadata": {},
   "source": [
    "To handle the class imbalance in the dataset, we will calculate the weights for each lable. The formula to calculate these weights was found on the following source:\n",
    "https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_1 = 140/(33 * 2)\n",
    "weight_0 = 140/(107*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cc652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(weight_1)\n",
    "print(weight_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class weights column\n",
    "train = train.withColumn(\"weight\", F.when(F.col(\"dependent\") == 1, weight_1).otherwise(weight_0))\n",
    "test = test.withColumn(\"weight\", F.when(F.col(\"dependent\") == 1, weight_1).otherwise(weight_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f926f",
   "metadata": {},
   "source": [
    "# 6. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e7ec9",
   "metadata": {},
   "source": [
    "## 6.1 Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, GBTClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376476db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexer (IDX)\n",
    "IDX = StringIndexer(inputCol=\"dependent\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the numeric features\n",
    "numFeatureCols = [\"avg_emojis\", \"avg_engagement_rate\", \"avg_exclamation_marks\", \"avg_hashtags\", \"avg_num_mentions\", \"avg_words\", \"number_of_favorites\", \"number_of_followers\", \"number_of_retweets\", \"number_tweets\"]\n",
    "\n",
    "# define vector assembler\n",
    "VA_num = VectorAssembler(inputCols=numFeatureCols, outputCol=\"numeric_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standard scaler (SS)\n",
    "SS = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest we don't scale the features \n",
    "VA_RF = VectorAssembler(inputCols=[\"avg_emojis\", \"avg_engagement_rate\", \"avg_exclamation_marks\", \"avg_hashtags\", \"avg_num_mentions\", \"avg_words\", \"number_of_favorites\", \"number_of_followers\", \"number_of_retweets\", \"number_tweets\", \"avg_polarity\", \"avg_subjectivity\"], outputCol=\"featuresrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a75d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector assembler (VA_all)\n",
    "VA_all = VectorAssembler(inputCols=[\"scaled_numeric_features\", \"avg_polarity\", \"avg_subjectivity\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d02332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logistic regression model\n",
    "LR = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decision tree model\n",
    "DT = DecisionTreeClassifier(featuresCol = \"features\", labelCol = \"label\", weightCol= \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decision tree model\n",
    "GBT = GBTClassifier(featuresCol = \"features\", labelCol = \"label\", weightCol= \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest model\n",
    "RF = RandomForestClassifier(featuresCol = \"featuresrf\", labelCol = \"label\", weightCol= \"weight\", numTrees= 200, featureSubsetStrategy= 'all', maxDepth= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3b110",
   "metadata": {},
   "source": [
    "## 6.2 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training data\n",
    "LR_pipeline = Pipeline().setStages([IDX, VA_num, SS, VA_all, LR]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdc0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "LR_preds = LR_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training data\n",
    "DT_pipeline = Pipeline().setStages([IDX, VA_num, SS, VA_all, DT]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "DT_preds = DT_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training data\n",
    "GBT_pipeline = Pipeline().setStages([IDX, VA_num, SS, VA_all, GBT]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb63eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "GBT_preds = GBT_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on training data\n",
    "RF_pipeline = Pipeline().setStages([IDX, VA_RF, RF]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "RF_preds = RF_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf5b92",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05200a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator (for AUC)\n",
    "evaluator_auc = BinaryClassificationEvaluator()\n",
    "\n",
    "# define evaluator (for other metrics)\n",
    "evaluator_mc = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff360e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for LR model\n",
    "lr_f1 = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"f1\"})\n",
    "lr_accuracy = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"accuracy\"})\n",
    "lr_recall = evaluator_mc.evaluate(LR_preds, {evaluator_mc.metricName: \"recallByLabel\"})\n",
    "lr_auc = evaluator_auc.evaluate(LR_preds, {evaluator_auc.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for DT model\n",
    "dt_f1 = evaluator_mc.evaluate(DT_preds, {evaluator_mc.metricName: \"f1\"})\n",
    "dt_accuracy = evaluator_mc.evaluate(DT_preds, {evaluator_mc.metricName: \"accuracy\"})\n",
    "dt_recall = evaluator_mc.evaluate(DT_preds, {evaluator_mc.metricName: \"recallByLabel\"})\n",
    "dt_auc = evaluator_auc.evaluate(DT_preds, {evaluator_auc.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31356358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for GBT model\n",
    "gbt_f1 = evaluator_mc.evaluate(GBT_preds, {evaluator_mc.metricName: \"f1\"})\n",
    "gbt_accuracy = evaluator_mc.evaluate(GBT_preds, {evaluator_mc.metricName: \"accuracy\"})\n",
    "gbt_recall = evaluator_mc.evaluate(GBT_preds, {evaluator_mc.metricName: \"recallByLabel\"})\n",
    "gbt_auc = evaluator_auc.evaluate(GBT_preds, {evaluator_auc.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2465cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for RF model\n",
    "rf_f1 = evaluator_mc.evaluate(RF_preds, {evaluator_mc.metricName: \"f1\"})\n",
    "rf_accuracy = evaluator_mc.evaluate(RF_preds, {evaluator_mc.metricName: \"accuracy\"})\n",
    "rf_recall = evaluator_mc.evaluate(RF_preds, {evaluator_mc.metricName: \"recallByLabel\"})\n",
    "rf_auc = evaluator_auc.evaluate(RF_preds, {evaluator_auc.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931affdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"LOGISTIC REGRESSION:\")\n",
    "print('  F1       : %g' % lr_f1)\n",
    "print('  ACCURACY : %g' % lr_accuracy)\n",
    "print('  RECALL   : %g' % lr_recall)\n",
    "print('  AUC      : %g' % lr_auc)\n",
    "print(\"------------------\")\n",
    "print(\"SINGLE DECISION TREE:\")\n",
    "print('  F1       : %g' % dt_f1)\n",
    "print('  ACCURACY : %g' % dt_accuracy)\n",
    "print('  RECALL   : %g' % dt_recall)\n",
    "print('  AUC      : %g' % dt_auc)\n",
    "print(\"------------------\")\n",
    "print(\"GRADIENT-BOOSTED TREES:\")\n",
    "print('  F1       : %g' % gbt_f1)\n",
    "print('  ACCURACY : %g' % gbt_accuracy)\n",
    "print('  RECALL   : %g' % gbt_recall)\n",
    "print('  AUC      : %g' % gbt_auc)\n",
    "print(\"------------------\")\n",
    "print(\"RANDOM FOREST:\")\n",
    "print('  F1       : %g' % rf_f1)\n",
    "print('  ACCURACY : %g' % rf_accuracy)\n",
    "print('  RECALL   : %g' % rf_recall)\n",
    "print('  AUC      : %g' % rf_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b950f",
   "metadata": {},
   "source": [
    "# 8 Plot model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353ef3f",
   "metadata": {},
   "source": [
    "## 8.1 Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db71d0d",
   "metadata": {},
   "source": [
    "We plot the confussion matrix for the model with the highest AUC, this is the random forest model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# get predictions and labels\n",
    "preds_and_labels = RF_preds.select(['prediction','label']) \\\n",
    "                                  .withColumn('label', F.col('label').cast(FloatType())) \\\n",
    "                                  .orderBy('prediction') \\\n",
    "                                  .toPandas()\n",
    "\n",
    "# get confusion matrix\n",
    "cm = confusion_matrix(preds_and_labels[\"label\"], preds_and_labels[\"prediction\"], labels=[0, 1])\n",
    "# get confusion matrix figure\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "# plot figure\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5d4ee",
   "metadata": {},
   "source": [
    "Calculate sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0269b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, recall_score\n",
    "recall_sensitivity = metrics.recall_score(preds_and_labels[\"label\"], preds_and_labels[\"prediction\"], pos_label=1)\n",
    "recall_specificity = metrics.recall_score(preds_and_labels[\"label\"], preds_and_labels[\"prediction\"], pos_label=0)\n",
    "recall_sensitivity, recall_specificity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19aacb",
   "metadata": {},
   "source": [
    "## 8.2 Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1021b49",
   "metadata": {},
   "source": [
    "We plot the future importance for the model with the highest AUC, this is the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb064b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect feature importance\n",
    "feature_importance = RF_pipeline.stages[-1].featureImportances.toArray()\n",
    "# define all the features\n",
    "all_feature_names = numFeatureCols + [\"avg_polarity\", \"avg_subjectivity\"] \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(x=range(len(feature_importance)), height=feature_importance)\n",
    "plt.xticks(range(len(feature_importance)), all_feature_names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fc250",
   "metadata": {},
   "source": [
    "## 8.3 ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3ece0",
   "metadata": {},
   "source": [
    "We will plot the ROC curve for the model with the highest AUC, this is the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba58f99",
   "metadata": {},
   "source": [
    "We will use plotty to plot the ROC curve, so we will have to perform some data type transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the rf predictions to a pandas dataframe\n",
    "RF_preds_pd = RF_preds.toPandas()\n",
    "RF_preds_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function that will select the probabilities for value 1\n",
    "def select1(column):\n",
    "    one = column[1]\n",
    "    return one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de45eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we apply this function to our probabilities \n",
    "Yscore = RF_preds_pd['probability'].apply(lambda x: select1(x)).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yscore contains the probabilities of our predictions to be 1\n",
    "Yscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yscore is of the correct type to perform the plot \n",
    "type(Yscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y is the real value of our dependent\n",
    "Y = test.select('dependent').toPandas()\n",
    "Y = Y['dependent'].to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703699ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yscore is of the correct type to perform the plot \n",
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the roc_curve function to predict the false-positive, true-positive rate and thresholds based on the Y an Y score\n",
    "fpr, tpr, thresholds = roc_curve(Y, Yscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
