{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76508cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "# initialize findspark with spark directory\n",
    "findspark.init(\"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\")\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99192d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dafa38",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dcdae",
   "metadata": {},
   "source": [
    "In this notebook we will buid a model that predicts if the trend of a certain topic goes up or down on a certain day based on Twitter data of that day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1943e2",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa5e6",
   "metadata": {},
   "source": [
    "### 1.1 Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a3d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read trend data \n",
    "trend = spark.read.csv(\".././../data/Google_trends/daily_trends.csv\", header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6b34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|               date|dependent_vegan|\n",
      "+-------------------+---------------+\n",
      "|2021-10-04 00:00:00|              0|\n",
      "|2021-10-05 00:00:00|              1|\n",
      "|2021-10-06 00:00:00|              1|\n",
      "|2021-10-07 00:00:00|              1|\n",
      "|2021-10-08 00:00:00|              1|\n",
      "|2021-10-09 00:00:00|              1|\n",
      "|2021-10-10 00:00:00|              0|\n",
      "|2021-10-11 00:00:00|              0|\n",
      "|2021-10-12 00:00:00|              0|\n",
      "|2021-10-13 00:00:00|              0|\n",
      "|2021-10-14 00:00:00|              0|\n",
      "|2021-10-15 00:00:00|              0|\n",
      "|2021-10-16 00:00:00|              1|\n",
      "|2021-10-17 00:00:00|              1|\n",
      "|2021-10-18 00:00:00|              0|\n",
      "|2021-10-19 00:00:00|              1|\n",
      "|2021-10-20 00:00:00|              0|\n",
      "|2021-10-21 00:00:00|              0|\n",
      "|2021-10-22 00:00:00|              1|\n",
      "|2021-10-23 00:00:00|              1|\n",
      "|2021-10-24 00:00:00|              1|\n",
      "|2021-10-25 00:00:00|              0|\n",
      "|2021-10-26 00:00:00|              0|\n",
      "|2021-10-27 00:00:00|              0|\n",
      "|2021-10-28 00:00:00|              0|\n",
      "|2021-10-29 00:00:00|              1|\n",
      "|2021-10-30 00:00:00|              1|\n",
      "|2021-10-31 00:00:00|              1|\n",
      "|2021-11-01 00:00:00|              0|\n",
      "|2021-11-02 00:00:00|              0|\n",
      "|2021-11-03 00:00:00|              1|\n",
      "|2021-11-04 00:00:00|              1|\n",
      "|2021-11-05 00:00:00|              0|\n",
      "|2021-11-06 00:00:00|              1|\n",
      "|2021-11-07 00:00:00|              0|\n",
      "|2021-11-08 00:00:00|              0|\n",
      "|2021-11-09 00:00:00|              1|\n",
      "|2021-11-10 00:00:00|              0|\n",
      "|2021-11-11 00:00:00|              1|\n",
      "|2021-11-12 00:00:00|              0|\n",
      "|2021-11-13 00:00:00|              1|\n",
      "|2021-11-14 00:00:00|              1|\n",
      "|2021-11-15 00:00:00|              0|\n",
      "|2021-11-16 00:00:00|              1|\n",
      "|2021-11-17 00:00:00|              0|\n",
      "|2021-11-18 00:00:00|              1|\n",
      "|2021-11-19 00:00:00|              0|\n",
      "|2021-11-20 00:00:00|              1|\n",
      "|2021-11-21 00:00:00|              1|\n",
      "|2021-11-22 00:00:00|              0|\n",
      "|2021-11-23 00:00:00|              1|\n",
      "|2021-11-24 00:00:00|              1|\n",
      "|2021-11-25 00:00:00|              1|\n",
      "|2021-11-26 00:00:00|              0|\n",
      "|2021-11-27 00:00:00|              1|\n",
      "|2021-11-28 00:00:00|              0|\n",
      "|2021-11-29 00:00:00|              0|\n",
      "|2021-11-30 00:00:00|              0|\n",
      "|2021-12-01 00:00:00|              1|\n",
      "|2021-12-02 00:00:00|              1|\n",
      "|2021-12-03 00:00:00|              0|\n",
      "|2021-12-04 00:00:00|              1|\n",
      "|2021-12-05 00:00:00|              0|\n",
      "|2021-12-06 00:00:00|              0|\n",
      "|2021-12-07 00:00:00|              0|\n",
      "|2021-12-08 00:00:00|              1|\n",
      "|2021-12-09 00:00:00|              0|\n",
      "|2021-12-10 00:00:00|              0|\n",
      "|2021-12-11 00:00:00|              1|\n",
      "|2021-12-12 00:00:00|              0|\n",
      "|2021-12-13 00:00:00|              0|\n",
      "|2021-12-14 00:00:00|              0|\n",
      "|2021-12-15 00:00:00|              0|\n",
      "|2021-12-16 00:00:00|              1|\n",
      "|2021-12-17 00:00:00|              0|\n",
      "|2021-12-18 00:00:00|              1|\n",
      "|2021-12-19 00:00:00|              1|\n",
      "|2021-12-20 00:00:00|              0|\n",
      "|2021-12-21 00:00:00|              0|\n",
      "|2021-12-22 00:00:00|              1|\n",
      "|2021-12-23 00:00:00|              1|\n",
      "|2021-12-24 00:00:00|              1|\n",
      "|2021-12-25 00:00:00|              0|\n",
      "|2021-12-26 00:00:00|              0|\n",
      "|2021-12-27 00:00:00|              0|\n",
      "|2021-12-28 00:00:00|              1|\n",
      "|2021-12-29 00:00:00|              1|\n",
      "|2021-12-30 00:00:00|              0|\n",
      "|2021-12-31 00:00:00|              1|\n",
      "|2022-01-01 00:00:00|              1|\n",
      "|2022-01-02 00:00:00|              1|\n",
      "|2022-01-03 00:00:00|              0|\n",
      "|2022-01-04 00:00:00|              0|\n",
      "|2022-01-05 00:00:00|              1|\n",
      "|2022-01-06 00:00:00|              0|\n",
      "|2022-01-07 00:00:00|              1|\n",
      "|2022-01-08 00:00:00|              1|\n",
      "|2022-01-09 00:00:00|              0|\n",
      "|2022-01-10 00:00:00|              0|\n",
      "|2022-01-11 00:00:00|              1|\n",
      "|2022-01-12 00:00:00|              1|\n",
      "|2022-01-13 00:00:00|              1|\n",
      "|2022-01-14 00:00:00|              0|\n",
      "|2022-01-15 00:00:00|              1|\n",
      "|2022-01-16 00:00:00|              0|\n",
      "|2022-01-17 00:00:00|              0|\n",
      "|2022-01-18 00:00:00|              0|\n",
      "|2022-01-19 00:00:00|              1|\n",
      "|2022-01-20 00:00:00|              0|\n",
      "|2022-01-21 00:00:00|              1|\n",
      "|2022-01-22 00:00:00|              1|\n",
      "|2022-01-23 00:00:00|              1|\n",
      "|2022-01-24 00:00:00|              0|\n",
      "|2022-01-25 00:00:00|              0|\n",
      "|2022-01-26 00:00:00|              1|\n",
      "|2022-01-27 00:00:00|              0|\n",
      "|2022-01-28 00:00:00|              1|\n",
      "|2022-01-29 00:00:00|              1|\n",
      "|2022-01-30 00:00:00|              0|\n",
      "|2022-01-31 00:00:00|              0|\n",
      "|2022-02-01 00:00:00|              0|\n",
      "|2022-02-02 00:00:00|              0|\n",
      "|2022-02-03 00:00:00|              0|\n",
      "|2022-02-04 00:00:00|              1|\n",
      "|2022-02-05 00:00:00|              1|\n",
      "|2022-02-06 00:00:00|              0|\n",
      "|2022-02-07 00:00:00|              0|\n",
      "|2022-02-08 00:00:00|              0|\n",
      "|2022-02-09 00:00:00|              0|\n",
      "|2022-02-10 00:00:00|              0|\n",
      "|2022-02-11 00:00:00|              1|\n",
      "|2022-02-12 00:00:00|              1|\n",
      "|2022-02-13 00:00:00|              1|\n",
      "|2022-02-14 00:00:00|              0|\n",
      "|2022-02-15 00:00:00|              0|\n",
      "|2022-02-16 00:00:00|              0|\n",
      "|2022-02-17 00:00:00|              0|\n",
      "|2022-02-18 00:00:00|              1|\n",
      "|2022-02-19 00:00:00|              1|\n",
      "|2022-02-20 00:00:00|              0|\n",
      "|2022-02-21 00:00:00|              0|\n",
      "|2022-02-22 00:00:00|              0|\n",
      "|2022-02-23 00:00:00|              1|\n",
      "|2022-02-24 00:00:00|              0|\n",
      "|2022-02-25 00:00:00|              1|\n",
      "|2022-02-26 00:00:00|              1|\n",
      "|2022-02-27 00:00:00|              1|\n",
      "|2022-02-28 00:00:00|              0|\n",
      "|2022-03-01 00:00:00|              0|\n",
      "|2022-03-02 00:00:00|              0|\n",
      "|2022-03-03 00:00:00|              0|\n",
      "|2022-03-04 00:00:00|              1|\n",
      "|2022-03-05 00:00:00|              1|\n",
      "|2022-03-06 00:00:00|              1|\n",
      "|2022-03-07 00:00:00|              0|\n",
      "|2022-03-08 00:00:00|              1|\n",
      "|2022-03-09 00:00:00|              0|\n",
      "|2022-03-10 00:00:00|              1|\n",
      "|2022-03-11 00:00:00|              1|\n",
      "|2022-03-12 00:00:00|              1|\n",
      "|2022-03-13 00:00:00|              1|\n",
      "|2022-03-14 00:00:00|              1|\n",
      "|2022-03-15 00:00:00|              0|\n",
      "|2022-03-16 00:00:00|              1|\n",
      "|2022-03-17 00:00:00|              1|\n",
      "|2022-03-18 00:00:00|              1|\n",
      "|2022-03-19 00:00:00|              1|\n",
      "|2022-03-20 00:00:00|              1|\n",
      "|2022-03-21 00:00:00|              0|\n",
      "|2022-03-22 00:00:00|              0|\n",
      "|2022-03-23 00:00:00|              1|\n",
      "|2022-03-24 00:00:00|              1|\n",
      "|2022-03-25 00:00:00|              0|\n",
      "|2022-03-26 00:00:00|              1|\n",
      "|2022-03-27 00:00:00|              0|\n",
      "|2022-03-28 00:00:00|              0|\n",
      "|2022-03-29 00:00:00|              1|\n",
      "|2022-03-30 00:00:00|              1|\n",
      "|2022-03-31 00:00:00|              0|\n",
      "|2022-04-01 00:00:00|              1|\n",
      "|2022-04-02 00:00:00|              1|\n",
      "|2022-04-03 00:00:00|              0|\n",
      "|2022-04-04 00:00:00|              0|\n",
      "|2022-04-05 00:00:00|              0|\n",
      "|2022-04-06 00:00:00|              1|\n",
      "|2022-04-07 00:00:00|              0|\n",
      "|2022-04-08 00:00:00|              1|\n",
      "|2022-04-09 00:00:00|              1|\n",
      "|2022-04-10 00:00:00|              1|\n",
      "|2022-04-11 00:00:00|              0|\n",
      "|2022-04-12 00:00:00|              0|\n",
      "|2022-04-13 00:00:00|              1|\n",
      "|2022-04-14 00:00:00|              0|\n",
      "|2022-04-15 00:00:00|              1|\n",
      "|2022-04-16 00:00:00|              1|\n",
      "|2022-04-17 00:00:00|              0|\n",
      "|2022-04-18 00:00:00|              0|\n",
      "|2022-04-19 00:00:00|              1|\n",
      "|2022-04-20 00:00:00|              0|\n",
      "|2022-04-21 00:00:00|              1|\n",
      "|2022-04-22 00:00:00|              1|\n",
      "|2022-04-23 00:00:00|              1|\n",
      "|2022-04-24 00:00:00|              1|\n",
      "|2022-04-25 00:00:00|              0|\n",
      "|2022-04-26 00:00:00|              0|\n",
      "|2022-04-27 00:00:00|              1|\n",
      "|2022-04-28 00:00:00|              0|\n",
      "|2022-04-29 00:00:00|              1|\n",
      "|2022-04-30 00:00:00|              1|\n",
      "|2022-05-01 00:00:00|              1|\n",
      "|2022-05-02 00:00:00|              0|\n",
      "|2022-05-03 00:00:00|              0|\n",
      "|2022-05-04 00:00:00|              0|\n",
      "|2022-05-05 00:00:00|              0|\n",
      "|2022-05-06 00:00:00|              1|\n",
      "|2022-05-07 00:00:00|              1|\n",
      "|2022-05-08 00:00:00|              1|\n",
      "|2022-05-09 00:00:00|              0|\n",
      "|2022-05-10 00:00:00|              0|\n",
      "|2022-05-11 00:00:00|              0|\n",
      "|2022-05-12 00:00:00|              1|\n",
      "|2022-05-13 00:00:00|              0|\n",
      "|2022-05-14 00:00:00|              1|\n",
      "|2022-05-15 00:00:00|              0|\n",
      "|2022-05-16 00:00:00|              0|\n",
      "|2022-05-17 00:00:00|              0|\n",
      "|2022-05-18 00:00:00|              1|\n",
      "|2022-05-19 00:00:00|              0|\n",
      "|2022-05-20 00:00:00|              1|\n",
      "|2022-05-21 00:00:00|              1|\n",
      "|2022-05-22 00:00:00|              1|\n",
      "|2022-05-23 00:00:00|              0|\n",
      "|2022-05-24 00:00:00|              1|\n",
      "|2022-05-25 00:00:00|              0|\n",
      "|2022-05-26 00:00:00|              1|\n",
      "|2022-05-27 00:00:00|              1|\n",
      "|2022-05-28 00:00:00|              1|\n",
      "|2022-05-29 00:00:00|              1|\n",
      "|2022-05-30 00:00:00|              0|\n",
      "|2022-05-31 00:00:00|              0|\n",
      "|2022-06-01 00:00:00|              1|\n",
      "|2022-06-02 00:00:00|              0|\n",
      "|2022-06-03 00:00:00|              1|\n",
      "|2022-06-04 00:00:00|              1|\n",
      "|2022-06-05 00:00:00|              0|\n",
      "|2022-06-06 00:00:00|              0|\n",
      "|2022-06-07 00:00:00|              0|\n",
      "|2022-06-08 00:00:00|              1|\n",
      "|2022-06-09 00:00:00|              0|\n",
      "|2022-06-10 00:00:00|              1|\n",
      "|2022-06-11 00:00:00|              1|\n",
      "|2022-06-12 00:00:00|              1|\n",
      "|2022-06-13 00:00:00|              0|\n",
      "|2022-06-14 00:00:00|              0|\n",
      "|2022-06-15 00:00:00|              1|\n",
      "|2022-06-16 00:00:00|              0|\n",
      "|2022-06-17 00:00:00|              0|\n",
      "|2022-06-18 00:00:00|              1|\n",
      "|2022-06-19 00:00:00|              1|\n",
      "|2022-06-20 00:00:00|              0|\n",
      "|2022-06-21 00:00:00|              0|\n",
      "|2022-06-22 00:00:00|              0|\n",
      "|2022-06-23 00:00:00|              0|\n",
      "|2022-06-24 00:00:00|              1|\n",
      "|2022-06-25 00:00:00|              1|\n",
      "|2022-06-26 00:00:00|              1|\n",
      "|2022-06-27 00:00:00|              0|\n",
      "|2022-06-28 00:00:00|              1|\n",
      "|2022-06-29 00:00:00|              0|\n",
      "|2022-06-30 00:00:00|              1|\n",
      "|2022-07-01 00:00:00|              1|\n",
      "|2022-07-02 00:00:00|              1|\n",
      "|2022-07-03 00:00:00|              1|\n",
      "|2022-07-04 00:00:00|              0|\n",
      "|2022-07-05 00:00:00|              0|\n",
      "|2022-07-06 00:00:00|              1|\n",
      "|2022-07-07 00:00:00|              0|\n",
      "|2022-07-08 00:00:00|              1|\n",
      "|2022-07-09 00:00:00|              1|\n",
      "|2022-07-10 00:00:00|              1|\n",
      "|2022-07-11 00:00:00|              0|\n",
      "|2022-07-12 00:00:00|              0|\n",
      "|2022-07-13 00:00:00|              1|\n",
      "|2022-07-14 00:00:00|              1|\n",
      "|2022-07-15 00:00:00|              1|\n",
      "|2022-07-16 00:00:00|              1|\n",
      "|2022-07-17 00:00:00|              0|\n",
      "|2022-07-18 00:00:00|              0|\n",
      "|2022-07-19 00:00:00|              0|\n",
      "|2022-07-20 00:00:00|              1|\n",
      "|2022-07-21 00:00:00|              0|\n",
      "|2022-07-22 00:00:00|              1|\n",
      "|2022-07-23 00:00:00|              1|\n",
      "|2022-07-24 00:00:00|              1|\n",
      "|2022-07-25 00:00:00|              0|\n",
      "|2022-07-26 00:00:00|              1|\n",
      "|2022-07-27 00:00:00|              0|\n",
      "|2022-07-28 00:00:00|              1|\n",
      "|2022-07-29 00:00:00|              1|\n",
      "|2022-07-30 00:00:00|              1|\n",
      "|2022-07-31 00:00:00|              0|\n",
      "|2022-08-01 00:00:00|              0|\n",
      "|2022-08-02 00:00:00|              0|\n",
      "|2022-08-03 00:00:00|              1|\n",
      "|2022-08-04 00:00:00|              1|\n",
      "|2022-08-05 00:00:00|              1|\n",
      "|2022-08-06 00:00:00|              1|\n",
      "|2022-08-07 00:00:00|              1|\n",
      "|2022-08-08 00:00:00|              0|\n",
      "|2022-08-09 00:00:00|              0|\n",
      "|2022-08-10 00:00:00|              1|\n",
      "|2022-08-11 00:00:00|              0|\n",
      "|2022-08-12 00:00:00|              1|\n",
      "|2022-08-13 00:00:00|              1|\n",
      "|2022-08-14 00:00:00|              1|\n",
      "|2022-08-15 00:00:00|              0|\n",
      "|2022-08-16 00:00:00|              0|\n",
      "|2022-08-17 00:00:00|              1|\n",
      "|2022-08-18 00:00:00|              0|\n",
      "|2022-08-19 00:00:00|              1|\n",
      "|2022-08-20 00:00:00|              1|\n",
      "|2022-08-21 00:00:00|              1|\n",
      "|2022-08-22 00:00:00|              0|\n",
      "|2022-08-23 00:00:00|              0|\n",
      "|2022-08-24 00:00:00|              0|\n",
      "|2022-08-25 00:00:00|              0|\n",
      "|2022-08-26 00:00:00|              1|\n",
      "|2022-08-27 00:00:00|              1|\n",
      "|2022-08-28 00:00:00|              1|\n",
      "|2022-08-29 00:00:00|              0|\n",
      "|2022-08-30 00:00:00|              1|\n",
      "|2022-08-31 00:00:00|              1|\n",
      "|2022-09-01 00:00:00|              1|\n",
      "|2022-09-02 00:00:00|              1|\n",
      "|2022-09-03 00:00:00|              1|\n",
      "|2022-09-04 00:00:00|              0|\n",
      "|2022-09-05 00:00:00|              0|\n",
      "|2022-09-06 00:00:00|              0|\n",
      "|2022-09-07 00:00:00|              0|\n",
      "|2022-09-08 00:00:00|              1|\n",
      "|2022-09-09 00:00:00|              1|\n",
      "|2022-09-10 00:00:00|              1|\n",
      "|2022-09-11 00:00:00|              0|\n",
      "|2022-09-12 00:00:00|              0|\n",
      "|2022-09-13 00:00:00|              1|\n",
      "|2022-09-14 00:00:00|              0|\n",
      "|2022-09-15 00:00:00|              0|\n",
      "|2022-09-16 00:00:00|              1|\n",
      "|2022-09-17 00:00:00|              1|\n",
      "|2022-09-18 00:00:00|              1|\n",
      "|2022-09-19 00:00:00|              0|\n",
      "|2022-09-20 00:00:00|              1|\n",
      "|2022-09-21 00:00:00|              0|\n",
      "|2022-09-22 00:00:00|              0|\n",
      "|2022-09-23 00:00:00|              1|\n",
      "|2022-09-24 00:00:00|              1|\n",
      "|2022-09-25 00:00:00|              1|\n",
      "|2022-09-26 00:00:00|              0|\n",
      "|2022-09-27 00:00:00|              0|\n",
      "|2022-09-28 00:00:00|              1|\n",
      "|2022-09-29 00:00:00|              0|\n",
      "|2022-09-30 00:00:00|              1|\n",
      "|2022-10-01 00:00:00|              1|\n",
      "|2022-10-02 00:00:00|              1|\n",
      "|2022-10-03 00:00:00|              0|\n",
      "|2022-10-04 00:00:00|              1|\n",
      "|2022-10-05 00:00:00|              0|\n",
      "|2022-10-06 00:00:00|              0|\n",
      "|2022-10-07 00:00:00|              1|\n",
      "|2022-10-08 00:00:00|              1|\n",
      "|2022-10-09 00:00:00|              0|\n",
      "|2022-10-10 00:00:00|              0|\n",
      "|2022-10-11 00:00:00|              0|\n",
      "|2022-10-12 00:00:00|              1|\n",
      "|2022-10-13 00:00:00|              0|\n",
      "|2022-10-14 00:00:00|              1|\n",
      "|2022-10-15 00:00:00|              1|\n",
      "|2022-10-16 00:00:00|              1|\n",
      "|2022-10-17 00:00:00|              0|\n",
      "|2022-10-18 00:00:00|              0|\n",
      "|2022-10-19 00:00:00|              1|\n",
      "|2022-10-20 00:00:00|              1|\n",
      "|2022-10-21 00:00:00|              0|\n",
      "|2022-10-22 00:00:00|              1|\n",
      "|2022-10-23 00:00:00|              1|\n",
      "|2022-10-24 00:00:00|              0|\n",
      "|2022-10-25 00:00:00|              0|\n",
      "|2022-10-26 00:00:00|              1|\n",
      "|2022-10-27 00:00:00|              0|\n",
      "|2022-10-28 00:00:00|              1|\n",
      "|2022-10-29 00:00:00|              1|\n",
      "|2022-10-30 00:00:00|              1|\n",
      "|2022-10-31 00:00:00|              0|\n",
      "+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trend.show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb226e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "trend.createOrReplaceTempView(\"trendSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b9d77",
   "metadata": {},
   "source": [
    "The binary variable indicates if the trend goes up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aff149",
   "metadata": {},
   "source": [
    "### 1.2 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795db881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data dir\n",
    "data_dir = \"../../data/Topic/\"\n",
    "\n",
    "# get all twitter files\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2953fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter data \n",
    "#twitter_df = spark.read.json(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ce5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hashtags = [\"vegan\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\",\n",
    "                \"veganfood\",\n",
    "                \"vegano\",\n",
    "                \"veganrecipes\",\n",
    "                \"vegansofig\",\n",
    "                \"vegansofinstagram\"]\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "files_hashtags = [file for file in tweet_files if (file.find(list_hashtags[3]) != -1)]             \n",
    "twitter_df = spark.read.option(\"multiline\",\"true\").json(files_hashtags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select interesting features\n",
    "twitter_df = twitter_df.select(F.col('user.name'),\n",
    "                                F.col('user.screen_name'),\n",
    "                                F.col('user.followers_count'),\n",
    "                                F.col('user.following'),\n",
    "                                F.col('user.statuses_count'),\n",
    "                                F.col('user.listed_count'),\n",
    "                                F.col('created_at'),\n",
    "                                F.col('full_text'),\n",
    "                                F.col('entities.hashtags'),\n",
    "                                F.col('favorite_count'),\n",
    "                                F.col('retweet_count'),\n",
    "                                F.col('user.friends_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad4622",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9dd70",
   "metadata": {},
   "source": [
    "#### 2.1 Check time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96607a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "twitter_df = twitter_df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760e9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           earliest|             latest|\n",
      "+-------------------+-------------------+\n",
      "|2021-10-27 21:28:18|2022-09-07 17:30:32|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get first post\n",
    "first_post = F.min('post_created_at').alias('earliest')\n",
    "# get latest post\n",
    "latest_post = F.max('post_created_at').alias('latest')\n",
    "# show tweet period in our dataset\n",
    "twitter_df.select(first_post, latest_post).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f4e9b",
   "metadata": {},
   "source": [
    "#### 2.2 Remove retweets and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1db1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all retweets from dataset\n",
    "no_retweets_df = twitter_df.filter(~F.col(\"full_text\").startswith(\"RT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0be0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sort no_retweets_df based on date in chronological order (most recent ones on top)\n",
    "no_retweets_sorted_df = no_retweets_df.sort(\"post_created_at\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "276c763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12649"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations before dropping duplicates\n",
    "no_retweets_sorted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abf73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates based on tweet text and the profile it was posted from\n",
    "final_no_duplicates_df = no_retweets_sorted_df.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a33594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12099"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations after dropping duplicates\n",
    "final_no_duplicates_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908138e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename dataframe\n",
    "final_twitter_df = final_no_duplicates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6316",
   "metadata": {},
   "source": [
    "## 3. Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165908d",
   "metadata": {},
   "source": [
    "For our independent variables we need to design a pipeline that transforms the data into the desired aggregated metrics per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0e22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "final_twitter_df.createOrReplaceTempView(\"twitterSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cf7e6",
   "metadata": {},
   "source": [
    "### 3.1 Volume of tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff5598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "tweet_volume = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, COUNT(*) as tweet_volume \\\n",
    "                                    FROM twitterSQL \\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffa7816d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      date|tweet_volume|\n",
      "+----------+------------+\n",
      "|2021-10-27|           1|\n",
      "|2021-10-28|         118|\n",
      "|2021-10-29|         136|\n",
      "|2021-10-30|         117|\n",
      "|2021-10-31|         116|\n",
      "|2021-11-01|         400|\n",
      "|2021-11-02|         178|\n",
      "|2021-11-03|         182|\n",
      "|2021-11-04|         164|\n",
      "|2021-11-05|         102|\n",
      "|2021-12-06|           4|\n",
      "|2021-12-07|           4|\n",
      "|2021-12-08|          39|\n",
      "|2021-12-09|         204|\n",
      "|2021-12-10|         170|\n",
      "|2021-12-11|         147|\n",
      "|2021-12-12|         152|\n",
      "|2021-12-13|         190|\n",
      "|2021-12-14|         150|\n",
      "|2021-12-15|           1|\n",
      "| 2022-2-11|          22|\n",
      "| 2022-2-12|         199|\n",
      "| 2022-2-13|         219|\n",
      "| 2022-2-14|         159|\n",
      "| 2022-2-15|         218|\n",
      "| 2022-2-16|         185|\n",
      "| 2022-2-17|         247|\n",
      "| 2022-2-18|         230|\n",
      "| 2022-2-19|         241|\n",
      "| 2022-2-20|          27|\n",
      "| 2022-3-05|          95|\n",
      "| 2022-3-06|         190|\n",
      "| 2022-3-07|         191|\n",
      "| 2022-3-08|         208|\n",
      "| 2022-3-09|         194|\n",
      "| 2022-3-10|         194|\n",
      "| 2022-3-11|         207|\n",
      "| 2022-3-12|         176|\n",
      "| 2022-3-13|         172|\n",
      "| 2022-3-14|          11|\n",
      "| 2022-4-22|         153|\n",
      "| 2022-4-23|         155|\n",
      "| 2022-4-24|         154|\n",
      "| 2022-4-25|         172|\n",
      "| 2022-4-26|         163|\n",
      "| 2022-4-27|         191|\n",
      "| 2022-4-28|         188|\n",
      "| 2022-4-29|         192|\n",
      "| 2022-4-30|          57|\n",
      "| 2022-5-20|          12|\n",
      "| 2022-5-21|         167|\n",
      "| 2022-5-22|         161|\n",
      "| 2022-5-23|         180|\n",
      "| 2022-5-24|         169|\n",
      "| 2022-5-25|         159|\n",
      "| 2022-5-26|         159|\n",
      "| 2022-5-27|         168|\n",
      "| 2022-5-28|         157|\n",
      "| 2022-5-29|           9|\n",
      "| 2022-7-29|          93|\n",
      "| 2022-7-30|         170|\n",
      "| 2022-7-31|         167|\n",
      "| 2022-8-01|         169|\n",
      "| 2022-8-02|         190|\n",
      "| 2022-8-03|         161|\n",
      "| 2022-8-04|         156|\n",
      "| 2022-8-05|         136|\n",
      "| 2022-8-06|         139|\n",
      "| 2022-8-07|         156|\n",
      "| 2022-8-08|          49|\n",
      "| 2022-8-29|          72|\n",
      "| 2022-8-30|         164|\n",
      "| 2022-8-31|         225|\n",
      "| 2022-9-01|         167|\n",
      "| 2022-9-02|         196|\n",
      "| 2022-9-03|         191|\n",
      "| 2022-9-04|         164|\n",
      "| 2022-9-05|         178|\n",
      "| 2022-9-06|         252|\n",
      "| 2022-9-07|         178|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "tweet_volume.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "072ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "tweet_volume.createOrReplaceTempView(\"tweet_volumeSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb81a7",
   "metadata": {},
   "source": [
    "### 3.2 Average likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e49f2",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28099a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_likes = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(favorite_count) as avg_likes \\\n",
    "                           FROM twitterSQL \\\n",
    "                           WHERE favorite_count > 0 \\\n",
    "                           GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                           ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67edf65d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|         avg_likes|\n",
      "+----------+------------------+\n",
      "|2021-10-28|5.2615384615384615|\n",
      "|2021-10-29|           9.09375|\n",
      "|2021-10-30| 7.130434782608695|\n",
      "|2021-10-31| 9.347222222222221|\n",
      "|2021-11-01| 4.408256880733945|\n",
      "|2021-11-02|               5.0|\n",
      "|2021-11-03|6.8173076923076925|\n",
      "|2021-11-04| 6.574712643678161|\n",
      "|2021-11-05| 2.235294117647059|\n",
      "|2021-12-06| 9.666666666666666|\n",
      "|2021-12-07|               1.5|\n",
      "|2021-12-08| 8.192307692307692|\n",
      "|2021-12-09|20.021505376344088|\n",
      "|2021-12-10| 5.390243902439025|\n",
      "|2021-12-11| 9.817204301075268|\n",
      "|2021-12-12| 7.447368421052632|\n",
      "|2021-12-13| 6.813186813186813|\n",
      "|2021-12-14| 6.945945945945946|\n",
      "| 2022-2-11|10.666666666666666|\n",
      "| 2022-2-12| 12.16793893129771|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_likes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6003b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_likes.createOrReplaceTempView(\"avg_likesSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa4e0b",
   "metadata": {},
   "source": [
    "### 3.3 Average Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff829814",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4b17a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_retweets = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(retweet_count) as avg_retweets \\\n",
    "                          FROM twitterSQL \\\n",
    "                          WHERE retweet_count > 0 \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96a6f578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|      avg_retweets|\n",
      "+----------+------------------+\n",
      "|2021-10-28|               2.0|\n",
      "|2021-10-29|  2.41025641025641|\n",
      "|2021-10-30| 3.303030303030303|\n",
      "|2021-10-31|               2.9|\n",
      "|2021-11-01| 2.515151515151515|\n",
      "|2021-11-02|1.9523809523809523|\n",
      "|2021-11-03|              2.12|\n",
      "|2021-11-04| 2.510204081632653|\n",
      "|2021-11-05|1.5217391304347827|\n",
      "|2021-12-06|               4.5|\n",
      "|2021-12-07|               1.0|\n",
      "|2021-12-08|2.6153846153846154|\n",
      "|2021-12-09|              5.04|\n",
      "|2021-12-10|1.9142857142857144|\n",
      "|2021-12-11| 4.416666666666667|\n",
      "|2021-12-12|2.6792452830188678|\n",
      "|2021-12-13|2.0317460317460316|\n",
      "|2021-12-14|3.5609756097560976|\n",
      "| 2022-2-11|2.1818181818181817|\n",
      "| 2022-2-12| 3.532258064516129|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_retweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57e76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_retweets.createOrReplaceTempView(\"avg_retweetsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c11de2",
   "metadata": {},
   "source": [
    "### 3.4 Engagement rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271c4c1",
   "metadata": {},
   "source": [
    "We define engagement rate of a tweet as the sum of likes and retweets divided by the amount of followers of the account that sent out the tweet. For our purpose we will take the avergage engagement rate per day. We exclude accounts who have no followers and we only take tweets into account which are liked and retweeted at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecee44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_engagement_rate = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(engagement_rate) as avg_engagement_rate \\\n",
    "                                     FROM (  SELECT screen_name, post_created_at, (favorite_count+retweet_count)/followers_count as engagement_rate \\\n",
    "                                             FROM twitterSQL \\\n",
    "                                             WHERE favorite_count > 0 AND retweet_count > 0 AND followers_count > 0 ) \\\n",
    "                                     GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                     ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d05ebc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date| avg_engagement_rate|\n",
      "+----------+--------------------+\n",
      "|2021-10-28|0.020606728893851515|\n",
      "|2021-10-29| 0.07340379042367053|\n",
      "|2021-10-30| 0.17919702575767021|\n",
      "|2021-10-31| 0.08996355791000546|\n",
      "|2021-11-01| 0.20946879040003968|\n",
      "|2021-11-02| 0.03199078134961682|\n",
      "|2021-11-03| 0.08867414385733928|\n",
      "|2021-11-04| 0.08067982230339633|\n",
      "|2021-11-05| 0.05866521462317866|\n",
      "|2021-12-06|7.808669607732365E-4|\n",
      "|2021-12-08|0.007915940641805837|\n",
      "|2021-12-09| 0.28033717603505437|\n",
      "|2021-12-10| 0.02277622745914704|\n",
      "|2021-12-11| 0.14491531461862578|\n",
      "|2021-12-12| 0.07350065473576302|\n",
      "|2021-12-13| 0.15918884782728934|\n",
      "|2021-12-14| 0.03254682747088554|\n",
      "| 2022-2-11|0.024072641042102536|\n",
      "| 2022-2-12| 0.09974280721272229|\n",
      "| 2022-2-13| 0.04839131603550477|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_engagement_rate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e2dfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_engagement_rate.createOrReplaceTempView(\"avg_engagement_rateSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea74252",
   "metadata": {},
   "source": [
    "### 3.5 Number of influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77904a",
   "metadata": {},
   "source": [
    "We will calculate how many influencers actively tweeted a certain day. We define an influencer as someone with:\n",
    "- followers > 1000 \n",
    "- engagement_rate > 0.20 \n",
    "- weekly tweet frequency > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54d92a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influencers(follower_count_tresh, eng_rate_tresh, freq_week_tresh, data):\n",
    "\n",
    "    #df\n",
    "    df = data\n",
    "    \n",
    "    # get all users with their amount of followers\n",
    "    influencers = df.groupBy(\"screen_name\") \\\n",
    "                    .agg(first(\"followers_count\").alias(\"followers_count\"))\n",
    "\n",
    "    # average engagement rate for each user\n",
    "    eng_rate = df.withColumn('eng_rate', ((df['favorite_count'] + df['retweet_count'])/df['followers_count']))\n",
    "\n",
    "    eng_rate_user = eng_rate.groupBy(\"screen_name\") \\\n",
    "                            .agg(avg(\"eng_rate\").alias(\"eng_rate\"))\n",
    "\n",
    "    # average freq_weekly per user\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('screen_name', 'year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('screen_name', 'year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select('screen_name', 'freq')\n",
    "\n",
    "    freq_week = freq_week.groupby(\"screen_name\").agg(avg(freq_week.freq).alias('freq'))\n",
    "\n",
    "    # put the data together\n",
    "    data_joined = eng_rate_user.join(influencers, \"screen_name\").join(freq_week, \"screen_name\")\n",
    "\n",
    "    # filter the data\n",
    "    data_joined = data_joined.filter((data_joined.followers_count > follower_count_tresh) & (data_joined.eng_rate > eng_rate_tresh) & (data_joined.freq > freq_week_tresh))\n",
    "    \n",
    "    # show the data\n",
    "    data_joined.show()\n",
    "    return data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16df2e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---------------+------------------+\n",
      "|    screen_name|            eng_rate|followers_count|              freq|\n",
      "+---------------+--------------------+---------------+------------------+\n",
      "| _Alex_Greenway|0.009057542454856534|           1693|2.3333333333333335|\n",
      "|   chefmompiche|0.002041559397617...|           1216|               3.0|\n",
      "|    vivaluvegan| 0.07332462784595821|           3907|               3.0|\n",
      "|     wiservegan| 0.06963865224637812|           1020|               3.0|\n",
      "|   DarrenLong71|0.013008270573555896|           2217|             3.125|\n",
      "| PlantBasedGent|0.002493369018065403|           1128|              21.6|\n",
      "|      VeganGuys|0.004872359357709762|           3975|              2.75|\n",
      "|  nolancharlene|0.003874202370100...|           1097|               4.0|\n",
      "|         innkyo| 0.03430053334605656|           1268| 2.272727272727273|\n",
      "|FearOfTheDuck74| 0.11069032095018164|           1255| 3.357142857142857|\n",
      "|LornaMa03249374|0.014224769730596533|           1931| 7.823529411764706|\n",
      "|      sauteslut|0.013172304234463968|           2509|              2.75|\n",
      "|smile_n_be_nice| 0.00965343755823208|           1328|3.1818181818181817|\n",
      "| Chihuahua__Mom|0.003449985245008...|           1691|               3.0|\n",
      "|     caavakushi|0.003516598949088...|           1040|  9.38888888888889|\n",
      "|BeerInStemGlass|0.002281266774020...|           1863|               4.0|\n",
      "|       6milesup|0.013058752718676951|           6511| 3.142857142857143|\n",
      "|plantpowercoupl|0.004304964738475857|           1351|              2.25|\n",
      "|  MyVegan_Reach|0.028098614995732848|           2935|14.909090909090908|\n",
      "|      SteveW69x|0.017224813069790245|          18643|3.3333333333333335|\n",
      "+---------------+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "influencers = get_influencers(1000, 0.002, 2, final_twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8acae7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "influencers.createOrReplaceTempView(\"influencersSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d8a7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "number_of_influencers = spark.sql(\" SELECT DATE_FORMAT(a.post_created_at, 'Y-M-dd') as date, COUNT(b.screen_name) as influencers \\\n",
    "                                    FROM twitterSQL a \\\n",
    "                                    RIGHT OUTER JOIN influencersSQL b ON a.screen_name = b.screen_name\\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "954c3a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      date|influencers|\n",
      "+----------+-----------+\n",
      "|2021-10-28|         11|\n",
      "|2021-10-29|          9|\n",
      "|2021-10-30|          5|\n",
      "|2021-10-31|         10|\n",
      "|2021-11-01|         16|\n",
      "|2021-11-02|         17|\n",
      "|2021-11-03|         18|\n",
      "|2021-11-04|         12|\n",
      "|2021-11-05|          9|\n",
      "|2021-12-07|          1|\n",
      "|2021-12-08|          1|\n",
      "|2021-12-09|         21|\n",
      "|2021-12-10|         13|\n",
      "|2021-12-11|          5|\n",
      "|2021-12-12|         10|\n",
      "|2021-12-13|         11|\n",
      "|2021-12-14|         14|\n",
      "| 2022-2-11|          1|\n",
      "| 2022-2-12|         10|\n",
      "| 2022-2-13|         18|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "number_of_influencers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd978455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"number_of_influencersSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b60dd",
   "metadata": {},
   "source": [
    "## 4. Basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0456801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basetable\n",
    "basetable = spark.sql(\"SELECT DATE_FORMAT(a.date, 'Y-M-dd') as date, a.dependent_vegan, b.tweet_volume, COALESCE(c.avg_likes,0) as avg_likes, \\\n",
    "                       COALESCE(d.avg_retweets,0) as avg_retweets, \\\n",
    "                       COALESCE(e.avg_engagement_rate,0) as avg_engagement_rate, COALESCE(f.influencers,0) as influencers \\\n",
    "                       FROM trendSQL a \\\n",
    "                       INNER JOIN tweet_volumeSQL b ON DATE_FORMAT(a.date, 'Y-M-dd') = b.date \\\n",
    "                       LEFT OUTER JOIN avg_likesSQL c ON b.date = c.date \\\n",
    "                       LEFT OUTER JOIN avg_retweetsSQL d ON c.date = d.date \\\n",
    "                       LEFT OUTER JOIN avg_engagement_rateSQL e ON d.date = e.date \\\n",
    "                       LEFT OUTER JOIN number_of_influencersSQL f ON e.date = f.date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "874c8114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+------------------+------------------+--------------------+-----------+\n",
      "|      date|dependent_vegan|tweet_volume|         avg_likes|      avg_retweets| avg_engagement_rate|influencers|\n",
      "+----------+---------------+------------+------------------+------------------+--------------------+-----------+\n",
      "|2021-11-03|              1|         182|6.8173076923076925|              2.12| 0.08867414385733928|         18|\n",
      "| 2022-5-26|              1|         159| 11.85576923076923|           3.21875| 0.15057985999688228|         26|\n",
      "| 2022-3-13|              1|         172|12.474747474747474|2.1176470588235294| 0.08013253638838845|         26|\n",
      "| 2022-8-03|              1|         161|15.727272727272727| 4.333333333333333|0.053563194338015395|         22|\n",
      "| 2022-8-31|              1|         225|10.816176470588236|              3.68| 0.12391731095293393|         30|\n",
      "| 2022-5-29|              1|           9|               2.0|1.3333333333333333|0.015208098987626546|          3|\n",
      "|2021-12-06|              0|           4| 9.666666666666666|               4.5|7.808669607732365E-4|          0|\n",
      "| 2022-9-07|              0|         178| 4.052173913043478|1.6451612903225807| 0.10544505830730229|         15|\n",
      "| 2022-9-03|              1|         191|12.092436974789916|2.7115384615384617| 0.05852181630692502|         12|\n",
      "| 2022-2-20|              0|          27|               3.0|              1.25| 0.16796140054962902|          2|\n",
      "| 2022-2-16|              0|         185|19.863636363636363| 6.551020408163265| 0.09354109442748304|         20|\n",
      "|2021-12-08|              1|          39| 8.192307692307692|2.6153846153846154|0.007915940641805837|          1|\n",
      "| 2022-2-17|              0|         247| 8.882882882882884|2.4915254237288136| 0.06069986549083461|         22|\n",
      "|2021-11-02|              0|         178|               5.0|1.9523809523809523| 0.03199078134961682|         17|\n",
      "| 2022-5-27|              1|         168| 7.991071428571429| 2.537313432835821| 0.07336832671725256|         21|\n",
      "| 2022-2-13|              1|         219|13.134751773049645|  9.78688524590164| 0.04839131603550477|         18|\n",
      "| 2022-4-27|              1|         191|12.242424242424242| 3.287878787878788| 0.06286301609653519|         23|\n",
      "| 2022-3-05|              1|          95|15.867924528301886|              2.76| 0.07174959720343697|          6|\n",
      "| 2022-8-07|              1|         156|12.208791208791208|             3.125|0.059489405651015105|         20|\n",
      "|2021-12-13|              0|         190| 6.813186813186813|2.0317460317460316| 0.15918884782728934|         11|\n",
      "|2021-10-27|              0|           1|               0.0|               0.0|                 0.0|          0|\n",
      "| 2022-8-29|              0|          72|10.628571428571428| 3.923076923076923|  0.1230370060732074|          8|\n",
      "|2021-11-05|              0|         102| 2.235294117647059|1.5217391304347827| 0.05866521462317866|          9|\n",
      "| 2022-4-22|              1|         153|11.552380952380952|3.9272727272727272| 0.07190513859913819|         35|\n",
      "| 2022-2-19|              1|         241|13.026548672566372| 3.980392156862745|  1.7220981252803553|         13|\n",
      "| 2022-8-08|              0|          49|2.6538461538461537|               1.6| 0.28170038555510396|          5|\n",
      "| 2022-8-01|              0|         169|10.409090909090908|3.4423076923076925|0.058901045905135584|         17|\n",
      "| 2022-2-18|              1|         230| 7.343065693430657|3.4285714285714284| 0.04244017937187841|         12|\n",
      "| 2022-8-04|              1|         156| 7.791666666666667| 2.466666666666667|0.053276610768395585|         21|\n",
      "| 2022-4-26|              0|         163|27.121739130434783| 5.955223880597015| 0.21400290464506086|         32|\n",
      "| 2022-3-06|              1|         190|22.065693430656935| 8.422535211267606| 0.08110129449857188|         23|\n",
      "| 2022-5-22|              1|         161|11.264705882352942|3.7049180327868854|0.060407993079861645|         33|\n",
      "| 2022-2-12|              1|         199| 12.16793893129771| 3.532258064516129| 0.09974280721272229|         10|\n",
      "|2021-12-14|              0|         150| 6.945945945945946|3.5609756097560976| 0.03254682747088554|         14|\n",
      "| 2022-3-11|              1|         207| 17.21311475409836| 4.787878787878788|  0.0956005604136762|         22|\n",
      "| 2022-8-02|              0|         190| 10.77570093457944| 3.135593220338983|  0.1503820201630779|         28|\n",
      "|2021-12-10|              0|         170| 5.390243902439025|1.9142857142857144| 0.02277622745914704|         13|\n",
      "| 2022-9-02|              1|         196|20.324074074074073|3.5964912280701755| 0.05593850598184665|         18|\n",
      "| 2022-2-15|              0|         218|              10.5| 3.970149253731343| 0.03472498352435216|         32|\n",
      "|2021-10-28|              0|         118|5.2615384615384615|               2.0|0.020606728893851515|         11|\n",
      "| 2022-2-11|              1|          22|10.666666666666666|2.1818181818181817|0.024072641042102536|          1|\n",
      "|2021-11-01|              0|         400| 4.408256880733945| 2.515151515151515| 0.20946879040003968|         16|\n",
      "| 2022-3-09|              0|         194|20.009174311926607| 4.633333333333334|  0.0642508238846412|         31|\n",
      "| 2022-3-14|              1|          11|               8.6|1.6666666666666667|0.002050908699043904|          3|\n",
      "| 2022-7-29|              1|          93|12.666666666666666| 3.391304347826087| 0.07124201662186683|          4|\n",
      "| 2022-5-24|              1|         169| 8.225225225225225|3.2063492063492065|0.048040877912245775|         35|\n",
      "| 2022-4-28|              0|         188|11.721804511278195| 3.864406779661017|  0.0588848527570557|         35|\n",
      "| 2022-3-07|              0|         191|10.289473684210526|              2.35| 0.07755701577634858|         28|\n",
      "| 2022-8-05|              1|         136| 6.779220779220779|              2.15| 0.11166984748920919|         15|\n",
      "| 2022-9-04|              0|         164| 16.02247191011236|3.2888888888888888|  0.0562752326605442|         15|\n",
      "+----------+---------------+------------+------------------+------------------+--------------------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "basetable.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be427e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required functions\n",
    "from pyspark.ml.feature import Binarizer, StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de196982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define string indexer to index price \n",
    "SI = StringIndexer(inputCol = 'dependent_vegan', outputCol = 'label')\n",
    "\n",
    "# define vector assembler for numeric variables\n",
    "numColumns = ['avg_likes','avg_retweets','avg_engagement_rate','influencers']\n",
    "VAnum = VectorAssembler(inputCols=numColumns, outputCol=\"numFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b432f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [SI, VAnum]\n",
    "# define pipeline and fit on data\n",
    "preprocessingPipeline = Pipeline().setStages(stages).fit(basetable)\n",
    "# apply pipeline on data\n",
    "basetable = preprocessingPipeline.transform(basetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87f641a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "basetable = basetable.select([\"numFeatures\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "935b2ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         numFeatures|label|\n",
      "+--------------------+-----+\n",
      "|[6.81730769230769...|  0.0|\n",
      "|[11.8557692307692...|  0.0|\n",
      "|[12.4747474747474...|  0.0|\n",
      "|[15.7272727272727...|  0.0|\n",
      "|[10.8161764705882...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "basetable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73cf59",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "- Split the data in a train en test set (70/30).\n",
    "- Build one pipeline that:\n",
    "  - standardizes the numerical variables\n",
    "  - applies a logistic regression to the data\n",
    "  - check the performance using the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66cad8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = basetable.randomSplit([0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c37d22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# check number of observations in train and test set\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42e4c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   46|\n",
      "|  1.0|   34|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   36|\n",
      "|  1.0|   22|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   10|\n",
      "|  1.0|   12|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect distribution of label in train and test set\n",
    "basetable.groupBy(\"label\").count().show()\n",
    "train.groupBy(\"label\").count().show()\n",
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89c3e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required features\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "609168ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "SS = StandardScaler(inputCol = 'numFeatures', outputCol = 'scaledNumFeatures', withStd = True, withMean = False)\n",
    "\n",
    "# define vector assembler\n",
    "VA = VectorAssembler(inputCols = ['scaledNumFeatures'], outputCol = 'features')\n",
    "\n",
    "# define logistic regression model\n",
    "LR = LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b271625a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o265.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 936.0 (TID 1552) (192.168.1.2 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:293)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:173)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:143)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:260)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:258)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:258)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:230)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:372)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2870)\r\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2877)\r\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:113)\r\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:84)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m stages \u001b[38;5;241m=\u001b[39m [SS, VA, LR]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# create pipeline and fit on training set\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m lrModelPipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetStages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# apply pipeline on test set to get predictions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m lrModelPipeline\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:199\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    201\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o265.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 936.0 (TID 1552) (192.168.1.2 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:293)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:173)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:143)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:260)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:258)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:258)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:230)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:372)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2870)\r\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2877)\r\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:113)\r\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:84)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "# define pipeline stages\n",
    "stages = [SS, VA, LR]\n",
    "# create pipeline and fit on training set\n",
    "lrModelPipeline = Pipeline().setStages(stages).fit(train)\n",
    "# apply pipeline on test set to get predictions\n",
    "predictions = lrModelPipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce473b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect predictions\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 61904)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\Lenne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\Lenne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\Lenne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] De externe host heeft een verbinding verbroken\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# get evaluation metric\n",
    "lrAUC = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "# inspect model performance\n",
    "print('AUC lr: %f' %(lrAUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce8f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc9467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
