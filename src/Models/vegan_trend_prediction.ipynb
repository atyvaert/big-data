{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76508cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:38:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "# initialize findspark with spark directory\n",
    "#findspark.init(\"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\")\n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99192d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import emojis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dafa38",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dcdae",
   "metadata": {},
   "source": [
    "In this notebook we will buid a model that predicts if the trend of a certain topic goes up or down on a certain day based on Twitter data of that day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1943e2",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa5e6",
   "metadata": {},
   "source": [
    "### 1.1 Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a3d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read trend data \n",
    "trend = spark.read.csv(\".././../data/Google_trends/daily_trends.csv\", header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be47e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, dependent_vegan: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6b34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:38:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/07 18:38:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/07 18:38:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/07 18:38:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/07 18:38:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-------------------+---------------+\n",
      "|               date|dependent_vegan|\n",
      "+-------------------+---------------+\n",
      "|2021-10-04 00:00:00|              1|\n",
      "|2021-10-05 00:00:00|              1|\n",
      "|2021-10-06 00:00:00|              1|\n",
      "|2021-10-07 00:00:00|              1|\n",
      "|2021-10-08 00:00:00|              1|\n",
      "|2021-10-09 00:00:00|              0|\n",
      "|2021-10-10 00:00:00|              0|\n",
      "|2021-10-11 00:00:00|              0|\n",
      "|2021-10-12 00:00:00|              0|\n",
      "|2021-10-13 00:00:00|              0|\n",
      "|2021-10-14 00:00:00|              0|\n",
      "|2021-10-15 00:00:00|              1|\n",
      "|2021-10-16 00:00:00|              1|\n",
      "|2021-10-17 00:00:00|              0|\n",
      "|2021-10-18 00:00:00|              1|\n",
      "|2021-10-19 00:00:00|              0|\n",
      "|2021-10-20 00:00:00|              0|\n",
      "|2021-10-21 00:00:00|              1|\n",
      "|2021-10-22 00:00:00|              1|\n",
      "|2021-10-23 00:00:00|              1|\n",
      "+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy().orderBy(col(\"date\"))\n",
    "trend.withColumn(\"dependent_vegan\", lag(\"dependent_vegan\", -1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb226e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "trend.createOrReplaceTempView(\"trendSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b9d77",
   "metadata": {},
   "source": [
    "The binary variable indicates if the trend goes up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aff149",
   "metadata": {},
   "source": [
    "### 1.2 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795db881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data dir\n",
    "data_dir = \"../../data/Topic/\"\n",
    "\n",
    "# get all twitter files\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2953fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter data \n",
    "#twitter_df = spark.read.json(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ce5a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:38:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_hashtags = [\"vegan\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\",\n",
    "                \"veganfood\",\n",
    "                \"vegano\",\n",
    "                \"veganrecipes\",\n",
    "                \"vegansofig\",\n",
    "                \"vegansofinstagram\"]\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "files_hashtags = [file for file in tweet_files if (file.find(list_hashtags[0]) != -1)]             \n",
    "twitter_df = spark.read.option(\"multiline\",\"true\").json(files_hashtags) \n",
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select interesting features\n",
    "twitter_df = twitter_df.select(F.col('user.name'),\n",
    "                                F.col('user.screen_name'),\n",
    "                                F.col('user.followers_count'),\n",
    "                                F.col('user.following'),\n",
    "                                F.col('user.statuses_count'),\n",
    "                                F.col('user.listed_count'),\n",
    "                                F.col('created_at'),\n",
    "                                F.col('full_text'),\n",
    "                                F.col('entities.hashtags'),\n",
    "                                F.col('favorite_count'),\n",
    "                                F.col('retweet_count'),\n",
    "                                F.col('user.friends_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad4622",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9dd70",
   "metadata": {},
   "source": [
    "#### 2.1 Check time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96607a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "twitter_df = twitter_df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "760e9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================================> (140 + 3) / 143]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           earliest|             latest|\n",
      "+-------------------+-------------------+\n",
      "|2021-10-25 07:19:40|2022-10-11 23:17:33|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get first post\n",
    "first_post = F.min('post_created_at').alias('earliest')\n",
    "# get latest post\n",
    "latest_post = F.max('post_created_at').alias('latest')\n",
    "# show tweet period in our dataset\n",
    "twitter_df.select(first_post, latest_post).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f4e9b",
   "metadata": {},
   "source": [
    "#### 2.2 Remove retweets and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1db1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all retweets from dataset\n",
    "no_retweets_df = twitter_df.filter(~F.col(\"full_text\").startswith(\"RT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0be0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sort no_retweets_df based on date in chronological order (most recent ones on top)\n",
    "no_retweets_sorted_df = no_retweets_df.sort(\"post_created_at\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276c763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "745916"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations before dropping duplicates\n",
    "no_retweets_sorted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates based on tweet text and the profile it was posted from\n",
    "final_no_duplicates_df = no_retweets_sorted_df.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a33594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "693932"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations after dropping duplicates\n",
    "final_no_duplicates_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908138e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename dataframe\n",
    "final_twitter_df = final_no_duplicates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6316",
   "metadata": {},
   "source": [
    "## 3. Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165908d",
   "metadata": {},
   "source": [
    "For our independent variables we need to design a pipeline that transforms the data into the desired aggregated metrics per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0e22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "final_twitter_df.createOrReplaceTempView(\"twitterSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cf7e6",
   "metadata": {},
   "source": [
    "### 3.1 Volume of tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff5598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "tweet_volume = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, COUNT(*) as tweet_volume \\\n",
    "                                    FROM twitterSQL \\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa7816d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      date|tweet_volume|\n",
      "+----------+------------+\n",
      "|2021-10-25|          50|\n",
      "|2021-10-26|          45|\n",
      "|2021-10-27|         894|\n",
      "|2021-10-28|        2825|\n",
      "|2021-10-29|       14021|\n",
      "|2021-10-30|       12497|\n",
      "|2021-10-31|       12414|\n",
      "|2021-11-01|       24108|\n",
      "|2021-11-02|       17623|\n",
      "|2021-11-03|        3316|\n",
      "|2021-11-04|        2560|\n",
      "|2021-11-05|         593|\n",
      "|2021-11-06|           6|\n",
      "|2021-12-03|           4|\n",
      "|2021-12-04|          66|\n",
      "|2021-12-05|          72|\n",
      "|2021-12-06|        1336|\n",
      "|2021-12-07|        4560|\n",
      "|2021-12-08|       13077|\n",
      "|2021-12-09|       12693|\n",
      "|2021-12-10|       13848|\n",
      "|2021-12-11|       12213|\n",
      "|2021-12-12|       10589|\n",
      "|2021-12-13|        2930|\n",
      "|2021-12-14|        1941|\n",
      "|2021-12-15|        1596|\n",
      "|2021-12-16|         107|\n",
      "|2021-12-25|         637|\n",
      "| 2022-1-01|        1744|\n",
      "| 2022-1-02|         973|\n",
      "| 2022-1-08|        1326|\n",
      "| 2022-1-09|        1672|\n",
      "| 2022-1-10|        1886|\n",
      "| 2022-1-11|        1891|\n",
      "| 2022-1-12|        1996|\n",
      "| 2022-1-13|        1799|\n",
      "| 2022-1-14|        1981|\n",
      "| 2022-1-15|        1570|\n",
      "| 2022-1-16|         227|\n",
      "| 2022-1-20|        1460|\n",
      "| 2022-1-21|        2338|\n",
      "| 2022-1-22|        1707|\n",
      "| 2022-1-23|        2045|\n",
      "| 2022-1-24|        2326|\n",
      "| 2022-1-25|        1765|\n",
      "| 2022-1-26|        2283|\n",
      "| 2022-1-27|        1900|\n",
      "| 2022-1-31|          78|\n",
      "|2022-10-09|        1073|\n",
      "|2022-10-10|        1375|\n",
      "|2022-10-11|        1371|\n",
      "|2022-12-26|        1112|\n",
      "|2022-12-27|        1436|\n",
      "|2022-12-28|        1645|\n",
      "|2022-12-29|        1827|\n",
      "|2022-12-30|        1466|\n",
      "|2022-12-31|        1589|\n",
      "| 2022-2-01|        1663|\n",
      "| 2022-2-02|        1878|\n",
      "| 2022-2-03|        1610|\n",
      "| 2022-2-04|        1848|\n",
      "| 2022-2-05|        2233|\n",
      "| 2022-2-06|        1760|\n",
      "| 2022-2-07|        1718|\n",
      "| 2022-2-08|        1309|\n",
      "| 2022-2-11|         648|\n",
      "| 2022-2-12|        1997|\n",
      "| 2022-2-13|        2093|\n",
      "| 2022-2-14|        2243|\n",
      "| 2022-2-15|        2277|\n",
      "| 2022-2-16|        2293|\n",
      "| 2022-2-17|        2435|\n",
      "| 2022-2-18|        2303|\n",
      "| 2022-2-19|        6250|\n",
      "| 2022-2-20|        1521|\n",
      "| 2022-2-21|        1711|\n",
      "| 2022-2-22|        1906|\n",
      "| 2022-2-23|        1890|\n",
      "| 2022-3-03|          26|\n",
      "| 2022-3-04|         102|\n",
      "| 2022-3-05|        4183|\n",
      "| 2022-3-06|       10378|\n",
      "| 2022-3-07|       11151|\n",
      "| 2022-3-08|       11844|\n",
      "| 2022-3-09|       11762|\n",
      "| 2022-3-10|       11995|\n",
      "| 2022-3-11|       11691|\n",
      "| 2022-3-12|        2031|\n",
      "| 2022-3-13|        2192|\n",
      "| 2022-3-14|        1608|\n",
      "| 2022-3-15|        1510|\n",
      "| 2022-3-16|        1647|\n",
      "| 2022-3-25|        1366|\n",
      "| 2022-3-26|        1518|\n",
      "| 2022-3-27|        1505|\n",
      "| 2022-3-28|        1471|\n",
      "| 2022-3-29|        1576|\n",
      "| 2022-3-30|        1621|\n",
      "| 2022-3-31|        1686|\n",
      "| 2022-4-01|         812|\n",
      "+----------+------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "tweet_volume.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "072ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "tweet_volume.createOrReplaceTempView(\"tweet_volumeSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb81a7",
   "metadata": {},
   "source": [
    "### 3.2 Average likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e49f2",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28099a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_likes = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(favorite_count) as avg_likes \\\n",
    "                           FROM twitterSQL \\\n",
    "                           WHERE favorite_count > 0 \\\n",
    "                           GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                           ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67edf65d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|         avg_likes|\n",
      "+----------+------------------+\n",
      "|2021-10-25|           4.65625|\n",
      "|2021-10-26|             5.125|\n",
      "|2021-10-27|11.645669291338583|\n",
      "|2021-10-28|11.103731815306768|\n",
      "|2021-10-29| 12.31424108305129|\n",
      "|2021-10-30|11.979163693449408|\n",
      "|2021-10-31|12.956186317321688|\n",
      "|2021-11-01| 13.27580421620833|\n",
      "|2021-11-02| 8.794319501636576|\n",
      "|2021-11-03|15.065796937039138|\n",
      "|2021-11-04|10.239657631954351|\n",
      "|2021-11-05| 3.459016393442623|\n",
      "|2021-11-06|               2.5|\n",
      "|2021-12-03|              20.0|\n",
      "|2021-12-04|             10.08|\n",
      "|2021-12-05|5.7105263157894735|\n",
      "|2021-12-06| 11.53735255570118|\n",
      "|2021-12-07|26.699334319526628|\n",
      "|2021-12-08| 14.52754383542731|\n",
      "|2021-12-09|12.793646370349729|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_likes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6003b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_likes.createOrReplaceTempView(\"avg_likesSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa4e0b",
   "metadata": {},
   "source": [
    "### 3.3 Average Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff829814",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4b17a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_retweets = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(retweet_count) as avg_retweets \\\n",
    "                          FROM twitterSQL \\\n",
    "                          WHERE retweet_count > 0 \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96a6f578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|      avg_retweets|\n",
      "+----------+------------------+\n",
      "|2021-10-25|               3.0|\n",
      "|2021-10-26| 4.166666666666667|\n",
      "|2021-10-27| 5.993006993006993|\n",
      "|2021-10-28| 5.175879396984925|\n",
      "|2021-10-29|6.7106673161227475|\n",
      "|2021-10-30| 5.183630640083946|\n",
      "|2021-10-31| 6.077004219409282|\n",
      "|2021-11-01| 6.752923976608187|\n",
      "|2021-11-02| 4.722175732217573|\n",
      "|2021-11-03| 8.869379014989294|\n",
      "|2021-11-04| 5.420485175202156|\n",
      "|2021-11-05|1.7222222222222223|\n",
      "|2021-11-06|               1.5|\n",
      "|2021-12-03|               2.5|\n",
      "|2021-12-04|2.5714285714285716|\n",
      "|2021-12-05|2.7777777777777777|\n",
      "|2021-12-06| 7.503703703703704|\n",
      "|2021-12-07|13.976780185758514|\n",
      "|2021-12-08| 6.731100963977676|\n",
      "|2021-12-09|  6.67574931880109|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_retweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57e76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_retweets.createOrReplaceTempView(\"avg_retweetsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c11de2",
   "metadata": {},
   "source": [
    "### 3.4 Engagement rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271c4c1",
   "metadata": {},
   "source": [
    "We define engagement rate of a tweet as the sum of likes and retweets divided by the amount of followers of the account that sent out the tweet. For our purpose we will take the avergage engagement rate per day. We exclude accounts who have no followers and we only take tweets into account which are liked and retweeted at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecee44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_engagement_rate = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(engagement_rate) as avg_engagement_rate \\\n",
    "                                     FROM (  SELECT screen_name, post_created_at, (favorite_count+retweet_count)/followers_count as engagement_rate \\\n",
    "                                             FROM twitterSQL \\\n",
    "                                             WHERE favorite_count > 0 AND retweet_count > 0 AND followers_count > 0 ) \\\n",
    "                                     GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                     ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d05ebc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:=====================================================>(142 + 1) / 143]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:44:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date| avg_engagement_rate|\n",
      "+----------+--------------------+\n",
      "|2021-10-25|0.035312352622552404|\n",
      "|2021-10-26|0.041442045473123704|\n",
      "|2021-10-27|  0.0741752578942463|\n",
      "|2021-10-28|  0.2127516303746722|\n",
      "|2021-10-29| 0.07491809784621552|\n",
      "|2021-10-30| 0.11431344329991702|\n",
      "|2021-10-31| 0.19613345872986776|\n",
      "|2021-11-01| 0.09034112086921374|\n",
      "|2021-11-02| 0.06331765741485894|\n",
      "|2021-11-03|   0.342346333831607|\n",
      "|2021-11-04|0.056622317733272476|\n",
      "|2021-11-05| 0.21296895770236038|\n",
      "|2021-11-06|0.005484460694698354|\n",
      "|2021-12-03| 0.01529917011031044|\n",
      "|2021-12-04|  0.3279277126010579|\n",
      "|2021-12-05|0.009540321788060942|\n",
      "|2021-12-06| 0.06278172079703152|\n",
      "|2021-12-07| 0.22602079420407264|\n",
      "|2021-12-08|  0.1298943012098277|\n",
      "|2021-12-09| 0.06074244817993019|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_engagement_rate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e2dfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_engagement_rate.createOrReplaceTempView(\"avg_engagement_rateSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea74252",
   "metadata": {},
   "source": [
    "### 3.5 Number of influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77904a",
   "metadata": {},
   "source": [
    "We will calculate how many influencers actively tweeted a certain day. We define an influencer as someone with:\n",
    "- followers > 1000 \n",
    "- engagement_rate > 0.20 \n",
    "- weekly tweet frequency > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54d92a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influencers(follower_count_tresh, eng_rate_tresh, freq_week_tresh, data):\n",
    "\n",
    "    #df\n",
    "    df = data\n",
    "    \n",
    "    # get all users with their amount of followers\n",
    "    influencers = df.groupBy(\"screen_name\") \\\n",
    "                    .agg(first(\"followers_count\").alias(\"followers_count\"))\n",
    "\n",
    "    # average engagement rate for each user\n",
    "    eng_rate = df.withColumn('eng_rate', ((df['favorite_count'] + df['retweet_count'])/df['followers_count']))\n",
    "\n",
    "    eng_rate_user = eng_rate.groupBy(\"screen_name\") \\\n",
    "                            .agg(avg(\"eng_rate\").alias(\"eng_rate\"))\n",
    "\n",
    "    # average freq_weekly per user\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('screen_name', 'year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('screen_name', 'year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select('screen_name', 'freq')\n",
    "\n",
    "    freq_week = freq_week.groupby(\"screen_name\").agg(avg(freq_week.freq).alias('freq'))\n",
    "\n",
    "    # put the data together\n",
    "    data_joined = eng_rate_user.join(influencers, \"screen_name\").join(freq_week, \"screen_name\")\n",
    "\n",
    "    # filter the data\n",
    "    data_joined = data_joined.filter((data_joined.followers_count > follower_count_tresh) & (data_joined.eng_rate > eng_rate_tresh) & (data_joined.freq > freq_week_tresh))\n",
    "    \n",
    "    # show the data\n",
    "    data_joined.show()\n",
    "    return data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16df2e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:(140 + 3) / 143][Stage 73:=>  (3 + 5) / 9][Stage 75:>   (0 + 0) / 9]3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:50:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:50:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---------------+------------------+\n",
      "|    screen_name|            eng_rate|followers_count|              freq|\n",
      "+---------------+--------------------+---------------+------------------+\n",
      "|      5GenocIDe|0.003218405440206144|           1140| 4.942857142857143|\n",
      "|        AQUAB23|0.022003034901365705|           1318|               3.0|\n",
      "|AlsJane_therapy|0.008247976142192238|           6226|               2.5|\n",
      "|AmazingArbuckle|0.003063373540111...|           3482|               3.0|\n",
      "|   AmeliaLynn70|0.014513189093212512|           2234|2.3333333333333335|\n",
      "|Antoniosaiyajin|0.005135345260946718|           3699|               3.0|\n",
      "|   BDAWOSBranch|0.002719854941069...|           1103|               3.0|\n",
      "|    BlogofVegan|0.003437569278129488|           9257| 5.115384615384615|\n",
      "|   BrianKateman|0.004763913172491486|           1542|               3.5|\n",
      "|   CathyGreen67|0.003029875597498...|           1161|3.1666666666666665|\n",
      "|   ChubbieVegan|0.003564221783895...|           2028|2.4411764705882355|\n",
      "|      CloseUpPR|0.002289026649959...|           1229|            2.3125|\n",
      "|CookingWithJoya|0.013832347405043948|           3969|               2.6|\n",
      "|     DBelardoMD|0.003578721189301...|          67692|               2.5|\n",
      "|DirtyDiceGoblin|0.006253635834787667|           4584|               3.0|\n",
      "|     Donna_Barr|0.002244432928481...|           1188|               3.0|\n",
      "|      FATALSINZ|0.002857142857142857|           1750|               3.0|\n",
      "|     Fan2retour| 0.02423411065386374|           2187|               4.0|\n",
      "|    FeelUrPower|0.002616944717042852|           1019|               3.0|\n",
      "|    Franc_Aller|0.007318795437607484|           2825|3.5714285714285716|\n",
      "+---------------+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "influencers = get_influencers(1000, 0.002, 2, final_twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8acae7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "influencers.createOrReplaceTempView(\"influencersSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d8a7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "number_of_influencers = spark.sql(\" SELECT DATE_FORMAT(a.post_created_at, 'Y-M-dd') as date, COUNT(b.screen_name) as influencers \\\n",
    "                                    FROM twitterSQL a \\\n",
    "                                    RIGHT OUTER JOIN influencersSQL b ON a.screen_name = b.screen_name\\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "954c3a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:============>(142 + 1) / 143][Stage 111:>                (0 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:53:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:53:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 139:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      date|influencers|\n",
      "+----------+-----------+\n",
      "|2021-10-26|          1|\n",
      "|2021-10-27|         47|\n",
      "|2021-10-28|        124|\n",
      "|2021-10-29|        764|\n",
      "|2021-10-30|        580|\n",
      "|2021-10-31|        631|\n",
      "|2021-11-01|       1216|\n",
      "|2021-11-02|        887|\n",
      "|2021-11-03|        180|\n",
      "|2021-11-04|        147|\n",
      "|2021-11-05|         20|\n",
      "|2021-12-04|          3|\n",
      "|2021-12-05|          6|\n",
      "|2021-12-06|         26|\n",
      "|2021-12-07|        264|\n",
      "|2021-12-08|        584|\n",
      "|2021-12-09|        698|\n",
      "|2021-12-10|        759|\n",
      "|2021-12-11|        775|\n",
      "|2021-12-12|        528|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "number_of_influencers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd978455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"number_of_influencersSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b60dd",
   "metadata": {},
   "source": [
    "## 4. Basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0456801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basetable\n",
    "basetable = spark.sql(\"SELECT DATE_FORMAT(a.date, 'Y-M-dd') as date, a.dependent_vegan, b.tweet_volume, COALESCE(c.avg_likes,0) as avg_likes, \\\n",
    "                       COALESCE(d.avg_retweets,0) as avg_retweets, \\\n",
    "                       COALESCE(e.avg_engagement_rate,0) as avg_engagement_rate, COALESCE(f.influencers,0) as influencers \\\n",
    "                       FROM trendSQL a \\\n",
    "                       INNER JOIN tweet_volumeSQL b ON DATE_FORMAT(a.date, 'Y-M-dd') = b.date \\\n",
    "                       LEFT OUTER JOIN avg_likesSQL c ON b.date = c.date \\\n",
    "                       LEFT OUTER JOIN avg_retweetsSQL d ON c.date = d.date \\\n",
    "                       LEFT OUTER JOIN avg_engagement_rateSQL e ON d.date = e.date \\\n",
    "                       LEFT OUTER JOIN number_of_influencersSQL f ON e.date = f.date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "874c8114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:(141 + 2) / 143][Stage 171:>  (0 + 6) / 9][Stage 173:>  (0 + 0) / 9]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:59:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:==>(7 + 2) / 9][Stage 173:>  (0 + 6) / 9][Stage 175:>  (0 + 0) / 9]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:59:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:==>(8 + 1) / 9][Stage 173:>  (0 + 7) / 9][Stage 175:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:>  (0 + 8) / 9][Stage 177:>  (0 + 0) / 9][Stage 179:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:>  (0 + 8) / 9][Stage 182:>  (0 + 0) / 9][Stage 184:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:59:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 18:59:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+------------------+------------------+--------------------+-----------+\n",
      "|      date|dependent_vegan|tweet_volume|         avg_likes|      avg_retweets| avg_engagement_rate|influencers|\n",
      "+----------+---------------+------------+------------------+------------------+--------------------+-----------+\n",
      "|2021-11-03|              1|        3316|15.065796937039138| 8.869379014989294|   0.342346333831607|        180|\n",
      "| 2022-8-15|              0|        1456| 22.95566502463054|        8.76171875|0.061455990873796947|        209|\n",
      "| 2022-3-03|              0|          26|2.6470588235294117|               1.0| 0.03432893276873259|          1|\n",
      "|2021-10-25|              0|          50|           4.65625|               3.0|0.035312352622552404|          0|\n",
      "| 2022-6-13|              0|         108| 7.879310344827586| 2.675675675675676| 0.03734894322976568|         13|\n",
      "| 2022-8-14|              1|        1194|13.847765363128492| 5.109913793103448| 0.19406459767799963|        160|\n",
      "| 2022-7-07|              0|        1464|13.913098236775818| 5.949820788530466| 0.13766857497340554|        143|\n",
      "| 2022-5-26|              1|        1374|14.552009456264775| 4.858508604206501| 0.09708207820297729|        138|\n",
      "| 2022-3-13|              1|        2192|16.486307053941907|  8.30173775671406| 0.10631460693901569|        216|\n",
      "|2021-10-27|              0|         894|11.645669291338583| 5.993006993006993| 0.07417525789424631|         47|\n",
      "| 2022-7-15|              1|         543|12.907348242811501|          7.578125| 0.05936278434091265|         53|\n",
      "| 2022-1-10|              0|        1886|13.497196261682243| 4.737752161383285| 0.07403917200837887|        169|\n",
      "| 2022-5-01|              1|        1390|21.418793503480277| 5.600355239786857| 0.13110014483976745|        192|\n",
      "| 2022-4-16|              1|        1647|24.454965357967666| 5.566225165562914| 0.09146007096141294|        167|\n",
      "| 2022-8-03|              1|       12577|14.165961049957662|6.6615732880588565| 0.13662348648309613|        567|\n",
      "| 2022-8-31|              1|        1501|13.120567375886525| 6.333333333333333|0.058587810993614986|        149|\n",
      "| 2022-5-29|              1|        1190| 17.95277777777778| 6.856548856548857| 0.08050480900482963|        138|\n",
      "|2021-12-06|              0|        1336| 11.53735255570118| 7.503703703703704| 0.06278172079703152|         26|\n",
      "| 2022-9-07|              0|        1869|14.073770491803279| 6.209174311926605|  0.1402499567936055|        176|\n",
      "| 2022-9-03|              1|        1520|21.922273781902554|6.2642487046632125|   0.092151345146165|        159|\n",
      "| 2022-6-03|              1|          32|             2.125|              1.75| 0.27455357142857145|          4|\n",
      "| 2022-6-26|              1|        1661| 15.54229934924078| 5.784070796460177| 0.07833840855724218|        199|\n",
      "| 2022-5-31|              0|        1859|        13.2265625| 5.772563176895307| 0.05991537985721991|        173|\n",
      "| 2022-2-20|              0|        1521|21.311184939091916|   8.2756052141527|  0.1372551718883705|        134|\n",
      "| 2022-5-16|              0|        1428| 11.01508120649652|3.4232209737827715| 0.09643307464924718|        158|\n",
      "| 2022-2-16|              0|        2293| 12.76432078559738| 5.492668621700879| 0.06176692400126486|        102|\n",
      "|2021-12-08|              1|       13077| 14.52754383542731| 6.731100963977676|  0.1298943012098277|        584|\n",
      "| 2022-2-17|              0|        2435| 18.71583850931677| 7.859281437125748| 0.07585945142544791|        120|\n",
      "|2021-11-02|              0|       17623| 8.794319501636576| 4.722175732217573| 0.06331765741485894|        887|\n",
      "|2022-10-09|              0|        1073|17.855243722304284| 6.402247191011236| 0.07716835762591454|        156|\n",
      "| 2022-5-27|              1|        3394|12.392371995820271| 7.867243867243867|  0.3672824548825954|        240|\n",
      "| 2022-4-18|              0|        1468|15.387931034482758| 4.952095808383233| 0.08870974252784747|        163|\n",
      "| 2022-6-22|              0|        1380|17.238156209987196|             7.024| 0.07550628145897935|        163|\n",
      "| 2022-2-06|              0|        1760| 18.36587982832618|  9.45138888888889| 0.06273289791758416|        203|\n",
      "| 2022-6-21|              0|        1487|18.527093596059114| 7.765656565656566| 0.08430649636276823|        159|\n",
      "| 2022-2-13|              1|        2093|28.390794979079498|10.369337979094077|  0.1214225225437001|         88|\n",
      "| 2022-1-23|              1|        2045|14.484546360917248| 5.623762376237623|0.059115330058682344|        188|\n",
      "| 2022-8-19|              1|        1476|18.737032569360675| 7.833333333333333| 0.10941709360652307|        149|\n",
      "| 2022-4-27|              1|       14169|15.562800709040264| 7.318474492322932| 0.20574694416206676|        689|\n",
      "| 2022-4-15|              1|        1517|22.668817204301074| 9.522314049586777| 0.17091970834254472|        208|\n",
      "| 2022-4-01|              1|         812|  7.86737400530504|3.5846153846153848| 0.09401340746860037|         72|\n",
      "| 2022-3-05|              1|        4183|13.412153782554775| 7.975609756097561| 0.05549248297539281|        198|\n",
      "| 2022-8-07|              1|        1827| 12.46611909650924| 3.520249221183801| 0.08942101075233219|        106|\n",
      "| 2022-6-25|              1|        1275| 12.34217877094972| 4.467706013363029| 0.10761285020353922|        141|\n",
      "| 2022-1-27|              0|        1900| 6.911342894393742|2.9566160520607374| 0.05374755797780658|        136|\n",
      "|2021-12-13|              0|        2930| 13.18048469387755| 5.057142857142857| 0.14477246759511125|        142|\n",
      "| 2022-6-02|              0|        1345| 7.141391106043329|3.3876651982378854| 0.06798741045254017|        147|\n",
      "| 2022-2-23|              1|        1890|11.810526315789474| 3.839779005524862|  0.0858829425445421|        194|\n",
      "| 2022-8-28|              1|        1287|13.312320916905444|3.8951612903225805| 0.07771012701787186|        151|\n",
      "| 2022-7-17|              0|        1294| 20.82529335071708| 6.892057026476579| 0.16291500534159734|        157|\n",
      "+----------+---------------+------------+------------------+------------------+--------------------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "basetable.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be427e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required functions\n",
    "from pyspark.ml.feature import Binarizer, StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de196982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define string indexer to index price \n",
    "SI = StringIndexer(inputCol = 'dependent_vegan', outputCol = 'label')\n",
    "\n",
    "# define vector assembler for numeric variables\n",
    "numColumns = ['avg_likes','avg_retweets','avg_engagement_rate','influencers']\n",
    "VAnum = VectorAssembler(inputCols=numColumns, outputCol=\"numFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b432f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 268:(142 + 1) / 143][Stage 272:>  (0 + 7) / 9][Stage 274:>  (0 + 0) / 9] \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:08:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:==>(6 + 3) / 9][Stage 274:>  (0 + 5) / 9][Stage 276:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:08:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:>  (0 + 8) / 9][Stage 278:>  (0 + 0) / 9][Stage 280:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:08:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 280:>  (0 + 8) / 9][Stage 283:>  (0 + 0) / 9][Stage 285:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:08:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:08:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# define pipeline stages\n",
    "stages = [SI, VAnum]\n",
    "# define pipeline and fit on data\n",
    "preprocessingPipeline = Pipeline().setStages(stages).fit(basetable)\n",
    "# apply pipeline on data\n",
    "basetable = preprocessingPipeline.transform(basetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87f641a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "basetable = basetable.select([\"numFeatures\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "935b2ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 375:(142 + 1) / 143][Stage 381:>  (0 + 7) / 9][Stage 383:>  (0 + 0) / 9] \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 383:>  (0 + 8) / 9][Stage 385:>  (0 + 0) / 9][Stage 387:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 387:>  (0 + 8) / 9][Stage 390:>  (0 + 0) / 9][Stage 393:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:14:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:14:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         numFeatures|label|\n",
      "+--------------------+-----+\n",
      "|[15.0657969370391...|  0.0|\n",
      "|[22.9556650246305...|  1.0|\n",
      "|[2.64705882352941...|  1.0|\n",
      "|[4.65625,3.0,0.03...|  1.0|\n",
      "|[7.87931034482758...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "basetable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73cf59",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "- Split the data in a train and test set (70/30).\n",
    "- Build one pipeline that:\n",
    "  - standardizes the numerical variables\n",
    "  - applies a logistic regression to the data\n",
    "  - check the performance using the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a3a93",
   "metadata": {},
   "source": [
    "We cannot use the randomsplit function, because we have time series data, so we have to use another approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8ec34",
   "metadata": {},
   "source": [
    "First we look at the amount of observations that will be assigned to the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dece8543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 476:(141 + 2) / 143][Stage 478:==>(6 + 3) / 9][Stage 480:>  (0 + 3) / 9] \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:21:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 480:==>(8 + 1) / 9][Stage 482:>  (0 + 7) / 9][Stage 484:>  (0 + 0) / 9]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 482:==>(8 + 1) / 9][Stage 484:>  (0 + 7) / 9][Stage 486:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 488:>  (0 + 8) / 9][Stage 491:>  (0 + 0) / 9][Stage 493:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:21:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:21:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_train = int(basetable.count()*0.7)\n",
    "nr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5eb92",
   "metadata": {},
   "source": [
    "convert the final basetable to a pandas dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52568ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 583:(140 + 3) / 143][Stage 585:==>(6 + 3) / 9][Stage 587:>  (0 + 2) / 9]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:27:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 583:(142 + 1) / 143][Stage 587:==>(6 + 3) / 9][Stage 589:>  (0 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:27:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 589:==>(7 + 2) / 9][Stage 591:>  (0 + 6) / 9][Stage 593:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:27:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 593:==>(8 + 1) / 9][Stage 595:>  (0 + 7) / 9][Stage 598:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:27:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:27:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numFeatures</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15.065796937039138, 8.869379014989294, 0.3423...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[22.95566502463054, 8.76171875, 0.061455990873...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2.6470588235294117, 1.0, 0.03432893276873259,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4.65625, 3.0, 0.035312352622552404, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[7.879310344827586, 2.675675675675676, 0.03734...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         numFeatures  label\n",
       "0  [15.065796937039138, 8.869379014989294, 0.3423...    0.0\n",
       "1  [22.95566502463054, 8.76171875, 0.061455990873...    1.0\n",
       "2  [2.6470588235294117, 1.0, 0.03432893276873259,...    1.0\n",
       "3          [4.65625, 3.0, 0.035312352622552404, 0.0]    1.0\n",
       "4  [7.879310344827586, 2.675675675675676, 0.03734...    1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_pd = basetable.toPandas()\n",
    "basetable_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaf526",
   "metadata": {},
   "source": [
    "Split the dataframe into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9be927ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wouterdewitte/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/wouterdewitte/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "train_pd = basetable_pd.iloc[:nr_train,:]\n",
    "test_pd = basetable_pd.iloc[nr_train:,:]\n",
    "train = spark.createDataFrame(train_pd)\n",
    "test = spark.createDataFrame(test_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c37d22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 670:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check number of observations in train and test set\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42e4c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 690:(140 + 3) / 143][Stage 692:==>(6 + 3) / 9][Stage 694:>  (0 + 2) / 9]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:33:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 696:==>(6 + 3) / 9][Stage 698:>  (0 + 5) / 9][Stage 700:>  (0 + 0) / 9]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:33:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 698:>  (0 + 8) / 9][Stage 700:>  (0 + 0) / 9][Stage 702:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:33:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 700:==>(7 + 2) / 9][Stage 702:>  (0 + 6) / 9][Stage 705:>  (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/07 19:33:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  121|\n",
      "|  1.0|  115|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   84|\n",
      "|  1.0|   81|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   37|\n",
      "|  1.0|   34|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect distribution of label in train and test set\n",
    "basetable.groupBy(\"label\").count().show()\n",
    "train.groupBy(\"label\").count().show()\n",
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89c3e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required features\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "609168ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "SS = StandardScaler(inputCol = 'numFeatures', outputCol = 'scaledNumFeatures', withStd = True, withMean = False)\n",
    "\n",
    "# define vector assembler\n",
    "VA = VectorAssembler(inputCols = ['scaledNumFeatures'], outputCol = 'features')\n",
    "\n",
    "# define logistic regression model\n",
    "LR = LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b271625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:34:01 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "22/12/07 19:34:01 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/12/07 19:34:01 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "# define pipeline stages\n",
    "stages = [SS, VA, LR]\n",
    "# create pipeline and fit on training set\n",
    "lrModelPipeline = Pipeline().setStages(stages).fit(train)\n",
    "# apply pipeline on test set to get predictions\n",
    "predictions = lrModelPipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce473b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|         numFeatures|label|   scaledNumFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|[5.49019607843137...|  1.0|[1.03296060626405...|[1.03296060626405...|[-0.0504193216419...|[0.48739783914756...|       1.0|\n",
      "|[12.6830835117773...|  1.0|[2.38627645469564...|[2.38627645469564...|[-0.1032990966531...|[0.47419816540094...|       1.0|\n",
      "|[11.2695214105793...|  0.0|[2.12031983963381...|[2.12031983963381...|[0.25179346522256...|[0.56261788446566...|       0.0|\n",
      "|[42.0873859045338...|  1.0|[7.91858998093160...|[7.91858998093160...|[-0.1845362087328...|[0.45399642278111...|       1.0|\n",
      "|[17.9372881355932...|  0.0|[3.37483611925374...|[3.37483611925374...|[-0.0175440398049...|[0.49561410254401...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect predictions\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd3e553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC lr: 0.503975\n"
     ]
    }
   ],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# get evaluation metric\n",
    "lrAUC = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "# inspect model performance\n",
    "print('AUC lr: %f' %(lrAUC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
