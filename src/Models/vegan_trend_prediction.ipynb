{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76508cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "# initialize findspark with spark directory\n",
    "findspark.init(\"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\")\n",
    "#findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99192d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import emojis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dafa38",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dcdae",
   "metadata": {},
   "source": [
    "In this notebook we will buid a model that predicts if the trend of a certain topic goes up or down on a certain day based on Twitter data of that day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1943e2",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa5e6",
   "metadata": {},
   "source": [
    "### 1.1 Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a3d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read trend data \n",
    "trend = spark.read.csv(\".././../data/Google_trends/daily_trends.csv\", header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be47e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, dependent_vegan: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6b34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|               date|dependent_vegan|\n",
      "+-------------------+---------------+\n",
      "|2021-10-04 00:00:00|              1|\n",
      "|2021-10-05 00:00:00|              1|\n",
      "|2021-10-06 00:00:00|              1|\n",
      "|2021-10-07 00:00:00|              1|\n",
      "|2021-10-08 00:00:00|              1|\n",
      "|2021-10-09 00:00:00|              0|\n",
      "|2021-10-10 00:00:00|              0|\n",
      "|2021-10-11 00:00:00|              0|\n",
      "|2021-10-12 00:00:00|              0|\n",
      "|2021-10-13 00:00:00|              0|\n",
      "|2021-10-14 00:00:00|              0|\n",
      "|2021-10-15 00:00:00|              1|\n",
      "|2021-10-16 00:00:00|              1|\n",
      "|2021-10-17 00:00:00|              0|\n",
      "|2021-10-18 00:00:00|              1|\n",
      "|2021-10-19 00:00:00|              0|\n",
      "|2021-10-20 00:00:00|              0|\n",
      "|2021-10-21 00:00:00|              1|\n",
      "|2021-10-22 00:00:00|              1|\n",
      "|2021-10-23 00:00:00|              1|\n",
      "+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy().orderBy(col(\"date\"))\n",
    "trend.withColumn(\"dependent_vegan\", lag(\"dependent_vegan\", -1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb226e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "trend.createOrReplaceTempView(\"trendSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b9d77",
   "metadata": {},
   "source": [
    "The binary variable indicates if the trend goes up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aff149",
   "metadata": {},
   "source": [
    "### 1.2 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795db881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data dir\n",
    "data_dir = \"../../data/Topic/\"\n",
    "\n",
    "# get all twitter files\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2953fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter data \n",
    "#twitter_df = spark.read.json(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ce5a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_hashtags = [\"vegan\"]\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "files_hashtags = [file for file in tweet_files if (file.find(list_hashtags[0]) != -1)]             \n",
    "twitter_df = spark.read.option(\"multiline\",\"true\").json(files_hashtags) \n",
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select interesting features\n",
    "twitter_df = twitter_df.select(F.col('user.name'),\n",
    "                                F.col('user.screen_name'),\n",
    "                                F.col('user.followers_count'),\n",
    "                                F.col('user.following'),\n",
    "                                F.col('user.statuses_count'),\n",
    "                                F.col('user.listed_count'),\n",
    "                                F.col('created_at'),\n",
    "                                F.col('full_text'),\n",
    "                                F.col('entities.hashtags'),\n",
    "                                F.col('favorite_count'),\n",
    "                                F.col('retweet_count'),\n",
    "                                F.col('user.friends_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad4622",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9dd70",
   "metadata": {},
   "source": [
    "### 2.1 Check time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96607a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "twitter_df = twitter_df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "760e9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           earliest|             latest|\n",
      "+-------------------+-------------------+\n",
      "|2021-10-25 07:19:40|2022-10-11 23:17:33|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get first post\n",
    "first_post = F.min('post_created_at').alias('earliest')\n",
    "# get latest post\n",
    "latest_post = F.max('post_created_at').alias('latest')\n",
    "# show tweet period in our dataset\n",
    "twitter_df.select(first_post, latest_post).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f4e9b",
   "metadata": {},
   "source": [
    "### 2.2 Remove retweets and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1db1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all retweets from dataset\n",
    "no_retweets_df = twitter_df.filter(~F.col(\"full_text\").startswith(\"RT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0be0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sort no_retweets_df based on date in chronological order (most recent ones on top)\n",
    "no_retweets_sorted_df = no_retweets_df.sort(\"post_created_at\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276c763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745916"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations before dropping duplicates\n",
    "no_retweets_sorted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates based on tweet text and the profile it was posted from\n",
    "final_no_duplicates_df = no_retweets_sorted_df.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a33594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693932"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations after dropping duplicates\n",
    "final_no_duplicates_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908138e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename dataframe\n",
    "final_twitter_df = final_no_duplicates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6316",
   "metadata": {},
   "source": [
    "## 3. Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165908d",
   "metadata": {},
   "source": [
    "For our independent variables we need to design a pipeline that transforms the data into the desired aggregated metrics per day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a928c",
   "metadata": {},
   "source": [
    "### 3.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad6e83",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37e0ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29094c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8902d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd8f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count number of emojis used\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a83aa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate engagement rate\n",
    "def engagement_rate(favorite_count, retweet_count, followers_count):\n",
    "    if(followers_count == 0):\n",
    "        eng_rate = 0\n",
    "    else:\n",
    "        eng_rate = (favorite_count + retweet_count)/followers_count\n",
    "    \n",
    "    return eng_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c98452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_UDF = F.udf(emoji_counter, IntegerType())\n",
    "engagement_rate_UDF = F.udf(engagement_rate, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b519d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions to create new features\n",
    "final_twitter_df = final_twitter_df.withColumn(\"emoji_count\", emoji_counter_UDF(\"full_text\")) \\\n",
    "        .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "        .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "        .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "        .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "        .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\")) \\\n",
    "        .withColumn(\"engagement_rate\", engagement_rate_UDF(\"favorite_count\", \"retweet_count\", \"followers_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574b0c2",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b483df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english tweets (NOTE: for the assignment you can translate non-english tweets using an API)\n",
    "final_twitter_df = final_twitter_df.filter(F.col(\"lang\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72531af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469429"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of observations\n",
    "final_twitter_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21463c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78be9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to udf\n",
    "clean_text_udf = F.udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10aa5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string\n",
    "final_twitter_df = final_twitter_df.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da1c608f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! We will be open 10am-9pm today and tomorrow!! Much love you all!!\\n\\n#veganuary #veganuary2022 #vegan #veganfood #veganlife #vkind  #vegas #lasvegas #vegoutvegas #plantbased #veganfood  #food #crueltyfree #healthy #organic #veganmarket #foodie #govegan #vegansofig #love #veganism</td>\n",
       "      <td>we will be open ampm today and tomorrow much love you all veganuary veganuary vegan veganfood veganlife vkind vegas lasvegas vegoutvegas plantbased veganfood food crueltyfree healthy organic veganmarket foodie govegan vegansofig love veganism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!! Daily Updates !!\\n\\nA taste of Vegan Food today on #TheMorningWave with @sam_lehoko and @Lauri_Leah on #ChefsTable joined by the team from Kaylee's Eatery.\\n\\n#MyMusicMyMix #Music #Food #Vegan #Green #Wednesday #Radio https://t.co/UUNQwkDWxt</td>\n",
       "      <td>daily updates taste of vegan food today on themorningwave with sam lehoko and lauri leah on chefstable joined by the team from kaylees eatery mymusicmymix music food vegan green wednesday radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!love !iq !waddup Building time on the Sketch SMP! Emote Art Raffle at 5 Subs! https://t.co/aKZoWy32dW @CozyIslandLIVE @Streambeanzttv #twitch #TwitchStreamers #twitchstreamer #twitchtv #twitchaffiliate #Livestream #live #LGBTQ #LGBTQIA #Vegan #Minecraft #minecraftsmp #cozy #lgbt</td>\n",
       "      <td>love iq waddup building time on the sketch smp emote art raffle at subs cozyislandlive streambeanzttv twitch twitchstreamers twitchstreamer twitchtv twitchaffiliate livestream live lgbtq lgbtqia vegan minecraft minecraftsmp cozy lgbt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" DID YOU KNOW ?\"\\n\\n#senoritacosmetics #senoritacosmetics_ #skincareroutine #veganskincare #vegan #skincare #skincaretips #gogreen #govegan #facts #funfact #funfactory #funfactsoftheday https://t.co/eodzSAXqH8</td>\n",
       "      <td>did you know senoritacosmetics senoritacosmetics skincareroutine veganskincare vegan skincare skincaretips gogreen govegan facts funfact funfactory funfactsoftheday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" Nature's Interconnection \" \\n\\nCommission for @Stefanostattoos\\n\\nTattoo on @apound_of_flesh\\n\\n#inari #inariokami #fox #foxes #redhead #mädchen #vegan #veganism #woman #women #nature #peru #peruvian #tatuador  #tatuaje #tatuajes #тату #plants #plantbased  #dorsettstyle #веган https://t.co/2G7IiLiWWW</td>\n",
       "      <td>natures interconnection commission for stefanostattoos tattoo on apound of flesh inari inariokami fox foxes redhead mädchen vegan veganism woman women nature peru peruvian tatuador tatuaje tatuajes тату plants plantbased dorsettstyle веган</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         full_text  \\\n",
       "0                       ! We will be open 10am-9pm today and tomorrow!! Much love you all!!\\n\\n#veganuary #veganuary2022 #vegan #veganfood #veganlife #vkind  #vegas #lasvegas #vegoutvegas #plantbased #veganfood  #food #crueltyfree #healthy #organic #veganmarket #foodie #govegan #vegansofig #love #veganism   \n",
       "1                                                             !! Daily Updates !!\\n\\nA taste of Vegan Food today on #TheMorningWave with @sam_lehoko and @Lauri_Leah on #ChefsTable joined by the team from Kaylee's Eatery.\\n\\n#MyMusicMyMix #Music #Food #Vegan #Green #Wednesday #Radio https://t.co/UUNQwkDWxt   \n",
       "2                         !love !iq !waddup Building time on the Sketch SMP! Emote Art Raffle at 5 Subs! https://t.co/aKZoWy32dW @CozyIslandLIVE @Streambeanzttv #twitch #TwitchStreamers #twitchstreamer #twitchtv #twitchaffiliate #Livestream #live #LGBTQ #LGBTQIA #Vegan #Minecraft #minecraftsmp #cozy #lgbt   \n",
       "3                                                                                               \" DID YOU KNOW ?\"\\n\\n#senoritacosmetics #senoritacosmetics_ #skincareroutine #veganskincare #vegan #skincare #skincaretips #gogreen #govegan #facts #funfact #funfactory #funfactsoftheday https://t.co/eodzSAXqH8   \n",
       "4  \" Nature's Interconnection \" \\n\\nCommission for @Stefanostattoos\\n\\nTattoo on @apound_of_flesh\\n\\n#inari #inariokami #fox #foxes #redhead #mädchen #vegan #veganism #woman #women #nature #peru #peruvian #tatuador  #tatuaje #tatuajes #тату #plants #plantbased  #dorsettstyle #веган https://t.co/2G7IiLiWWW   \n",
       "\n",
       "                                                                                                                                                                                                                                         cleaned_text  \n",
       "0  we will be open ampm today and tomorrow much love you all veganuary veganuary vegan veganfood veganlife vkind vegas lasvegas vegoutvegas plantbased veganfood food crueltyfree healthy organic veganmarket foodie govegan vegansofig love veganism  \n",
       "1                                                   daily updates taste of vegan food today on themorningwave with sam lehoko and lauri leah on chefstable joined by the team from kaylees eatery mymusicmymix music food vegan green wednesday radio  \n",
       "2           love iq waddup building time on the sketch smp emote art raffle at subs cozyislandlive streambeanzttv twitch twitchstreamers twitchstreamer twitchtv twitchaffiliate livestream live lgbtq lgbtqia vegan minecraft minecraftsmp cozy lgbt  \n",
       "3                                                                                did you know senoritacosmetics senoritacosmetics skincareroutine veganskincare vegan skincare skincaretips gogreen govegan facts funfact funfactory funfactsoftheday  \n",
       "4     natures interconnection commission for stefanostattoos tattoo on apound of flesh inari inariokami fox foxes redhead mädchen vegan veganism woman women nature peru peruvian tatuador tatuaje tatuajes тату plants plantbased dorsettstyle веган  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "final_twitter_df.select(\"full_text\", \"cleaned_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270e572",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a67623",
   "metadata": {},
   "source": [
    "VADER sentimental analysis relies on a dictionary that maps lexical features to emotion intensities known as sentiment scores. The sentiment score of a text can be obtained by summing up the intensity of each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae5c817a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using the vaderSentiment package \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c91ff1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)\n",
    "\n",
    "get_sentiment_udf = udf(get_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "006cc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_twitter_df = final_twitter_df.withColumn(\"sentiment_vader\", get_sentiment_udf(F.col(\"cleaned_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4304a09d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|        cleaned_text|sentiment_vader|\n",
      "+--------------------+---------------+\n",
      "|we will be open a...|           0.27|\n",
      "|daily updates tas...|            0.0|\n",
      "|love iq waddup bu...|          0.127|\n",
      "|did you know seno...|            0.0|\n",
      "|natures interconn...|            0.0|\n",
      "|scarlet punica gr...|            0.0|\n",
      "|dairy milk no lon...|            0.0|\n",
      "|greenwashing is r...|            0.0|\n",
      "|greenwashing is r...|            0.0|\n",
      "|newyorkcity schoo...|          0.128|\n",
      "|vegan food is dis...|            0.0|\n",
      "|vegan food’s abil...|          0.064|\n",
      "|my bad cholestero...|          0.054|\n",
      "|vegan festival ho...|          0.132|\n",
      "|dont be too hard ...|          0.396|\n",
      "|steve brexit stev...|          0.077|\n",
      "|it becomes questi...|          0.211|\n",
      "|or vegan ones bra...|            0.0|\n",
      "|vita russia anima...|          0.068|\n",
      "|of surveyed swine...|            0.0|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_twitter_df.select(\"cleaned_text\", \"sentiment_vader\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cf496",
   "metadata": {},
   "source": [
    "#### TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4cd7e0",
   "metadata": {},
   "source": [
    "TextBlob returns polarity and subjectivity of a sentence. \n",
    "\n",
    "**Polarity** lies between [-1,1],  -1 defines a negative sentiment and 1 defines a positive sentiment.  \n",
    "\n",
    "**Subjectivity** quantifies the amount of personal opinion and factual information contained in the text. Subjectivity lies between [0,1]. The higher subjectivity means that the text contains personal opinion rather than factual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ca266e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+-------------------+\n",
      "|        cleaned_text|sentiment_vader|            polarity|       subjectivity|\n",
      "+--------------------+---------------+--------------------+-------------------+\n",
      "|we will be open a...|           0.27|               0.375|               0.55|\n",
      "|daily updates tas...|            0.0|                -0.1|               0.15|\n",
      "|love iq waddup bu...|          0.127| 0.14545454545454548| 0.6166666666666667|\n",
      "|did you know seno...|            0.0|                 0.0|                0.0|\n",
      "|natures interconn...|            0.0|                 0.0|                0.0|\n",
      "|scarlet punica gr...|            0.0|                 0.0|                0.0|\n",
      "|dairy milk no lon...|            0.0|                 0.0|                0.0|\n",
      "|greenwashing is r...|            0.0|                 0.0|                0.0|\n",
      "|greenwashing is r...|            0.0|                 0.0|                0.0|\n",
      "|newyorkcity schoo...|          0.128| 0.13333333333333333| 0.7333333333333334|\n",
      "|vegan food is dis...|            0.0|                -1.0|                1.0|\n",
      "|vegan food’s abil...|          0.064|                -0.2|                0.2|\n",
      "|my bad cholestero...|          0.054|-0.23333333333333328| 0.5222222222222221|\n",
      "|vegan festival ho...|          0.132| 0.11249999999999999|             0.5875|\n",
      "|dont be too hard ...|          0.396| 0.24166666666666664| 0.6683333333333332|\n",
      "|steve brexit stev...|          0.077|-0.18333333333333335| 0.7999999999999999|\n",
      "|it becomes questi...|          0.211|  0.4166666666666667|                0.5|\n",
      "|or vegan ones bra...|            0.0|                 0.0|                0.0|\n",
      "|vita russia anima...|          0.068| 0.13333333333333333|0.19999999999999998|\n",
      "|of surveyed swine...|            0.0| -0.3666666666666667| 0.5190476190476191|\n",
      "+--------------------+---------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use polarity and subjectivity from TextBlob \n",
    "#https://textblob.readthedocs.io/en/dev/\n",
    "from textblob import TextBlob\n",
    "\n",
    "# define function to get polarity score of text document\n",
    "def get_polarity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[0]\n",
    "# define function to get subjectivity score of text document\n",
    "def get_subjectivity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[1]\n",
    "get_polarity_udf = F.udf(get_polarity, DoubleType())\n",
    "get_subjectivity_udf = F.udf(get_subjectivity, DoubleType())\n",
    "\n",
    "final_twitter_df = final_twitter_df.withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "        .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))\n",
    "\n",
    "final_twitter_df.select(\"cleaned_text\", \"sentiment_vader\", \"polarity\", \"subjectivity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60590543",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_twitter_df = final_twitter_df.withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "        .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edaa86eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+-------------------+\n",
      "|        cleaned_text|sentiment_vader|            polarity|       subjectivity|\n",
      "+--------------------+---------------+--------------------+-------------------+\n",
      "|we will be open a...|           0.27|               0.375|               0.55|\n",
      "|daily updates tas...|            0.0|                -0.1|               0.15|\n",
      "|love iq waddup bu...|          0.127| 0.14545454545454548| 0.6166666666666667|\n",
      "|did you know seno...|            0.0|                 0.0|                0.0|\n",
      "|natures interconn...|            0.0|                 0.0|                0.0|\n",
      "|scarlet punica gr...|            0.0|                 0.0|                0.0|\n",
      "|dairy milk no lon...|            0.0|                 0.0|                0.0|\n",
      "|greenwashing is r...|            0.0|                 0.0|                0.0|\n",
      "|greenwashing is r...|            0.0|                 0.0|                0.0|\n",
      "|newyorkcity schoo...|          0.128| 0.13333333333333333| 0.7333333333333334|\n",
      "|vegan food is dis...|            0.0|                -1.0|                1.0|\n",
      "|vegan food’s abil...|          0.064|                -0.2|                0.2|\n",
      "|my bad cholestero...|          0.054|-0.23333333333333328| 0.5222222222222221|\n",
      "|vegan festival ho...|          0.132| 0.11249999999999999|             0.5875|\n",
      "|dont be too hard ...|          0.396| 0.24166666666666664| 0.6683333333333332|\n",
      "|steve brexit stev...|          0.077|-0.18333333333333335| 0.7999999999999999|\n",
      "|it becomes questi...|          0.211|  0.4166666666666667|                0.5|\n",
      "|or vegan ones bra...|            0.0|                 0.0|                0.0|\n",
      "|vita russia anima...|          0.068| 0.13333333333333333|0.19999999999999998|\n",
      "|of surveyed swine...|            0.0| -0.3666666666666667| 0.5190476190476191|\n",
      "+--------------------+---------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_twitter_df.select(\"cleaned_text\", \"sentiment_vader\", \"polarity\", \"subjectivity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9d72eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "final_twitter_df.createOrReplaceTempView(\"twitterSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cf7e6",
   "metadata": {},
   "source": [
    "### 3.1 Volume of tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ff5598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "volume = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, COUNT(*) as volume \\\n",
    "                                    FROM twitterSQL \\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffa7816d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|volume|\n",
      "+----------+------+\n",
      "|2021-10-25|     2|\n",
      "|2021-10-27|   396|\n",
      "|2021-10-28|  1007|\n",
      "|2021-10-29|  9701|\n",
      "|2021-10-30|  8811|\n",
      "|2021-10-31|  8602|\n",
      "|2021-11-01| 17663|\n",
      "|2021-11-02| 12826|\n",
      "|2021-11-03|  1549|\n",
      "|2021-11-04|   966|\n",
      "|2021-11-05|   186|\n",
      "|2021-11-06|     5|\n",
      "|2021-12-05|    12|\n",
      "|2021-12-06|    36|\n",
      "|2021-12-07|  2273|\n",
      "|2021-12-08|  8557|\n",
      "|2021-12-09|  8547|\n",
      "|2021-12-10|  9438|\n",
      "|2021-12-11|  8224|\n",
      "|2021-12-12|  6558|\n",
      "|2021-12-13|  1135|\n",
      "|2021-12-14|  1154|\n",
      "|2021-12-15|  1190|\n",
      "|2021-12-16|    89|\n",
      "|2021-12-25|   495|\n",
      "| 2022-1-01|  1425|\n",
      "| 2022-1-02|   707|\n",
      "| 2022-1-08|  1022|\n",
      "| 2022-1-09|  1226|\n",
      "| 2022-1-10|  1433|\n",
      "| 2022-1-11|  1464|\n",
      "| 2022-1-12|  1561|\n",
      "| 2022-1-13|  1358|\n",
      "| 2022-1-14|  1580|\n",
      "| 2022-1-15|  1185|\n",
      "| 2022-1-16|   167|\n",
      "| 2022-1-20|  1113|\n",
      "| 2022-1-21|  1904|\n",
      "| 2022-1-22|  1297|\n",
      "| 2022-1-23|  1611|\n",
      "| 2022-1-24|  1819|\n",
      "| 2022-1-25|  1337|\n",
      "| 2022-1-26|  1819|\n",
      "| 2022-1-27|  1490|\n",
      "| 2022-1-31|    71|\n",
      "|2022-10-09|   730|\n",
      "|2022-10-10|   972|\n",
      "|2022-10-11|   990|\n",
      "|2022-12-26|   796|\n",
      "|2022-12-27|  1082|\n",
      "|2022-12-28|  1219|\n",
      "|2022-12-29|  1397|\n",
      "|2022-12-30|  1050|\n",
      "|2022-12-31|  1206|\n",
      "| 2022-2-01|  1263|\n",
      "| 2022-2-02|  1419|\n",
      "| 2022-2-03|  1191|\n",
      "| 2022-2-04|  1411|\n",
      "| 2022-2-05|  1759|\n",
      "| 2022-2-06|  1273|\n",
      "| 2022-2-07|  1284|\n",
      "| 2022-2-08|   913|\n",
      "| 2022-2-11|   338|\n",
      "| 2022-2-12|   658|\n",
      "| 2022-2-13|   817|\n",
      "| 2022-2-14|   778|\n",
      "| 2022-2-15|   890|\n",
      "| 2022-2-16|   781|\n",
      "| 2022-2-17|   778|\n",
      "| 2022-2-18|   670|\n",
      "| 2022-2-19|  3876|\n",
      "| 2022-2-20|  1058|\n",
      "| 2022-2-21|  1212|\n",
      "| 2022-2-22|  1339|\n",
      "| 2022-2-23|  1275|\n",
      "| 2022-3-03|     2|\n",
      "| 2022-3-04|     1|\n",
      "| 2022-3-05|  3290|\n",
      "| 2022-3-06|  7843|\n",
      "| 2022-3-07|  8236|\n",
      "| 2022-3-08|  9058|\n",
      "| 2022-3-09|  9019|\n",
      "| 2022-3-10|  8891|\n",
      "| 2022-3-11|  8859|\n",
      "| 2022-3-12|  1531|\n",
      "| 2022-3-13|  1312|\n",
      "| 2022-3-14|  1182|\n",
      "| 2022-3-15|  1137|\n",
      "| 2022-3-16|  1235|\n",
      "| 2022-3-25|   950|\n",
      "| 2022-3-26|  1022|\n",
      "| 2022-3-27|   996|\n",
      "| 2022-3-28|   998|\n",
      "| 2022-3-29|  1117|\n",
      "| 2022-3-30|  1130|\n",
      "| 2022-3-31|  1243|\n",
      "| 2022-4-01|   568|\n",
      "| 2022-4-12|    79|\n",
      "| 2022-4-13|  1125|\n",
      "| 2022-4-14|  1093|\n",
      "+----------+------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "volume.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "072ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "volume.createOrReplaceTempView(\"volumeSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb81a7",
   "metadata": {},
   "source": [
    "### 3.2 Average likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e49f2",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28099a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_likes = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(favorite_count) as avg_likes \\\n",
    "                           FROM twitterSQL \\\n",
    "                           WHERE favorite_count > 0 \\\n",
    "                           GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                           ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67edf65d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|         avg_likes|\n",
      "+----------+------------------+\n",
      "|2021-10-25|               5.0|\n",
      "|2021-10-27|14.878923766816143|\n",
      "|2021-10-28| 8.787234042553191|\n",
      "|2021-10-29|13.020252694165737|\n",
      "|2021-10-30|12.171084337349397|\n",
      "|2021-10-31|11.718002081165453|\n",
      "|2021-11-01| 12.87780347983506|\n",
      "|2021-11-02| 8.506925207756233|\n",
      "|2021-11-03| 9.510228640192539|\n",
      "|2021-11-04| 11.72790294627383|\n",
      "|2021-11-05| 3.090909090909091|\n",
      "|2021-11-06|               1.0|\n",
      "|2021-12-05| 4.857142857142857|\n",
      "|2021-12-06|               5.0|\n",
      "|2021-12-07| 15.16415770609319|\n",
      "|2021-12-08|15.566714786553929|\n",
      "|2021-12-09|13.000201653559186|\n",
      "|2021-12-10|18.798192771084338|\n",
      "|2021-12-11|17.180092787853226|\n",
      "|2021-12-12| 8.526501766784452|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_likes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6003b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_likes.createOrReplaceTempView(\"avg_likesSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa4e0b",
   "metadata": {},
   "source": [
    "### 3.3 Average Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff829814",
   "metadata": {},
   "source": [
    "We exclude tweets with 0 retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4b17a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_retweets = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(retweet_count) as avg_retweets \\\n",
    "                          FROM twitterSQL \\\n",
    "                          WHERE retweet_count > 0 \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96a6f578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|      avg_retweets|\n",
      "+----------+------------------+\n",
      "|2021-10-27| 7.695121951219512|\n",
      "|2021-10-28| 3.169230769230769|\n",
      "|2021-10-29| 7.280495759947815|\n",
      "|2021-10-30| 5.383431085043989|\n",
      "|2021-10-31| 6.656727272727273|\n",
      "|2021-11-01|  6.63281027104137|\n",
      "|2021-11-02| 4.268883878241263|\n",
      "|2021-11-03| 3.362934362934363|\n",
      "|2021-11-04| 3.473170731707317|\n",
      "|2021-11-05|1.5476190476190477|\n",
      "|2021-11-06|               1.5|\n",
      "|2021-12-05|               1.0|\n",
      "|2021-12-06|             3.875|\n",
      "|2021-12-07| 7.544502617801047|\n",
      "|2021-12-08| 7.130705394190872|\n",
      "|2021-12-09|7.3532934131736525|\n",
      "|2021-12-10| 8.534136546184738|\n",
      "|2021-12-11|           10.0768|\n",
      "|2021-12-12| 5.430648769574944|\n",
      "|2021-12-13| 5.383954154727793|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_retweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57e76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_retweets.createOrReplaceTempView(\"avg_retweetsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c11de2",
   "metadata": {},
   "source": [
    "### 3.4 Average Engagement rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271c4c1",
   "metadata": {},
   "source": [
    "We define engagement rate of a tweet as the sum of likes and retweets divided by the amount of followers of the account that sent out the tweet. For our purpose we will take the avergage engagement rate per day. We exclude accounts who have no followers and we only take tweets into account which are liked and retweeted at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecee44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_engagement_rate = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(engagement_rate) as avg_engagement_rate \\\n",
    "                                     FROM twitterSQL \\\n",
    "                                     GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                     ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d05ebc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date| avg_engagement_rate|\n",
      "+----------+--------------------+\n",
      "|2021-10-25|0.003094059405940594|\n",
      "|2021-10-27| 0.02134529191518882|\n",
      "|2021-10-28|0.024249144556081494|\n",
      "|2021-10-29|  0.0223016464141139|\n",
      "|2021-10-30|0.026574548037982985|\n",
      "|2021-10-31| 0.04700267284774964|\n",
      "|2021-11-01| 0.02698780408562503|\n",
      "|2021-11-02|0.017589389867347455|\n",
      "|2021-11-03|0.030342320208788704|\n",
      "|2021-11-04|0.026801852188641857|\n",
      "|2021-11-05|0.011570938136072207|\n",
      "|2021-11-06|0.001165479244563...|\n",
      "|2021-12-05|0.003262968586207...|\n",
      "|2021-12-06|0.015643164611561896|\n",
      "|2021-12-07|0.022181606174828457|\n",
      "|2021-12-08| 0.03823743337493997|\n",
      "|2021-12-09|0.026620944322647062|\n",
      "|2021-12-10| 0.04562157050472956|\n",
      "|2021-12-11|0.030933989747992788|\n",
      "|2021-12-12|0.018746239331627782|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_engagement_rate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e2dfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_engagement_rate.createOrReplaceTempView(\"avg_engagement_rateSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea74252",
   "metadata": {},
   "source": [
    "### 3.5 Number of influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77904a",
   "metadata": {},
   "source": [
    "We will calculate how many influencers actively tweeted a certain day. We define an influencer as someone with:\n",
    "- followers > 1000 \n",
    "- engagement_rate > 0.20 \n",
    "- weekly tweet frequency > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54d92a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influencers(follower_count_tresh, eng_rate_tresh, freq_week_tresh, data):\n",
    "\n",
    "    #df\n",
    "    df = data\n",
    "    \n",
    "    # get all users with their amount of followers\n",
    "    influencers = df.groupBy(\"screen_name\") \\\n",
    "                    .agg(first(\"followers_count\").alias(\"followers_count\"))\n",
    "\n",
    "    # average engagement rate for each user\n",
    "    eng_rate = df.withColumn('eng_rate', ((df['favorite_count'] + df['retweet_count'])/df['followers_count']))\n",
    "\n",
    "    eng_rate_user = eng_rate.groupBy(\"screen_name\") \\\n",
    "                            .agg(avg(\"eng_rate\").alias(\"eng_rate\"))\n",
    "\n",
    "    # average freq_weekly per user\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('screen_name', 'year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('screen_name', 'year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select('screen_name', 'freq')\n",
    "\n",
    "    freq_week = freq_week.groupby(\"screen_name\").agg(avg(freq_week.freq).alias('freq'))\n",
    "\n",
    "    # put the data together\n",
    "    data_joined = eng_rate_user.join(influencers, \"screen_name\").join(freq_week, \"screen_name\")\n",
    "\n",
    "    # filter the data\n",
    "    data_joined = data_joined.filter((data_joined.followers_count > follower_count_tresh) & (data_joined.eng_rate > eng_rate_tresh) & (data_joined.freq > freq_week_tresh))\n",
    "    \n",
    "    # show the data\n",
    "    data_joined.show()\n",
    "    return data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16df2e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---------------+------------------+\n",
      "|    screen_name|            eng_rate|followers_count|              freq|\n",
      "+---------------+--------------------+---------------+------------------+\n",
      "|        AQUAB23|0.022003034901365705|           1318|               3.0|\n",
      "|AlsJane_therapy|0.008247976142192238|           6226|               2.5|\n",
      "|AmazingArbuckle|0.003063373540111...|           3482|               3.0|\n",
      "|   AmeliaLynn70|0.014513189093212512|           2234|2.3333333333333335|\n",
      "|AstridAlderleaf|0.002847876012638...|           6320|               4.0|\n",
      "|   BDAWOSBranch|0.002719854941069...|           1103|               3.0|\n",
      "|    BlogofVegan|0.003641420110945388|           9257| 4.653846153846154|\n",
      "|   BrianKateman|0.004763913172491486|           1542|               3.5|\n",
      "|   CathyGreen67|0.003029875597498...|           1161|3.1666666666666665|\n",
      "|   ChubbieVegan|0.003564221783895219|           2012|2.4411764705882355|\n",
      "|      CloseUpPR|0.002352610723569...|           1229|              2.25|\n",
      "|CookingWithJoya|0.014963680978210584|           3969|               2.4|\n",
      "|     DBelardoMD|0.003578721189301...|          67692|               2.5|\n",
      "|DirtyDiceGoblin|0.006253635834787667|           4584|               3.0|\n",
      "|     Donna_Barr|0.002244432928481...|           1188|               3.0|\n",
      "|Grnc_BY_VNation|0.003443245015555...|           6035|              2.25|\n",
      "|      GummiPies|0.048987820419077864|           3171|               3.0|\n",
      "|   JackBradders| 0.03425568909409382|           3064|              3.44|\n",
      "|JohnFisher2dot0|0.002780352177942...|           1079|               3.0|\n",
      "|KarlaIsThriving|0.004578721163760655|           1935|               5.0|\n",
      "+---------------+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "influencers = get_influencers(1000, 0.002, 2, final_twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8acae7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "influencers.createOrReplaceTempView(\"influencersSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d8a7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "number_of_influencers = spark.sql(\" SELECT DATE_FORMAT(a.post_created_at, 'Y-M-dd') as date, COUNT(b.screen_name) as influencers \\\n",
    "                                    FROM twitterSQL a \\\n",
    "                                    RIGHT OUTER JOIN influencersSQL b ON a.screen_name = b.screen_name\\\n",
    "                                    GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "954c3a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      date|influencers|\n",
      "+----------+-----------+\n",
      "|2021-10-27|         32|\n",
      "|2021-10-28|         66|\n",
      "|2021-10-29|        533|\n",
      "|2021-10-30|        400|\n",
      "|2021-10-31|        433|\n",
      "|2021-11-01|        895|\n",
      "|2021-11-02|        638|\n",
      "|2021-11-03|        120|\n",
      "|2021-11-04|         92|\n",
      "|2021-11-05|          9|\n",
      "|2021-12-05|          3|\n",
      "|2021-12-06|          2|\n",
      "|2021-12-07|        188|\n",
      "|2021-12-08|        394|\n",
      "|2021-12-09|        452|\n",
      "|2021-12-10|        547|\n",
      "|2021-12-11|        575|\n",
      "|2021-12-12|        325|\n",
      "|2021-12-13|         82|\n",
      "|2021-12-14|         77|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "number_of_influencers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd978455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"number_of_influencersSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd476ea0",
   "metadata": {},
   "source": [
    "### 3.6 Average Followers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ad4b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_followers = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(followers_count) as avg_followers \\\n",
    "                          FROM twitterSQL \\\n",
    "                          WHERE followers_count > 0 \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bcc8cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|     avg_followers|\n",
      "+----------+------------------+\n",
      "|2021-10-25|            1037.0|\n",
      "|2021-10-27|3704.7251308900522|\n",
      "|2021-10-28|11143.559559559559|\n",
      "|2021-10-29|15421.784556225317|\n",
      "|2021-10-30|13092.364626318202|\n",
      "|2021-10-31| 8639.303048134636|\n",
      "|2021-11-01|14902.429470672389|\n",
      "|2021-11-02|12490.079042670446|\n",
      "|2021-11-03|7811.1375488917865|\n",
      "|2021-11-04| 5468.707240293809|\n",
      "|2021-11-05| 4627.354838709677|\n",
      "|2021-11-06|             880.2|\n",
      "|2021-12-05|1248.4545454545455|\n",
      "|2021-12-06|14294.638888888889|\n",
      "|2021-12-07|  6173.18774966711|\n",
      "|2021-12-08| 18653.36134057116|\n",
      "|2021-12-09| 9285.449952896844|\n",
      "|2021-12-10|12855.178205128204|\n",
      "|2021-12-11| 11750.13307144611|\n",
      "|2021-12-12|10717.924249422633|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_followers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2eacfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"followersSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc543b1a",
   "metadata": {},
   "source": [
    "### 3.7 Average Emoji Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41de0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_emoji = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(emoji_count) as avg_emojis \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a55a8e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|         avg_emojis|\n",
      "+----------+-------------------+\n",
      "|2021-10-25|                0.0|\n",
      "|2021-10-27| 0.4393939393939394|\n",
      "|2021-10-28|0.40913604766633566|\n",
      "|2021-10-29| 0.6046799299041336|\n",
      "|2021-10-30| 0.6746112813528544|\n",
      "|2021-10-31| 0.6129969774471054|\n",
      "|2021-11-01| 0.7203193115552284|\n",
      "|2021-11-02| 0.5962108217682832|\n",
      "|2021-11-03| 0.4280180761781795|\n",
      "|2021-11-04|0.36231884057971014|\n",
      "|2021-11-05| 0.4731182795698925|\n",
      "|2021-11-06|                0.2|\n",
      "|2021-12-05| 1.3333333333333333|\n",
      "|2021-12-06| 1.1111111111111112|\n",
      "|2021-12-07| 0.4747030356357237|\n",
      "|2021-12-08| 0.5062521911885006|\n",
      "|2021-12-09| 0.5205335205335205|\n",
      "|2021-12-10|0.49502013138376777|\n",
      "|2021-12-11| 0.5052285992217899|\n",
      "|2021-12-12| 0.4501372369624886|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_emoji.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a4d7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "number_of_influencers.createOrReplaceTempView(\"emojiSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d4630",
   "metadata": {},
   "source": [
    "### 3.8 Avergae Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44b8432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_words = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_words) as avg_words \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4805a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|         avg_words|\n",
      "+----------+------------------+\n",
      "|2021-10-25|              15.0|\n",
      "|2021-10-27|29.348484848484848|\n",
      "|2021-10-28|25.028798411122146|\n",
      "|2021-10-29|23.752808988764045|\n",
      "|2021-10-30|23.280444898422427|\n",
      "|2021-10-31| 23.88246919321088|\n",
      "|2021-11-01| 24.81679216441148|\n",
      "|2021-11-02|24.399968813347886|\n",
      "|2021-11-03|25.956100710135573|\n",
      "|2021-11-04|29.697722567287784|\n",
      "|2021-11-05|26.892473118279568|\n",
      "|2021-11-06|              15.6|\n",
      "|2021-12-05|26.583333333333332|\n",
      "|2021-12-06|23.666666666666668|\n",
      "|2021-12-07|22.808622965244172|\n",
      "|2021-12-08| 22.82587355381559|\n",
      "|2021-12-09|23.022698022698023|\n",
      "|2021-12-10|23.552235643144734|\n",
      "|2021-12-11|23.504012645914397|\n",
      "|2021-12-12|23.552912473315036|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7a50fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_words.createOrReplaceTempView(\"wordsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58478250",
   "metadata": {},
   "source": [
    "### 3.9 Avergae Number of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "720c470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_hashtags = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_hashtags) as avg_hashtags \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f608879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|      avg_hashtags|\n",
      "+----------+------------------+\n",
      "|2021-10-25|               0.0|\n",
      "|2021-10-27|1.6767676767676767|\n",
      "|2021-10-28|1.3475670307845085|\n",
      "|2021-10-29|1.0503040923616123|\n",
      "|2021-10-30| 1.139371240494836|\n",
      "|2021-10-31|1.1068356196233433|\n",
      "|2021-11-01|1.4346939930929061|\n",
      "|2021-11-02|1.1361297364727896|\n",
      "|2021-11-03|1.5061329890251776|\n",
      "|2021-11-04|1.5838509316770186|\n",
      "|2021-11-05| 3.575268817204301|\n",
      "|2021-11-06|               5.6|\n",
      "|2021-12-05|             11.75|\n",
      "|2021-12-06| 6.833333333333333|\n",
      "|2021-12-07|0.9001319841619005|\n",
      "|2021-12-08| 1.120603015075377|\n",
      "|2021-12-09|1.0933660933660934|\n",
      "|2021-12-10|0.9635515999152363|\n",
      "|2021-12-11|0.8852140077821011|\n",
      "|2021-12-12| 1.313815187557182|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "062d7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_hashtags.createOrReplaceTempView(\"hashtagsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4de37",
   "metadata": {},
   "source": [
    "### 3.10 Average Number of Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0ed5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_mentions = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_mentions) as avg_mentions \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3adc2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|      avg_mentions|\n",
      "+----------+------------------+\n",
      "|2021-10-25|               0.5|\n",
      "|2021-10-27|1.0934343434343434|\n",
      "|2021-10-28|1.0903674280039721|\n",
      "|2021-10-29|   1.1762704875786|\n",
      "|2021-10-30|1.3434343434343434|\n",
      "|2021-10-31|1.2687747035573123|\n",
      "|2021-11-01|1.0906980694106323|\n",
      "|2021-11-02|1.3746296585061593|\n",
      "|2021-11-03|1.2246610716591348|\n",
      "|2021-11-04|1.2536231884057971|\n",
      "|2021-11-05|0.8118279569892473|\n",
      "|2021-11-06|               0.0|\n",
      "|2021-12-05|0.3333333333333333|\n",
      "|2021-12-06|0.7777777777777778|\n",
      "|2021-12-07|0.9661240651121865|\n",
      "|2021-12-08|0.8418838377936193|\n",
      "|2021-12-09|0.9275769275769276|\n",
      "|2021-12-10| 1.134668361941089|\n",
      "|2021-12-11|1.0730787937743191|\n",
      "|2021-12-12|1.0767002134797194|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2c18cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_mentions.createOrReplaceTempView(\"mentionsSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9df6f5",
   "metadata": {},
   "source": [
    "### 3.11 Average Number of Exclamation Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f27ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_marks = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(num_exclamation_marks) as avg_exclamation_marks \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b66d9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|      date|avg_exclamation_marks|\n",
      "+----------+---------------------+\n",
      "|2021-10-25|                  0.0|\n",
      "|2021-10-27|  0.20959595959595959|\n",
      "|2021-10-28|   0.2025819265143992|\n",
      "|2021-10-29|   0.2621379239253685|\n",
      "|2021-10-30|  0.23357167177391897|\n",
      "|2021-10-31|   0.2342478493373634|\n",
      "|2021-11-01|   0.3399196059559531|\n",
      "|2021-11-02|  0.23023545922345237|\n",
      "|2021-11-03|   0.2091672046481601|\n",
      "|2021-11-04|  0.16252587991718426|\n",
      "|2021-11-05|  0.41935483870967744|\n",
      "|2021-11-06|                  0.0|\n",
      "|2021-12-05|   0.3333333333333333|\n",
      "|2021-12-06|   0.3055555555555556|\n",
      "|2021-12-07|  0.29916410030796303|\n",
      "|2021-12-08|  0.24856842351291342|\n",
      "|2021-12-09|  0.24254124254124254|\n",
      "|2021-12-10|  0.23151091332909515|\n",
      "|2021-12-11|              0.21875|\n",
      "|2021-12-12|  0.23132052455016774|\n",
      "+----------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_marks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be552a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_marks.createOrReplaceTempView(\"marksSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17a757",
   "metadata": {},
   "source": [
    "### 3.12 Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54ba83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_sentiment = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(sentiment_vader) as avg_sentiment \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a92e47ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|      avg_sentiment|\n",
      "+----------+-------------------+\n",
      "|2021-10-25|0.21450000000000002|\n",
      "|2021-10-27| 0.1265151515151515|\n",
      "|2021-10-28|0.12339424031777559|\n",
      "|2021-10-29|0.13889227914647964|\n",
      "|2021-10-30| 0.1416517988877539|\n",
      "|2021-10-31|0.14321367123924658|\n",
      "|2021-11-01|0.16307739342127617|\n",
      "|2021-11-02|0.14077943240293153|\n",
      "|2021-11-03|0.13228986442866364|\n",
      "|2021-11-04|0.11481987577639752|\n",
      "|2021-11-05|0.14730107526881722|\n",
      "|2021-11-06|             0.1476|\n",
      "|2021-12-05|            0.15025|\n",
      "|2021-12-06|0.21541666666666665|\n",
      "|2021-12-07|0.15144830620325564|\n",
      "|2021-12-08|0.14207093607572754|\n",
      "|2021-12-09|0.14165531765531755|\n",
      "|2021-12-10|0.13792424242424242|\n",
      "|2021-12-11|0.13936016536964976|\n",
      "|2021-12-12|0.14111802378774016|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c0d9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_sentiment.createOrReplaceTempView(\"sentimentSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8a72f",
   "metadata": {},
   "source": [
    "### 3.13 Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b33858d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_polarity = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(polarity) as avg_polarity \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fccf852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|       avg_polarity|\n",
      "+----------+-------------------+\n",
      "|2021-10-25|0.32500000000000007|\n",
      "|2021-10-27|0.12702171565641085|\n",
      "|2021-10-28| 0.1141894718000903|\n",
      "|2021-10-29|0.12819660932414867|\n",
      "|2021-10-30| 0.1351922168225551|\n",
      "|2021-10-31|0.13300623006534804|\n",
      "|2021-11-01|0.18316873996993763|\n",
      "|2021-11-02| 0.1344894138598013|\n",
      "|2021-11-03|0.11277198890931094|\n",
      "|2021-11-04|0.10145347439457912|\n",
      "|2021-11-05| 0.2000849611982004|\n",
      "|2021-11-06|0.17398989898989897|\n",
      "|2021-12-05|0.14224537037037038|\n",
      "|2021-12-06| 0.2354082350262906|\n",
      "|2021-12-07|0.14625957594418673|\n",
      "|2021-12-08|0.13326438421092746|\n",
      "|2021-12-09| 0.1309096075876976|\n",
      "|2021-12-10|0.12113829041011834|\n",
      "|2021-12-11|0.12830378170827264|\n",
      "|2021-12-12|0.12405803675085038|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_polarity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4dbf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_polarity.createOrReplaceTempView(\"polaritySQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd0915",
   "metadata": {},
   "source": [
    "### 3.13 Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8433f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant data\n",
    "avg_subjectivity = spark.sql(\"SELECT DATE_FORMAT(post_created_at, 'Y-M-dd') as date, AVG(subjectivity) as avg_subjectivity \\\n",
    "                          FROM twitterSQL \\\n",
    "                          GROUP BY DATE_FORMAT(post_created_at, 'Y-M-dd') \\\n",
    "                          ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf34644c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|   avg_subjectivity|\n",
      "+----------+-------------------+\n",
      "|2021-10-25|              0.575|\n",
      "|2021-10-27| 0.4607615970022174|\n",
      "|2021-10-28|0.40723402379121265|\n",
      "|2021-10-29|0.41847483528656193|\n",
      "|2021-10-30| 0.4174534735960923|\n",
      "|2021-10-31|0.41968838608618286|\n",
      "|2021-11-01| 0.4484449299283815|\n",
      "|2021-11-02|0.42274322635546135|\n",
      "|2021-11-03|0.42525202685569896|\n",
      "|2021-11-04| 0.4268351742889971|\n",
      "|2021-11-05|0.48653377742826687|\n",
      "|2021-11-06| 0.3757070707070707|\n",
      "|2021-12-05|0.28032407407407406|\n",
      "|2021-12-06|  0.419161550897662|\n",
      "|2021-12-07| 0.4064326072683204|\n",
      "|2021-12-08| 0.4032351293825101|\n",
      "|2021-12-09|0.40076446549362343|\n",
      "|2021-12-10| 0.4005971880182179|\n",
      "|2021-12-11|0.40747466387117864|\n",
      "|2021-12-12| 0.4147956652341835|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "avg_subjectivity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dea58f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "avg_subjectivity.createOrReplaceTempView(\"subjectivitySQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b60dd",
   "metadata": {},
   "source": [
    "## 4. Basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0456801b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 11) (365076258.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [125], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    basetable = spark.sql(\"SELECT DATE_FORMAT(a.date, 'Y-M-dd') as date, a.dependent_vegan, b.tweet_volume, COALESCE(c.avg_likes,0) as avg_likes \\\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 11)\n"
     ]
    }
   ],
   "source": [
    "# create basetable\n",
    "basetable = spark.sql(\"SELECT DATE_FORMAT(a.date, 'Y-M-dd') as date, a.dependent_vegan, b.tweet_volume, COALESCE(c.avg_likes,0) as avg_likes \\\n",
    "                       COALESCE(d.avg_retweets,0) as avg_retweets, \\\n",
    "                       COALESCE(e.avg_engagement_rate,0) as avg_engagement_rate, COALESCE(f.influencers,0) as influencers \\\n",
    "                    FROM trendSQL a \\\n",
    "                    INNER JOIN tweet_volumeSQL b ON DATE_FORMAT(a.date, 'Y-M-dd') = b.date \\\n",
    "                    LEFT OUTER JOIN avg_likesSQL c ON b.date = c.date \\\n",
    "                    LEFT OUTER JOIN avg_retweetsSQL d ON c.date = d.date \\\n",
    "                    LEFT OUTER JOIN avg_engagement_rateSQL e ON d.date = e.date \\\n",
    "                    LEFT OUTER JOIN number_of_influencersSQL f ON e.date = f.date \\\n",
    "                    LEFT OUTER JOIN followersSQL g ON f.date = g.date \\ \n",
    "                    LEFT OUTER JOIN emojiSQL h ON g.date = h.date \\\n",
    "                    LEFT OUTER JOIN wordsSQL i ON h.date = i.date \\\n",
    "                    LEFT OUTER JOIN hashtagsSQL j ON i.date = j.date \\\n",
    "                    LEFT OUTER JOIN mentionsSQL k ON j.date = k.date \\\n",
    "                    LEFT OUTER JOIN marksSQL l ON k.date = l.date \\\n",
    "                    LEFt OUTER JOIN sentimentSQL m ON l.date = m.date \\\n",
    "                    LEFT OUTER JOIN polaritySQL n ON m.date = n.date \\\n",
    "                    LEFT OUTER JOIN subjectivitySQL o ON n.date = o.date \\\n",
    "                    ORDER BY DATE_FORMAT(post_created_at, 'Y-M-dd')\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "874c8114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basetable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [122], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# show\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbasetable\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'basetable' is not defined"
     ]
    }
   ],
   "source": [
    "# show\n",
    "basetable.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a25cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping the date column as this is not a feature \n",
    "basetable.drop('date', axis=1, inplace=True)\n",
    "basetable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store basetable as a .parquet file\n",
    "basetable.to_parquet(\"./../../data/basetable_vegan_trend_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export basetable as a .json file\n",
    "basetable.to_json(\"./../../data/basetable_vegan_trend_prediction.json\", orient=\"records\", force_ascii=False, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad0eda",
   "metadata": {},
   "source": [
    "## 5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the saved basetable (.parquet)\n",
    "#basetable_df = spark.read.parquet(\"./../../data/basetable_vegan_trend_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff16fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the saved basetable (.json)\n",
    "#basetable_df = spark.read.json(\"./../../data/basetable_vegan_trend_prediction.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db78fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basetable_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be427e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required functions\n",
    "from pyspark.ml.feature import Binarizer, StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de196982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define string indexer to index price \n",
    "SI = StringIndexer(inputCol = 'dependent_vegan', outputCol = 'label')\n",
    "\n",
    "# define vector assembler for numeric variables\n",
    "numColumns = ['volume','avg_likes','avg_retweets','avg_engagement_rate','influencers' \\\n",
    "             ,'avg_followers', 'avg_emojis', 'avg_words', 'avg_hashtags', 'avg_mentions' \\\n",
    "             ,'avg_exclamation_marks', 'avg_sentiment', 'avg_polarity','avg_subjectivity']\n",
    "VAnum = VectorAssembler(inputCols=numColumns, outputCol=\"numFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [SI, VAnum]\n",
    "# define pipeline and fit on data\n",
    "preprocessingPipeline = Pipeline().setStages(stages).fit(basetable)\n",
    "# apply pipeline on data\n",
    "basetable_df = preprocessingPipeline.transform(basetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f641a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "basetable = basetable.select([\"numFeatures\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "basetable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73cf59",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "- Split the data in a train and test set (70/30).\n",
    "- Build one pipeline that:\n",
    "  - standardizes the numerical variables\n",
    "  - applies a logistic regression to the data\n",
    "  - check the performance using the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a3a93",
   "metadata": {},
   "source": [
    "We cannot use the randomsplit function, because we have time series data, so we have to use another approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8ec34",
   "metadata": {},
   "source": [
    "First we look at the amount of observations that will be assigned to the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece8543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_train = int(basetable.count()*0.7)\n",
    "nr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5eb92",
   "metadata": {},
   "source": [
    "convert the final basetable to a pandas dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52568ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable_pd = basetable.toPandas()\n",
    "basetable_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaf526",
   "metadata": {},
   "source": [
    "Split the dataframe into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be927ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = basetable_pd.iloc[:nr_train,:]\n",
    "test_pd = basetable_pd.iloc[nr_train:,:]\n",
    "train = spark.createDataFrame(train_pd)\n",
    "test = spark.createDataFrame(test_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of observations in train and test set\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect distribution of label in train and test set\n",
    "basetable.groupBy(\"label\").count().show()\n",
    "train.groupBy(\"label\").count().show()\n",
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required features\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609168ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "SS = StandardScaler(inputCol = 'numFeatures', outputCol = 'scaledNumFeatures', withStd = True, withMean = False)\n",
    "\n",
    "# define vector assembler\n",
    "VA = VectorAssembler(inputCols = ['scaledNumFeatures'], outputCol = 'features')\n",
    "\n",
    "# define logistic regression model\n",
    "LR = LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline stages\n",
    "stages = [SS, VA, LR]\n",
    "# create pipeline and fit on training set\n",
    "lrModelPipeline = Pipeline().setStages(stages).fit(train)\n",
    "# apply pipeline on test set to get predictions\n",
    "predictions = lrModelPipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce473b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect predictions\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# get evaluation metric\n",
    "lrAUC = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "# inspect model performance\n",
    "print('AUC lr: %f' %(lrAUC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
