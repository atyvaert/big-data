{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit\n",
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import emojis\n",
    "from translate import Translator\n",
    "\n",
    "import sparknlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/wouterdewitte/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/wouterdewitte/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-m1_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b83eff4f-f59e-497a-9b05-8038cb97b6d7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.2.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 212ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.2.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b83eff4f-f59e-497a-9b05-8038cb97b6d7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/12ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:56:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start(m1=True)\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "#sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "#spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "#path_json = \".././../data/Topic_vegan/*.json\"\n",
    "\n",
    "# use this if you want all the tweet files, but this is usually too large\n",
    "#df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:57:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3428559"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic_vegan/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:55 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karen Reed üå∏</td>\n",
       "      <td>kandk670</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>@trudiebakescake Organic coconut oil in a jar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>„Éè„É´):)</td>\n",
       "      <td>patlnwza55</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ÔæåÔΩßÔæôÔæÑÔæûÔæó„Å£Â≠êorganicÊúâÊ©üüíôüíª</td>\n",
       "      <td>organic_yusai</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>„Éû„Ç∏„Åß„Éî„É≥„ÉÅÂä©„Åë„Å¶Ëá™Ëª¢Ëªä„Ç¨„Ç¨„Ç¨„Ç¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ÔΩ°‚óï‚Äø‚óïÔΩ°ùë±ùíÜ ùíï'ùíÇùíäùíéùíÜ üê∂üß°‚ú®</td>\n",
       "      <td>MyFnlovely97</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sang‚Ñ¢</td>\n",
       "      <td>asan_gk</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @NotechAna: Am I the only one who types in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trysia ):)‚ñ™Ô∏énever let me go‚ñ™Ô∏é</td>\n",
       "      <td>Winnie_thephuu</td>\n",
       "      <td>Mon Apr 04 10:09:48 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name     screen_name  \\\n",
       "0                        „ÅÆ„Çä/Nori      nori_k_629   \n",
       "1                          alice   myn4meizalize   \n",
       "2                   Karen Reed üå∏        kandk670   \n",
       "3                          „Éè„É´):)      patlnwza55   \n",
       "4                          alice   myn4meizalize   \n",
       "5            ÔæåÔΩßÔæôÔæÑÔæûÔæó„Å£Â≠êorganicÊúâÊ©üüíôüíª   organic_yusai   \n",
       "6                        „ÅÆ„Çä/Nori      nori_k_629   \n",
       "7             ÔΩ°‚óï‚Äø‚óïÔΩ°ùë±ùíÜ ùíï'ùíÇùíäùíéùíÜ üê∂üß°‚ú®    MyFnlovely97   \n",
       "8                          Sang‚Ñ¢         asan_gk   \n",
       "9  Trysia ):)‚ñ™Ô∏énever let me go‚ñ™Ô∏é  Winnie_thephuu   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Mon Apr 04 10:09:55 +0000 2022   \n",
       "1  Mon Apr 04 10:09:54 +0000 2022   \n",
       "2  Mon Apr 04 10:09:54 +0000 2022   \n",
       "3  Mon Apr 04 10:09:52 +0000 2022   \n",
       "4  Mon Apr 04 10:09:52 +0000 2022   \n",
       "5  Mon Apr 04 10:09:52 +0000 2022   \n",
       "6  Mon Apr 04 10:09:50 +0000 2022   \n",
       "7  Mon Apr 04 10:09:50 +0000 2022   \n",
       "8  Mon Apr 04 10:09:50 +0000 2022   \n",
       "9  Mon Apr 04 10:09:48 +0000 2022   \n",
       "\n",
       "                                           full_text  \n",
       "0  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...  \n",
       "1  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...  \n",
       "2  @trudiebakescake Organic coconut oil in a jar ...  \n",
       "3  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...  \n",
       "4  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...  \n",
       "5                                   „Éû„Ç∏„Åß„Éî„É≥„ÉÅÂä©„Åë„Å¶Ëá™Ëª¢Ëªä„Ç¨„Ç¨„Ç¨„Ç¨  \n",
       "6  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...  \n",
       "7  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...  \n",
       "8  RT @NotechAna: Am I the only one who types in ...  \n",
       "9  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "\n",
    "#df.printSchema()\n",
    "#df.count() #1340938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|emoji_count|      text_tokenized|num_words|num_hashtags|num_mentions|num_exclamation_marks|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33|          0|[!\\n#vegan, #GoVe...|        4|           3|           0|                    1|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|          0|[!, We, will, be,...|       35|          21|           0|                    3|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|          0|[!!, Daily, Updat...|       31|           9|           2|                    2|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|2022-01-25 01:00:20|          2|[!!Fresh, Vegetab...|       34|           6|           0|                    2|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|2022-02-03 15:32:38|          0|[!00%, Organic, C...|       12|           0|           1|                    1|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|          0|[!B, Mal, gute, N...|       30|           2|           0|                    1|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|          0|[!love, !iq, !wad...|       32|          14|           2|                    5|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|2022-09-25 17:30:05|          4|[!‚¨ÜÔ∏èüß°üéºüôå#cocina...|       13|          12|           0|                    1|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|2021-12-09 21:32:21|          0|[\", 'A√ßougue, veg...|       19|           0|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|2022-02-13 21:02:14|          0|[\", Cacao, butter...|       20|           1|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|2022-02-10 17:53:01|          0|[\", Coconut, maca...|       20|           5|           0|                    0|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|          0|[\", DID, YOU, KNO...|       18|          13|           0|                    0|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|          0|[\", Nature's, Int...|       30|          21|           2|                    0|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|          0|[\", Scarlet, Puni...|       32|          21|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|2022-02-09 12:15:43|          0|[\", Spanish-style...|       20|           1|           0|                    0|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|2022-02-26 16:40:54|          2|[\", Vegan, BLT, s...|       49|           3|           0|                    0|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|          0|[\", Vegan, meinet...|       22|           1|           0|                    1|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|2022-02-09 19:00:20|          0|[\", Vegetable, fr...|       21|           1|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|2022-02-10 22:02:09|          0|[\", chocolate, br...|       23|           4|           0|                    0|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|          2|[\", nuna, \"\\n\", T...|       20|           0|           0|                    0|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df = df.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "\n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc4ab2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate_mul_en download started this may take some time.\n",
      "Approx size to download 279,5 MB\n",
      "[ / ]translate_mul_en download started this may take some time.\n",
      "Approximate size to download 279,5 MB\n",
      "Download done! Loading the resource.\n",
      "[ ‚Äî ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:18:19.823250: W external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#from sparknlp.pretrained import PretrainedPipeline \n",
    "#pipeline = PretrainedPipeline(\"translate_mul_en\", lang = \"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df = pipeline.transform(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:00:55 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 29 (TID 4671): Attempting to kill Python Worker\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|         translation|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|2022-01-25 01:00:20|fresh vegetables ...|fresh vegetables ...|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|2022-02-03 15:32:38|organic cotton ha...|organic cotton ha...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|2022-09-25 17:30:05|arrow up orange h...|arrow up orange h...|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|2021-12-09 21:32:21|a√ßougue vegano tu...|Vegan Butcher Sho...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|2022-02-13 21:02:14|cacao butter cook...|cacao butter cook...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|2022-02-10 17:53:01|coconut macaroons...|coconut macaroons...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|did you know seno...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|natures interconn...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|scarlet punica gr...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|2022-02-09 12:15:43|spanishstyle squi...|spanishstyle squi...|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|2022-02-26 16:40:54|vegan blt sandwic...|vegan blt sandwic...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|vegan for all I c...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|2022-02-09 19:00:20|vegetable frittat...|vegetable frittat...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|2022-02-10 22:02:09|chocolate brownie...|chocolate brownie...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|nuna, thanks for ...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator() \n",
    "translation = translator.translate(\"test\", dest='en')\n",
    "\n",
    "def translate(tweet):\n",
    "    if len(tweet)!=0:\n",
    "        translator = Translator() \n",
    "        translation = translator.translate(tweet, dest = 'en')\n",
    "        return(translation.text)\n",
    "    return None\n",
    "\n",
    "translate_udf = F.udf(translate, StringType())\n",
    "\n",
    "twitter_df = twitter_df.withColumn(\"translation\", translate_udf(\"text\"))\n",
    "\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- translation: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df = twitter_df.withColumn(\"translation\", twitter_df.translation.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df = twitter_df.drop(\"document\",\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:02:32 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 41 (TID 4965): Attempting to kill Python Worker\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|         translation|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|2022-01-25 01:00:20|fresh vegetables ...|fresh vegetables ...|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|2022-02-03 15:32:38|organic cotton ha...|organic cotton ha...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|2022-09-25 17:30:05|arrow up orange h...|arrow up orange h...|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|2021-12-09 21:32:21|a√ßougue vegano tu...|Vegan Butcher Sho...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|2022-02-13 21:02:14|cacao butter cook...|cacao butter cook...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|2022-02-10 17:53:01|coconut macaroons...|coconut macaroons...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|did you know seno...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|natures interconn...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|scarlet punica gr...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|2022-02-09 12:15:43|spanishstyle squi...|spanishstyle squi...|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|2022-02-26 16:40:54|vegan blt sandwic...|vegan blt sandwic...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|vegan for all I c...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|2022-02-09 19:00:20|vegetable frittat...|vegetable frittat...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|2022-02-10 22:02:09|chocolate brownie...|chocolate brownie...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|nuna, thanks for ...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic Modelling\n",
    "\n",
    "https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb\n",
    "\n",
    "https://www.johnsnowlabs.com/spark-nlp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Spark NLP pipeline\n",
    "\n",
    "### 3.1.1 Basic NLP pipeline\n",
    "\n",
    "DocumentAssembler converts data into Spark NLP annotation format that can be used by Spark NLP annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "                .setInputCol(\"translation\") \\\n",
    "                .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize the data with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean out the data and lower it with Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with LemmatizerModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ / ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP doesn't provide stop word list, hence, we will use nltk package to download stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with NGramGenerator in Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP. We can access the model with PerceptronModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "[ / ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import PerceptronModel\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'lemmatized']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to input everything into a pipeline. Pipeline functionality is accessible with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 ngrammer,\n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pipeline.fit(twitter_df).transform(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:04:22 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 59 (TID 5296): Attempting to kill Python Worker\n",
      "22/11/30 22:04:22 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 59 (TID 5291): Attempting to kill Python Worker\n",
      "22/11/30 22:04:22 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 59 (TID 5290): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===================>                                      (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:04:24 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 59 (TID 5294): Attempting to kill Python Worker\n",
      "22/11/30 22:04:24 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 59 (TID 5292): Attempting to kill Python Worker\n",
      "22/11/30 22:04:24 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 59 (TID 5293): Attempting to kill Python Worker\n",
      "22/11/30 22:04:24 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 59 (TID 5295): Attempting to kill Python Worker\n",
      "22/11/30 22:04:24 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 59 (TID 5297): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:04:54 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 59 (TID 5298): Attempting to kill Python Worker\n",
      "+--------------------+--------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|   screen_name|          created_at|           full_text|    post_created_at|                text|         translation|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+--------------------+--------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã| vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|\n",
      "|üå±Veg-In-Out Mark...|veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|\n",
      "|         Mix 93.8 FM|      Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|\n",
      "|         GreenGrowth| Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|2022-01-25 01:00:20|fresh vegetables ...|fresh vegetables ...|[fresh, vegetable...|[fresh, vegetable...|[NN, JJ, JJ, NN, ...|\n",
      "|           EJBroyles|     EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|2022-02-03 15:32:38|organic cotton ha...|organic cotton ha...|[organic, cotton,...|[organic, cotton,...|[JJ, NN, NN, NN, ...|\n",
      "+--------------------+--------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Extended NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have our data in a form of unigrams that are lemmatized, with no stop words in there. I think it is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let's filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let's restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don't use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
    "\n",
    "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing.\n",
    "\n",
    "Let's start this processing. First, we need to join all our POS tags obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize our POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate n-grams from them in the same way we did that for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are ready to get POS tags ngrams with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create this new Spark NLP pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and again fit it and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what kind of data we have to operate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'screen_name',\n",
       " 'created_at',\n",
       " 'full_text',\n",
       " 'post_created_at',\n",
       " 'text',\n",
       " 'translation',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are our word n-grams with their corresponding pos n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:16:07 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 89 (TID 5898): Attempting to kill Python Worker\n",
      "22/11/30 22:16:07 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 89 (TID 5902): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "[Stage 89:======>                                                   (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:16:09 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 89 (TID 5902): Attempting to kill Python Worker\n",
      "22/11/30 22:16:09 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 89 (TID 5901): Attempting to kill Python Worker\n",
      "22/11/30 22:16:09 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 89 (TID 5899): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 89:=========================>                                (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:16:13 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 89 (TID 5896): Attempting to kill Python Worker\n",
      "22/11/30 22:16:13 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 89 (TID 5900): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "[Stage 89:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:16:17 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 89 (TID 5897): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:16:47 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 89 (TID 5895): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 22:20:01 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 89 (TID 5903): Attempting to kill Python Worker\n",
      "+--------------------+--------------------+\n",
      "|     finished_ngrams| finished_pos_ngrams|\n",
      "+--------------------+--------------------+\n",
      "|[this, afternoon,...|[DT, NN, VBG, JJ,...|\n",
      "|[sweet, potato, a...|[JJ, NN, CC, NN, ...|\n",
      "|[creamy, mushroom...|[NN, NN, NN, NN, ...|\n",
      "|[polypieter, vegg...|[NN, NN, VB, RB, ...|\n",
      "|[enrich, with, na...|[NN, IN, JJ, NN, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 05:19:09 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 993145 ms exceeds timeout 120000 ms\n",
      "22/12/01 05:19:09 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
    "                                                              F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how our filtered unigrams look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:49:25 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 81 (TID 5607): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:======>                                                   (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:49:27 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 81 (TID 5604): Attempting to kill Python Worker\n",
      "22/11/30 21:49:27 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 81 (TID 5605): Attempting to kill Python Worker\n",
      "22/11/30 21:49:27 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 81 (TID 5609): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):                                  (3 + 6) / 9]\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 81:=========================>                                (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:49:29 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 81 (TID 5608): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 81:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:50:11 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 81 (TID 5602): Attempting to kill Python Worker\n",
      "22/11/30 21:50:11 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 81 (TID 5603): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:50:13 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 81 (TID 5606): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:53:35 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 81 (TID 5610): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|                                   [sweet, potato, fritter, mint, dip, vegan, plant, base]|\n",
      "|                           [creamy, mushroom, bucatini, vegan, veganforlife, forevervegan]|\n",
      "|[polypieter, veggie, already, vegan, almost, impossible, get, hey, dont, ingredient, ve...|\n",
      "|                             [enrich, origin, ingredient, certify, vegan, wink, fragrance]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the function on word and POS n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
    "                                                                    F.col('finished_pos_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what we get after filtering for n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 21:55:08 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n",
      "    response.close()\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n",
      "    for chunk in self.iter_raw():\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n",
      "    for part in self._raw_stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n",
      "    for chunk in self.stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n",
      "    for chunk in self.iterator:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\t... 17 more\n",
      "22/11/30 21:55:08 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n",
      "    response.close()\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n",
      "    for chunk in self.iter_raw():\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n",
      "    for part in self._raw_stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n",
      "    for chunk in self.stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n",
      "    for chunk in self.iterator:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/30 21:55:08 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n",
      "    response.close()\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n",
      "    for chunk in self.iter_raw():\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n",
      "    for part in self._raw_stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n",
      "    for chunk in self.stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n",
      "    for chunk in self.iterator:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/30 21:55:08 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n",
      "    response.close()\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n",
      "    for chunk in self.iter_raw():\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n",
      "    for part in self._raw_stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n",
      "    for chunk in self.stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n",
      "    for chunk in self.iterator:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/30 21:55:08 ERROR Executor: Exception in task 6.0 in stage 98.0 (TID 5911)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n",
      "    response.close()\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n",
      "    for chunk in self.iter_raw():\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n",
      "    for part in self._raw_stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n",
      "    for chunk in self.stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n",
      "    for chunk in self.iterator:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/30 21:55:08 WARN TaskSetManager: Lost task 6.0 in stage 98.0 (TID 5911) (192.168.10.159 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n",
      "    response.close()\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n",
      "    self._content = b\"\".join([part for part in self.iter_bytes()])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n",
      "    for chunk in self.iter_raw():\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n",
      "    for part in self._raw_stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n",
      "    for chunk in self.stream:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n",
      "    for chunk in self.iterator:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "\n",
      "22/11/30 21:55:08 ERROR TaskSetManager: Task 6 in stage 98.0 failed 1 times; aborting job\n",
      "22/11/30 21:55:08 WARN TaskSetManager: Lost task 8.0 in stage 98.0 (TID 5913) (192.168.10.159 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n    data, response = self._translate(text, dest, src, kwargs)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n    r = self.client.get(url, params=params)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n    return self.request(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n    return self.send(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n    response.close()\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n    self._content = b\"\".join([part for part in self.iter_bytes()])\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n    self._content = b\"\".join([part for part in self.iter_bytes()])\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n    for chunk in self.iter_raw():\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n    for part in self._raw_stream:\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n    for chunk in self.stream:\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n    for chunk in self.iterator:\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n    event = self.connection.wait_for_event(self.stream_id, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n    self.receive_events(timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n    return self.sock.recv(n)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n    return False\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n    raise\nhttpcore._exceptions.ReadTimeout: The read operation timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/1442827006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtered_ngrams'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    613\u001b[0m                 )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_93262/2024449930.py\", line 9, in translate\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n    data, response = self._translate(text, dest, src, kwargs)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n    r = self.client.get(url, params=params)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n    return self.request(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n    return self.send(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 628, in send\n    response.close()\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in read\n    self._content = b\"\".join([part for part in self.iter_bytes()])\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 901, in <listcomp>\n    self._content = b\"\".join([part for part in self.iter_bytes()])\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 912, in iter_bytes\n    for chunk in self.iter_raw():\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_models.py\", line 945, in iter_raw\n    for part in self._raw_stream:\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 49, in __iter__\n    for chunk in self.stream:\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/base.py\", line 57, in __iter__\n    for chunk in self.iterator:\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 360, in body_iter\n    event = self.connection.wait_for_event(self.stream_id, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n    self.receive_events(timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n    return self.sock.recv(n)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n    return False\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n    raise\nhttpcore._exceptions.ReadTimeout: The read operation timed out\n"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(F.col('filtered_unigrams'), \n",
    "                                                      F.col('filtered_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our final look of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:39:06 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 106 (TID 6205): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "[Stage 106:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:39:08 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 106 (TID 6203): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:============>                                            (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:39:10 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 106 (TID 6204): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:39:18 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 106 (TID 6200): Attempting to kill Python Worker\n",
      "22/12/01 07:39:18 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 106 (TID 6201): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 106:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:39:22 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 106 (TID 6202): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 106:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:40:10 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 106 (TID 6199): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:40:14 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 106 (TID 6198): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 07:42:50 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 106 (TID 6206): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                     final|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|[sweet, potato, fritter, mint, dip, vegan, plant, base, sweet_potato, courgette_fritter...|\n",
      "|[creamy, mushroom, bucatini, vegan, veganforlife, forevervegan, creamy_mushroom, mushro...|\n",
      "|[polypieter, veggie, already, vegan, almost, impossible, get, hey, dont, ingredient, ve...|\n",
      "|[enrich, origin, ingredient, certify, vegan, wink, fragrance, natural_origin, origin_in...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('final').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set to vectorization of our data. First, we will proceed with TF (term frequency) vectorization with CountVectorizer in PySpark. We fit tf dictionary and then transform the data to vectors of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:>                                                        (0 + 8) / 9]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get TF results, we can account for words that are frequent for all the documents. We can use IDF (inverse document frequency) to lower score of such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:32:19 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:44:02 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to model topics in our data with LDA (Latent Dirichlet Allocation). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:46:12 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|[NN, NN, NN, NN_N...|[vegan, govegan, ...|[vegan_govegan, g...|[vegan, govegan, ...|(262144,[0,71,290...|(262144,[0,71,290...|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|[PRP, MD, VB, JJ,...|[today, tomorrow,...|[be_open, open_am...|[today, tomorrow,...|(262144,[0,1,3,9,...|(262144,[0,1,3,9,...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|[JJ, JJ, NN, IN, ...|[daily, update, t...|[daily_update, up...|[daily, update, t...|(262144,[3,32,68,...|(262144,[3,32,68,...|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|2022-01-25 01:00:20|fresh vegetables ...|[fresh, vegetable...|[fresh, vegetable...|[NN, JJ, JJ, NN, ...|[NN, JJ, JJ, NN, ...|[fresh, vegetable...|[fresh_vegetable,...|[fresh, vegetable...|(262144,[7,12,13,...|(262144,[7,12,13,...|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|2022-02-03 15:32:38|organic cotton ha...|[organic, cotton,...|[organic, cotton,...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[organic, cotton,...|[organic_cotton, ...|[organic, cotton,...|(262144,[1,128,26...|(262144,[1,128,26...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|[mal, gute, nachr...|[mal, gute, nachr...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[mal, gute, nachr...|[mal_gute, gute_n...|[mal, gute, nachr...|(262144,[73,161,2...|(262144,[73,161,2...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|[NN, NN, NN, VB, ...|[love, iq, waddup...|[love_iq, iq_wadd...|[love, iq, waddup...|(262144,[0,23,57,...|(262144,[0,23,57,...|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|2022-09-25 17:30:05|arrow up orange h...|[arrow, orange, h...|[arrow, up, orang...|[NN, RB, NN, NN, ...|[NN, RB, NN, NN, ...|[arrow, heart, mu...|[orange_heart, he...|[arrow, heart, mu...|(262144,[5,6,7,69...|(262144,[5,6,7,69...|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|2021-12-09 21:32:21|a√ßougue vegano tu...|[a√ßougue, vegano,...|[a√ßougue, vegano,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[a√ßougue, vegano,...|[a√ßougue_vegano, ...|[a√ßougue, vegano,...|(262144,[6,182,26...|(262144,[6,182,26...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|2022-02-13 21:02:14|cacao butter cook...|[cacao, butter, c...|[cacao, butter, c...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[cacao, butter, c...|[cacao_butter, bu...|[cacao, butter, c...|(262144,[58,133,2...|(262144,[58,133,2...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|2022-02-10 17:53:01|coconut macaroons...|[coconut, macaroo...|[coconut, macaroo...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[coconut, macaroo...|[coconut_macaroon...|[coconut, macaroo...|(262144,[80,146,2...|(262144,[80,146,2...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|[know, senoritaco...|[do, you, know, s...|[VBP, PRP, VBP, N...|[VBP, PRP, VBP, N...|[know, senoritaco...|[know_senoritacos...|[know, senoritaco...|(262144,[0,50,71,...|(262144,[0,50,71,...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|[nature, intercon...|[nature, intercon...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[nature, intercon...|[nature_interconn...|[nature, intercon...|(262144,[0,27,49,...|(262144,[0,27,49,...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|[scarlet, punica,...|[scarlet, punica,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[scarlet, punica,...|[scarlet_punica, ...|[scarlet, punica,...|(262144,[0,1,197,...|(262144,[0,1,197,...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|2022-02-09 12:15:43|spanishstyle squi...|[spanishstyle, sq...|[spanishstyle, sq...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[spanishstyle, sq...|[spanishstyle_squ...|[spanishstyle, sq...|(262144,[133,236,...|(262144,[133,236,...|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|2022-02-26 16:40:54|vegan blt sandwic...|[vegan, blt, sand...|[vegan, blt, sand...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[vegan, blt, sand...|[vegan_blt, blt_s...|[vegan, blt, sand...|(262144,[0,12,20,...|(262144,[0,12,20,...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|[vegan, meinetweg...|[vegan, meinetweg...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[vegan, meinetweg...|[vegan_meinetwege...|[vegan, meinetweg...|(262144,[0,73,161...|(262144,[0,73,161...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|2022-02-09 19:00:20|vegetable frittat...|[vegetable, fritt...|[vegetable, fritt...|[JJ, NN, TO, VB, ...|[JJ, NN, TO, VB, ...|[vegetable, fritt...|[vegetable_fritta...|[vegetable, fritt...|(262144,[2,75,191...|(262144,[2,75,191...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|2022-02-10 22:02:09|chocolate brownie...|[chocolate, brown...|[chocolate, brown...|[NN, NN, RB, JJ, ...|[NN, NN, RB, JJ, ...|[chocolate, brown...|[chocolate_browny...|[chocolate, brown...|(262144,[112,152,...|(262144,[112,152,...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|[nuna, thank, hel...|[nuna, thank, for...|[NN, VB, IN, PRP,...|[NN, VB, IN, PRP,...|[nuna, thank, say...|[aduh_saya, saya_...|[nuna, thank, say...|(262144,[0,78,94,...|(262144,[0,78,94,...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tfidf_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:49:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:58:43 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:58:43 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:58:44 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:58:44 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/11/30 12:58:44 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 183:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:58:53 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:58:54 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:58:55 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 192:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:58:58 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:58:58 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:58:59 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:03 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:03 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:04 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 210:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:06 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:07 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:07 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 219:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:10 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:10 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:11 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:13 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:13 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:14 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 237:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:15 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:15 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:16 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 246:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:17 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:17 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:18 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 255:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:19 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:19 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/30 12:59:20 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 264:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 12:59:21 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 6\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features').setTopicDistributionCol(\"topicDistributionCol\")\n",
    "lda_model = lda.fit(tfidf_result)\n",
    "transformed = lda_model.transform(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 273:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 13:01:04 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 277:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|topicDistributionCol|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|[NN, NN, NN, NN_N...|[vegan, govegan, ...|[vegan_govegan, g...|[vegan, govegan, ...|(262144,[0,71,290...|(262144,[0,71,290...|[0.00379494757528...|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|[PRP, MD, VB, JJ,...|[today, tomorrow,...|[be_open, open_am...|[today, tomorrow,...|(262144,[0,1,3,9,...|(262144,[0,1,3,9,...|[0.78049034396963...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|[JJ, JJ, NN, IN, ...|[daily, update, t...|[daily_update, up...|[daily, update, t...|(262144,[3,32,68,...|(262144,[3,32,68,...|[0.00140610731141...|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|2022-01-25 01:00:20|fresh vegetables ...|[fresh, vegetable...|[fresh, vegetable...|[NN, JJ, JJ, NN, ...|[NN, JJ, JJ, NN, ...|[fresh, vegetable...|[fresh_vegetable,...|[fresh, vegetable...|(262144,[7,12,13,...|(262144,[7,12,13,...|[3.99700850044067...|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|2022-02-03 15:32:38|organic cotton ha...|[organic, cotton,...|[organic, cotton,...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[organic, cotton,...|[organic_cotton, ...|[organic, cotton,...|(262144,[1,128,26...|(262144,[1,128,26...|[0.00205394029267...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|[mal, gute, nachr...|[mal, gute, nachr...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[mal, gute, nachr...|[mal_gute, gute_n...|[mal, gute, nachr...|(262144,[73,161,2...|(262144,[73,161,2...|[6.29779599699601...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|[NN, NN, NN, VB, ...|[love, iq, waddup...|[love_iq, iq_wadd...|[love, iq, waddup...|(262144,[0,23,57,...|(262144,[0,23,57,...|[0.11667293948602...|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|2022-09-25 17:30:05|arrow up orange h...|[arrow, orange, h...|[arrow, up, orang...|[NN, RB, NN, NN, ...|[NN, RB, NN, NN, ...|[arrow, heart, mu...|[orange_heart, he...|[arrow, heart, mu...|(262144,[5,6,7,69...|(262144,[5,6,7,69...|[3.94978715267354...|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|2021-12-09 21:32:21|a√ßougue vegano tu...|[a√ßougue, vegano,...|[a√ßougue, vegano,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[a√ßougue, vegano,...|[a√ßougue_vegano, ...|[a√ßougue, vegano,...|(262144,[6,182,26...|(262144,[6,182,26...|[0.00105030470865...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|2022-02-13 21:02:14|cacao butter cook...|[cacao, butter, c...|[cacao, butter, c...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[cacao, butter, c...|[cacao_butter, bu...|[cacao, butter, c...|(262144,[58,133,2...|(262144,[58,133,2...|[7.28659339562153...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|2022-02-10 17:53:01|coconut macaroons...|[coconut, macaroo...|[coconut, macaroo...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[coconut, macaroo...|[coconut_macaroon...|[coconut, macaroo...|(262144,[80,146,2...|(262144,[80,146,2...|[0.08553341730451...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|[know, senoritaco...|[do, you, know, s...|[VBP, PRP, VBP, N...|[VBP, PRP, VBP, N...|[know, senoritaco...|[know_senoritacos...|[know, senoritaco...|(262144,[0,50,71,...|(262144,[0,50,71,...|[0.32892281472714...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|[nature, intercon...|[nature, intercon...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[nature, intercon...|[nature_interconn...|[nature, intercon...|(262144,[0,27,49,...|(262144,[0,27,49,...|[0.23017719589893...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|[scarlet, punica,...|[scarlet, punica,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[scarlet, punica,...|[scarlet_punica, ...|[scarlet, punica,...|(262144,[0,1,197,...|(262144,[0,1,197,...|[2.59966450475405...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|2022-02-09 12:15:43|spanishstyle squi...|[spanishstyle, sq...|[spanishstyle, sq...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[spanishstyle, sq...|[spanishstyle_squ...|[spanishstyle, sq...|(262144,[133,236,...|(262144,[133,236,...|[4.73742524161183...|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|2022-02-26 16:40:54|vegan blt sandwic...|[vegan, blt, sand...|[vegan, blt, sand...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[vegan, blt, sand...|[vegan_blt, blt_s...|[vegan, blt, sand...|(262144,[0,12,20,...|(262144,[0,12,20,...|[5.85454937788639...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|[vegan, meinetweg...|[vegan, meinetweg...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[vegan, meinetweg...|[vegan_meinetwege...|[vegan, meinetweg...|(262144,[0,73,161...|(262144,[0,73,161...|[0.07408242057650...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|2022-02-09 19:00:20|vegetable frittat...|[vegetable, fritt...|[vegetable, fritt...|[JJ, NN, TO, VB, ...|[JJ, NN, TO, VB, ...|[vegetable, fritt...|[vegetable_fritta...|[vegetable, fritt...|(262144,[2,75,191...|(262144,[2,75,191...|[5.62147567252499...|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|2022-02-10 22:02:09|chocolate brownie...|[chocolate, brown...|[chocolate, brown...|[NN, NN, RB, JJ, ...|[NN, NN, RB, JJ, ...|[chocolate, brown...|[chocolate_browny...|[chocolate, brown...|(262144,[112,152,...|(262144,[112,152,...|[5.50081147017393...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|[nuna, thank, hel...|[nuna, thank, for...|[NN, VB, IN, PRP,...|[NN, VB, IN, PRP,...|[nuna, thank, say...|[aduh_saya, saya_...|[nuna, thank, say...|(262144,[0,78,94,...|(262144,[0,78,94,...|[0.00174560052764...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(input_list):\n",
    "    max = input_list[0]\n",
    "    index = 0\n",
    "    for i in range(1,len(input_list)):\n",
    "        if input_list[i] > max:\n",
    "            max = input_list[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "get_topic_udf = udf(lambda z: get_topic(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn('topic', get_topic_udf(\"topicDistributionCol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 285:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 13:03:10 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 289:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 13:20:41 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 294:============>                                            (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 13:20:42 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n",
      "22/11/30 13:20:42 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 13:20:43 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    }
   ],
   "source": [
    "freq_month = transformed.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "freq_month = freq_month.withColumn(\"month\", month(df[\"post_created_at\"]))\n",
    "\n",
    "freq_month = freq_month.groupBy('year', 'month', 'topic').agg(countDistinct(\"full_text\"))\\\n",
    "               .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                    .sort('year', 'month', ascending = True)\n",
    "freq_month = freq_month.select(concat_ws('_',freq_month.year, freq_month.month)\\\n",
    "                            .alias('date'), 'topic', 'freq').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>5</td>\n",
       "      <td>10339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>4</td>\n",
       "      <td>8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>10528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>16279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>3</td>\n",
       "      <td>8261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>5</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic   freq\n",
       "0   2021_10      5  10339\n",
       "1   2021_10      4   8625\n",
       "2   2021_10      2  10528\n",
       "3   2021_10      1  16279\n",
       "4   2021_10      3   8261\n",
       "..      ...    ...    ...\n",
       "73  2022_10      3    684\n",
       "74  2022_10      2    651\n",
       "75  2022_10      0    596\n",
       "76  2022_10      5    681\n",
       "77  2022_10      1    703\n",
       "\n",
       "[78 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "hoverongaps": true,
         "type": "heatmap",
         "x": [
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_12",
          "2021_12",
          "2021_12",
          "2021_12",
          "2021_12",
          "2021_12",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_10",
          "2022_10",
          "2022_10",
          "2022_10",
          "2022_10",
          "2022_10"
         ],
         "y": [
          5,
          4,
          2,
          1,
          3,
          0,
          1,
          2,
          0,
          4,
          3,
          5,
          3,
          2,
          0,
          5,
          4,
          1,
          4,
          3,
          1,
          5,
          0,
          2,
          1,
          4,
          0,
          2,
          3,
          5,
          5,
          2,
          4,
          1,
          3,
          0,
          4,
          0,
          2,
          5,
          3,
          1,
          2,
          4,
          5,
          3,
          0,
          1,
          4,
          0,
          2,
          1,
          5,
          3,
          5,
          4,
          2,
          3,
          1,
          0,
          2,
          3,
          5,
          4,
          0,
          1,
          5,
          2,
          1,
          0,
          3,
          4,
          4,
          3,
          2,
          0,
          5,
          1
         ],
         "z": [
          10339,
          8625,
          10528,
          16279,
          8261,
          14297,
          26327,
          16531,
          24405,
          18625,
          12198,
          19301,
          8937,
          17121,
          17067,
          18484,
          9482,
          27762,
          10410,
          12954,
          14708,
          11457,
          16914,
          13220,
          32591,
          26287,
          30197,
          23064,
          15138,
          20241,
          23668,
          17107,
          14419,
          31839,
          14473,
          25300,
          18372,
          26761,
          21592,
          18885,
          18310,
          32104,
          12781,
          12317,
          13439,
          11595,
          18655,
          21347,
          21155,
          27316,
          15682,
          26786,
          19911,
          17284,
          27711,
          24464,
          23594,
          21684,
          37140,
          32008,
          13466,
          11525,
          16754,
          8824,
          14902,
          21356,
          12131,
          10761,
          14174,
          17495,
          10967,
          20255,
          503,
          684,
          651,
          596,
          681,
          703
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   x=freq_month[\"date\"],\n",
    "                   y=freq_month[\"topic\"],\n",
    "                   z=freq_month[\"freq\"],\n",
    "                   hoverongaps = True))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>5</td>\n",
       "      <td>10339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>4</td>\n",
       "      <td>8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>10528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>16279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>3</td>\n",
       "      <td>8261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>3</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>5</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic   freq\n",
       "0   2021_10      5  10339\n",
       "1   2021_10      4   8625\n",
       "2   2021_10      2  10528\n",
       "3   2021_10      1  16279\n",
       "4   2021_10      3   8261\n",
       "..      ...    ...    ...\n",
       "73  2022_10      3    684\n",
       "74  2022_10      2    651\n",
       "75  2022_10      0    596\n",
       "76  2022_10      5    681\n",
       "77  2022_10      1    703\n",
       "\n",
       "[78 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 316:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340938, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((transformed.count(), len(transformed.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of top words per topic we would like to see and extract the words with our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|    0| [vegan, organic, joy, joy_joy, amp, food, healthylifestyle, healthyfood, health, healthy]|\n",
      "|    1|                       [vegan, organic, vegano, vegetarian, rofl, keto, go, eat, im, make]|\n",
      "|    2|                            [que, vegano, vegan, el, heart, tea, keto, organic, spice, en]|\n",
      "|    3|           [vegan, keto, und, ich, diet, die, check_mark, healthylifestyle, health, check]|\n",
      "|    4|[small_blue, blue_diamond, small_blue_diamond, small, blue, diamond, organic, diamond_s...|\n",
      "|    5|                           [vegan, organic, sob, amp, good, make, soap, sob_sob, diet, im]|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 325:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "vegan\n",
      "organic\n",
      "joy\n",
      "joy_joy\n",
      "amp\n",
      "food\n",
      "healthylifestyle\n",
      "healthyfood\n",
      "health\n",
      "healthy\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "vegan\n",
      "organic\n",
      "vegano\n",
      "vegetarian\n",
      "rofl\n",
      "keto\n",
      "go\n",
      "eat\n",
      "im\n",
      "make\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "que\n",
      "vegano\n",
      "vegan\n",
      "el\n",
      "heart\n",
      "tea\n",
      "keto\n",
      "organic\n",
      "spice\n",
      "en\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "vegan\n",
      "keto\n",
      "und\n",
      "ich\n",
      "diet\n",
      "die\n",
      "check_mark\n",
      "healthylifestyle\n",
      "health\n",
      "check\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "small_blue\n",
      "blue_diamond\n",
      "small_blue_diamond\n",
      "small\n",
      "blue\n",
      "diamond\n",
      "organic\n",
      "diamond_small\n",
      "vegan\n",
      "food\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "vegan\n",
      "organic\n",
      "sob\n",
      "amp\n",
      "good\n",
      "make\n",
      "soap\n",
      "sob_sob\n",
      "diet\n",
      "im\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 14:04:30 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 139945 ms exceeds timeout 120000 ms\n",
      "22/11/30 14:04:30 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
