{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit\n",
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import emojis\n",
    "from translate import Translator\n",
    "\n",
    "import sparknlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:26:14 WARN Utils: Your hostname, MacBook-Pro-van-Wouter.local resolves to a loopback address: 127.0.0.1; using 192.168.0.134 instead (on interface en0)\n",
      "22/12/01 18:26:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/wouterdewitte/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/wouterdewitte/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-m1_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4a394df-3147-44d5-9ba3-ff3a8c6a8f32;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.2.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 209ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.2.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c4a394df-3147-44d5-9ba3-ff3a8c6a8f32\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:26:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start(m1=True)\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "#sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "#spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "#path_json = \".././../data/Topic_vegan/*.json\"\n",
    "\n",
    "# use this if you want all the tweet files, but this is usually too large\n",
    "#df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:26:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1595676"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"_vegan_\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic_vegan/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "#filter on correct files via keyword\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>≈ü·ë≠ùêÑcŒπ‚í∫ùî∞ÔΩÅÔº≠ùî≤ùê¨ÔΩÖ·µà ü¶áüå≥üêíüê¥üêõ</td>\n",
       "      <td>speciesamused</td>\n",
       "      <td>Tue Sep 13 22:32:32 +0000 2022</td>\n",
       "      <td>RT @animalsavemvmt: Do you see us? Will you he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camuka üáπüá∑</td>\n",
       "      <td>Zomorok</td>\n",
       "      <td>Tue Sep 13 22:32:26 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael Belton</td>\n",
       "      <td>38Belton</td>\n",
       "      <td>Tue Sep 13 22:32:26 +0000 2022</td>\n",
       "      <td>RT @MyVegan_Reach: Cows are forcibly impregnat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stare Decisis ‚ìã</td>\n",
       "      <td>do_nothing_dem</td>\n",
       "      <td>Tue Sep 13 22:32:16 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mally</td>\n",
       "      <td>mizzishyde</td>\n",
       "      <td>Tue Sep 13 22:32:09 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hiedra-vegan</td>\n",
       "      <td>vegan02131055</td>\n",
       "      <td>Tue Sep 13 22:32:07 +0000 2022</td>\n",
       "      <td>RT @Dodo_Tribe: Jared Leto - \"No More Pus\"  #G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RadioFreeKrsna</td>\n",
       "      <td>JFave5</td>\n",
       "      <td>Tue Sep 13 22:31:54 +0000 2022</td>\n",
       "      <td>RT @veganrecipebowl: This recipe is also #vega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VerdeVetriolo@gingerzoerescueranch</td>\n",
       "      <td>VerdeVetriolo</td>\n",
       "      <td>Tue Sep 13 22:31:54 +0000 2022</td>\n",
       "      <td>RT @animalsavemvmt: Do you see us? Will you he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lightspeed</td>\n",
       "      <td>LightspeedSteps</td>\n",
       "      <td>Tue Sep 13 22:31:49 +0000 2022</td>\n",
       "      <td>RT @DanielleAnd15: #vegan https://t.co/a1eSrYlq23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kaz5thlife</td>\n",
       "      <td>kaz6thlife</td>\n",
       "      <td>Tue Sep 13 22:31:49 +0000 2022</td>\n",
       "      <td>Âà∫ÊøÄÁöÑ„Å†„Åë„Å©‚Ä•ÂãïÁâ©Â•Ω„Åç„Å™„Çâ„ÄÅÂøÖ„ÅöÁêÜËß£„Åô„ÇãÁîªÂÉè„Åß„Åô„ÄÇ„Çà„Å≠‚ùóÔ∏è#vegan „ÅÆÊ∞óÊåÅ„Å°„ÅØËâØ„ÅèÁêÜËß£„Åß...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name      screen_name  \\\n",
       "0                 ≈ü·ë≠ùêÑcŒπ‚í∫ùî∞ÔΩÅÔº≠ùî≤ùê¨ÔΩÖ·µà ü¶áüå≥üêíüê¥üêõ    speciesamused   \n",
       "1                           Camuka üáπüá∑          Zomorok   \n",
       "2                      Michael Belton         38Belton   \n",
       "3                     Stare Decisis ‚ìã   do_nothing_dem   \n",
       "4                               mally       mizzishyde   \n",
       "5                        hiedra-vegan    vegan02131055   \n",
       "6                      RadioFreeKrsna           JFave5   \n",
       "7  VerdeVetriolo@gingerzoerescueranch    VerdeVetriolo   \n",
       "8                          Lightspeed  LightspeedSteps   \n",
       "9                          kaz5thlife       kaz6thlife   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Sep 13 22:32:32 +0000 2022   \n",
       "1  Tue Sep 13 22:32:26 +0000 2022   \n",
       "2  Tue Sep 13 22:32:26 +0000 2022   \n",
       "3  Tue Sep 13 22:32:16 +0000 2022   \n",
       "4  Tue Sep 13 22:32:09 +0000 2022   \n",
       "5  Tue Sep 13 22:32:07 +0000 2022   \n",
       "6  Tue Sep 13 22:31:54 +0000 2022   \n",
       "7  Tue Sep 13 22:31:54 +0000 2022   \n",
       "8  Tue Sep 13 22:31:49 +0000 2022   \n",
       "9  Tue Sep 13 22:31:49 +0000 2022   \n",
       "\n",
       "                                           full_text  \n",
       "0  RT @animalsavemvmt: Do you see us? Will you he...  \n",
       "1  RT @angie_karan: #vegan \\n     for the animals...  \n",
       "2  RT @MyVegan_Reach: Cows are forcibly impregnat...  \n",
       "3  RT @angie_karan: #vegan \\n     for the animals...  \n",
       "4  RT @angie_karan: #vegan \\n     for the animals...  \n",
       "5  RT @Dodo_Tribe: Jared Leto - \"No More Pus\"  #G...  \n",
       "6  RT @veganrecipebowl: This recipe is also #vega...  \n",
       "7  RT @animalsavemvmt: Do you see us? Will you he...  \n",
       "8  RT @DanielleAnd15: #vegan https://t.co/a1eSrYlq23  \n",
       "9  Âà∫ÊøÄÁöÑ„Å†„Åë„Å©‚Ä•ÂãïÁâ©Â•Ω„Åç„Å™„Çâ„ÄÅÂøÖ„ÅöÁêÜËß£„Åô„ÇãÁîªÂÉè„Åß„Åô„ÄÇ„Çà„Å≠‚ùóÔ∏è#vegan „ÅÆÊ∞óÊåÅ„Å°„ÅØËâØ„ÅèÁêÜËß£„Åß...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF‚Äôs are used to extend the functions of the framework and re-use these functions on multiple DataFrame‚Äôs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "\n",
    "#df.printSchema()\n",
    "#df.count() #1340938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|emoji_count|      text_tokenized|num_words|num_hashtags|num_mentions|num_exclamation_marks|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33|          0|[!\\n#vegan, #GoVe...|        4|           3|           0|                    1|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|          0|[!, We, will, be,...|       35|          21|           0|                    3|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|          0|[!!, Daily, Updat...|       31|           9|           2|                    2|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|          0|[!B, Mal, gute, N...|       30|           2|           0|                    1|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|          0|[!love, !iq, !wad...|       32|          14|           2|                    5|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|          0|[\", DID, YOU, KNO...|       18|          13|           0|                    0|\n",
      "|     Anandita Pathak|   snowy_lizzie|Sun Jan 02 08:22:...|\" If having a sou...|2022-01-02 08:22:18|          0|[\", If, having, a...|       35|          10|           0|                    0|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|          0|[\", Nature's, Int...|       30|          21|           2|                    0|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|          0|[\", Scarlet, Puni...|       32|          21|           0|                    0|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|          0|[\", Vegan, meinet...|       22|           1|           0|                    1|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|          2|[\", nuna, \"\\n\", T...|       20|           0|           0|                    0|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24|          0|[\",  ú·¥Ä·¥ç,  ô, ·¥¢…™…¥·¥Ö…¢...|       13|           1|           0|                    0|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|          0|[\"#Dairy, milk, n...|       36|          15|           0|                    0|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|          0|[\"#NewYorkCity, s...|       40|           6|           0|                    0|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|          0|[\"#Vegan, food, i...|        5|           1|           0|                    0|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|          0|[\"#Vegan, food‚Äôs,...|       37|           2|           0|                    0|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|          0|[\"'My, bad, chole...|       22|           0|           1|                    0|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|          0|[\"'Vegan, Festiva...|       25|           1|           0|                    0|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|          0|[\"(1), Don't, be,...|       32|           7|           0|                    1|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|          1|[\"...or, vegan, o...|        5|           0|           0|                    0|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df = df.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_2520/1845679024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mclean_text_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words consisting out of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "\n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4ab2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.pretrained import PretrainedPipeline \n",
    "#pipeline = PretrainedPipeline(\"translate_mul_en\", lang = \"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df = pipeline.transform(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:28:57 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 29 (TID 2111): Attempting to kill Python Worker\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|         translation|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|did you know seno...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|natures interconn...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|scarlet punica gr...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|vegan for all I c...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|nuna, thanks for ...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|dairy milk no lon...|dairy milk no lon...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|newyorkcity schoo...|newyorkcity schoo...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|vegan food is dis...|vegan food is dis...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|vegan food‚Äôs abil...|vegan food‚Äôs abil...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|my bad cholestero...|my bad cholestero...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|vegan festival ho...|vegan festival ho...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|dont be too hard ...|dont be too hard ...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|or vegan ones bra...|or vegan ones bra...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|2022-03-13 00:23:54|vita russia anima...|vita russia anima...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator() \n",
    "translation = translator.translate(\"test\", dest='en')\n",
    "\n",
    "def translate(tweet):\n",
    "    if len(tweet)!=0:\n",
    "        translator = Translator() \n",
    "        translation = translator.translate(tweet, dest = 'en')\n",
    "        return(translation.text)\n",
    "    return None\n",
    "\n",
    "translate_udf = F.udf(translate, StringType())\n",
    "\n",
    "twitter_df = twitter_df.withColumn(\"translation\", translate_udf(\"text\"))\n",
    "\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- translation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df = twitter_df.withColumn(\"translation\", twitter_df.translation.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df = twitter_df.drop(\"document\",\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:30:16 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 41 (TID 2264): Attempting to kill Python Worker\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|         translation|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|did you know seno...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|natures interconn...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|scarlet punica gr...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|vegan for all I c...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|nuna, thanks for ...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|dairy milk no lon...|dairy milk no lon...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|newyorkcity schoo...|newyorkcity schoo...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|vegan food is dis...|vegan food is dis...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|vegan food‚Äôs abil...|vegan food‚Äôs abil...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|my bad cholestero...|my bad cholestero...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|vegan festival ho...|vegan festival ho...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|dont be too hard ...|dont be too hard ...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|or vegan ones bra...|or vegan ones bra...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|2022-03-13 00:23:54|vita russia anima...|vita russia anima...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic Modelling\n",
    "\n",
    "https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb\n",
    "\n",
    "https://www.johnsnowlabs.com/spark-nlp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Spark NLP pipeline\n",
    "\n",
    "### 3.1.1 Basic NLP pipeline\n",
    "\n",
    "DocumentAssembler converts data into Spark NLP annotation format that can be used by Spark NLP annotators. Prepares data into a format that is processable by Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "                .setInputCol(\"translation\") \\\n",
    "                .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize the data with Tokenizer. Tokenizes raw text in document type columns into TokenizedSentence. Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean out the data and lower it with Normalizer. Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with LemmatizerModel.\n",
    "Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with . The dictionary can be set as a delimited text file. Pretrained models can be loaded with ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP doesn't provide stop word list, hence, we will use nltk package to download stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with NGramGenerator in Spark NLP. N-grams try to predict which word will have the highest probability of appearing with the other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP. We can access the model with PerceptronModel. Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "[ / ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import PerceptronModel\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'lemmatized']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with Finisher. Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to input everything into a pipeline. Pipeline functionality is accessible with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 ngrammer,\n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pipeline.fit(twitter_df).transform(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, screen_name: string, created_at: string, post_created_at: timestamp, translation: string, finished_unigrams: array<string>, finished_ngrams: array<string>, finished_pos: array<string>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.drop(\"full_text\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:31:49 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 59 (TID 2452): Attempting to kill Python Worker\n",
      "22/12/01 18:31:49 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 59 (TID 2449): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:============>                                             (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:31:51 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 59 (TID 2455): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===================>                                      (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:31:53 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 59 (TID 2454): Attempting to kill Python Worker\n",
      "22/12/01 18:31:53 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 59 (TID 2448): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:31:55 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 59 (TID 2451): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:31:57 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 59 (TID 2453): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:32:05 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 59 (TID 2450): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:32:41 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 59 (TID 2456): Attempting to kill Python Worker\n",
      "+--------------------+--------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|   screen_name|          created_at|           full_text|    post_created_at|                text|         translation|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+--------------------+--------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã| vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|\n",
      "|üå±Veg-In-Out Mark...|veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|\n",
      "|         Mix 93.8 FM|      Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|\n",
      "|             Caroona|      Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|[good, news, than...|[good, news, than...|[JJ, NN, VB, TO, ...|\n",
      "|             Bugatea|      Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|\n",
      "+--------------------+--------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Extended NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have our data in a form of unigrams that are lemmatized, with no stop words in there. I think it is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let's filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let's restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don't use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
    "\n",
    "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing.\n",
    "\n",
    "Let's start this processing. First, we need to join all our POS tags obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize our POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate n-grams from them in the same way we did that for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are ready to get POS tags ngrams with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create this new Spark NLP pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and again fit it and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what kind of data we have to operate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'screen_name',\n",
       " 'created_at',\n",
       " 'full_text',\n",
       " 'post_created_at',\n",
       " 'text',\n",
       " 'translation',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are our word n-grams with their corresponding pos n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:23:38 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 76 (TID 2616): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:======>                                                   (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:23:48 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 76 (TID 2612): Attempting to kill Python Worker\n",
      "22/12/02 08:23:48 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 76 (TID 2615): Attempting to kill Python Worker\n",
      "22/12/02 08:23:48 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 76 (TID 2614): Attempting to kill Python Worker\n",
      "22/12/02 08:23:48 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 76 (TID 2611): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:23:50 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 76 (TID 2617): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:23:52 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 76 (TID 2610): Attempting to kill Python Worker\n",
      "22/12/02 08:23:52 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 76 (TID 2613): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:26:48 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 76 (TID 2618): Attempting to kill Python Worker\n",
      "+--------------------+--------------------+\n",
      "|     finished_ngrams| finished_pos_ngrams|\n",
      "+--------------------+--------------------+\n",
      "|[this, afternoon,...|[DT, NN, VBG, JJ,...|\n",
      "|[sweet, potato, a...|[JJ, NN, CC, NN, ...|\n",
      "|[creamy, mushroom...|[NN, NN, NN, NN, ...|\n",
      "|[polypieter, vegg...|[NN, NN, VB, RB, ...|\n",
      "|[enrich, with, na...|[NN, IN, JJ, NN, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe. WHAT ARE THE POS TAGSSSS\n",
    "\n",
    "- NN is singular noun\n",
    "- NNS is plural noun\n",
    "- VB is verb\n",
    "- VBP verb, present tense not 3rd person singular(wrap)\n",
    "- JJ is an adjective (large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
    "                                                              F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how our filtered unigrams look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:29:20 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 93 (TID 2774): Attempting to kill Python Worker\n",
      "22/12/02 08:29:20 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 93 (TID 2775): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:============>                                             (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:29:22 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 93 (TID 2777): Attempting to kill Python Worker\n",
      "22/12/02 08:29:22 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 93 (TID 2779): Attempting to kill Python Worker\n",
      "22/12/02 08:29:22 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 93 (TID 2773): Attempting to kill Python Worker\n",
      "22/12/02 08:29:22 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 93 (TID 2772): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:29:24 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 93 (TID 2776): Attempting to kill Python Worker\n",
      "22/12/02 08:29:24 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 93 (TID 2778): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:31:58 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 93 (TID 2780): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|                                   [sweet, potato, fritter, mint, dip, vegan, plant, base]|\n",
      "|                           [creamy, mushroom, bucatini, vegan, veganforlife, forevervegan]|\n",
      "|[polypieter, veggie, already, vegan, almost, impossible, get, hey, dont, ingredient, ve...|\n",
      "|                             [enrich, origin, ingredient, certify, vegan, wink, fragrance]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the function on word and POS n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
    "                                                                    F.col('finished_pos_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what we get after filtering for n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:35:00 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 110 (TID 2940): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:35:04 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 110 (TID 2934): Attempting to kill Python Worker\n",
      "22/12/02 08:35:04 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 110 (TID 2935): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:35:12 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 110 (TID 2941): Attempting to kill Python Worker\n",
      "22/12/02 08:35:12 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 110 (TID 2939): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:35:14 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 110 (TID 2936): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:35:18 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 110 (TID 2937): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:35:24 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 110 (TID 2938): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:37:51 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 110 (TID 2942): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                           filtered_ngrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[relax_conversation, book_dontmesswithasilverback, dontmesswithasilverback_govegan, gov...|\n",
      "|[sweet_potato, courgette_fritter, mint_yoghurt, yoghurt_dip, dip_vegan, vegan_plant, pl...|\n",
      "|[creamy_mushroom, mushroom_bucatini, bucatini_vegan, vegan_veganforlife, veganforlife_f...|\n",
      "|[polypieter_veggie, super_vegan, get_deficiency, be_vegan, theory_hey, dont_read, produ...|\n",
      "|[natural_origin, origin_ingredient, vegan_society, love_wink, wink_fragrance, fragrance...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(F.col('filtered_unigrams'), \n",
    "                                                      F.col('filtered_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our final look of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:40:09 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 127 (TID 3102): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:40:15 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 127 (TID 3100): Attempting to kill Python Worker\n",
      "22/12/02 08:40:15 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 127 (TID 3099): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:40:17 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 127 (TID 3098): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:=========================>                               (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:40:19 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 127 (TID 3103): Attempting to kill Python Worker\n",
      "22/12/02 08:40:19 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 127 (TID 3101): Attempting to kill Python Worker\n",
      "22/12/02 08:40:19 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 127 (TID 3096): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:40:21 WARN PythonUDFRunner: Detected deadlock while completing task 1.0 in stage 127 (TID 3097): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 08:42:44 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 127 (TID 3104): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                     final|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|[sweet, potato, fritter, mint, dip, vegan, plant, base, sweet_potato, courgette_fritter...|\n",
      "|[creamy, mushroom, bucatini, vegan, veganforlife, forevervegan, creamy_mushroom, mushro...|\n",
      "|[polypieter, veggie, already, vegan, almost, impossible, get, hey, dont, ingredient, ve...|\n",
      "|[enrich, origin, ingredient, certify, vegan, wink, fragrance, natural_origin, origin_in...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('final').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set to vectorization of our data. First, we will proceed with TF (term frequency) vectorization with CountVectorizer in PySpark. We fit tf dictionary and then transform the data to vectors of counts.\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 09:48:13 ERROR Executor: Exception in task 6.0 in stage 170.0 (TID 3586)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_6194/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 620, in send\n",
      "    response = self.send_handling_redirects(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 647, in send_handling_redirects\n",
      "    response = self.send_handling_auth(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 684, in send_handling_auth\n",
      "    response = self.send_single_request(request, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 727, in send_single_request\n",
      "    raise\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 160, in request\n",
      "    raise\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 78, in request\n",
      "    return self.connection.request(method, url, headers, stream, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 121, in request\n",
      "    raise\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 292, in request\n",
      "    status_code, headers = self.receive_response(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 344, in receive_response\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/12/02 09:48:13 WARN TaskSetManager: Lost task 6.0 in stage 170.0 (TID 3586) (192.168.0.134 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_6194/2024449930.py\", line 9, in translate\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n",
      "    data, response = self._translate(text, dest, src, kwargs)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n",
      "    r = self.client.get(url, params=params)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n",
      "    return self.request(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n",
      "    return self.send(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 620, in send\n",
      "    response = self.send_handling_redirects(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 647, in send_handling_redirects\n",
      "    response = self.send_handling_auth(\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 684, in send_handling_auth\n",
      "    response = self.send_single_request(request, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 727, in send_single_request\n",
      "    raise\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 160, in request\n",
      "    raise\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 78, in request\n",
      "    return self.connection.request(method, url, headers, stream, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 121, in request\n",
      "    raise\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 292, in request\n",
      "    status_code, headers = self.receive_response(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 344, in receive_response\n",
      "    event = self.connection.wait_for_event(self.stream_id, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n",
      "    self.receive_events(timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n",
      "    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n",
      "    return self.sock.recv(n)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n",
      "    return False\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n",
      "    raise\n",
      "httpcore._exceptions.ReadTimeout: The read operation timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "\n",
      "22/12/02 09:48:13 ERROR TaskSetManager: Task 6 in stage 170.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_6194/2024449930.py\", line 9, in translate\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n    data, response = self._translate(text, dest, src, kwargs)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n    r = self.client.get(url, params=params)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n    return self.request(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n    return self.send(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 620, in send\n    response = self.send_handling_redirects(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 647, in send_handling_redirects\n    response = self.send_handling_auth(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 684, in send_handling_auth\n    response = self.send_single_request(request, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 727, in send_single_request\n    raise\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 160, in request\n    raise\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 78, in request\n    return self.connection.request(method, url, headers, stream, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 121, in request\n    raise\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 292, in request\n    status_code, headers = self.receive_response(timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 344, in receive_response\n    event = self.connection.wait_for_event(self.stream_id, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n    self.receive_events(timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n    return self.sock.recv(n)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n    return False\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n    raise\nhttpcore._exceptions.ReadTimeout: The read operation timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_6194/1554251797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_6194/2024449930.py\", line 9, in translate\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 210, in translate\n    data, response = self._translate(text, dest, src, kwargs)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/googletrans/client.py\", line 108, in _translate\n    r = self.client.get(url, params=params)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 755, in get\n    return self.request(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 600, in request\n    return self.send(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 620, in send\n    response = self.send_handling_redirects(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 647, in send_handling_redirects\n    response = self.send_handling_auth(\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 684, in send_handling_auth\n    response = self.send_single_request(request, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\", line 727, in send_single_request\n    raise\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 160, in request\n    raise\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 78, in request\n    return self.connection.request(method, url, headers, stream, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 121, in request\n    raise\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 292, in request\n    status_code, headers = self.receive_response(timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 344, in receive_response\n    event = self.connection.wait_for_event(self.stream_id, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 197, in wait_for_event\n    self.receive_events(timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/http2.py\", line 204, in receive_events\n    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 62, in read\n    return self.sock.recv(n)\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/contextlib.py\", line 168, in __exit__\n    return False\n  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 13, in map_exceptions\n    raise\nhttpcore._exceptions.ReadTimeout: The read operation timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 09:48:13 WARN TaskSetManager: Lost task 8.0 in stage 170.0 (TID 3588) (192.168.0.134 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:>                                                        (0 + 7) / 9]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get TF results, we can account for words that are frequent for all the documents. We can use IDF (inverse document frequency) to lower score of such words.\n",
    "\n",
    "The inverse document frequency is a measure of whether a term is common or rare in a given document corpus. It is obtained by dividing the total number of documents by the number of documents containing the term in the corpus.\n",
    "\n",
    "While computing TF, all terms are considered equally important. However it is known that certain terms, such as ‚Äúis‚Äù, ‚Äúof‚Äù, and ‚Äúthat‚Äù, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as ‚Äúis‚Äù is present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n",
    "\n",
    "Now there are few other problems with the IDF , in case of a large corpus,say 100,000,000 , the IDF value explodes , to avoid the effect we take the log of idf .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Ftf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558&psig=AOvVaw2NphvHAPexM_UP_4UAcgLP&ust=1670086684586000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCJjOxO6z2_sCFQAAAAAdAAAAABAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 13:26:03 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 13:32:45 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2\n",
    "\n",
    "Finally, we are ready to model topics in our data with LDA (Latent Dirichlet Allocation). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 13:33:18 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 13:38:47 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 175 (TID 3601): Attempting to kill Python Worker\n",
      "22/12/01 13:38:47 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 175 (TID 3601): Attempting to kill Python Worker\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|         translation|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|[NN, NN, NN, NN_N...|[vegan, govegan, ...|[vegan_govegan, g...|[vegan, govegan, ...|(262144,[0,22,160...|(262144,[0,22,160...|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|[PRP, MD, VB, JJ,...|[today, tomorrow,...|[be_open, open_am...|[today, tomorrow,...|(262144,[0,2,11,1...|(262144,[0,2,11,1...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|[JJ, JJ, NN, IN, ...|[daily, update, t...|[daily_update, up...|[daily, update, t...|(262144,[2,21,39,...|(262144,[2,21,39,...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|[mal, gute, nachr...|[mal, gute, nachr...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[mal, gute, nachr...|[mal_gute, gute_n...|[mal, gute, nachr...|(262144,[25,54,90...|(262144,[25,54,90...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|[NN, NN, NN, VB, ...|[love, iq, waddup...|[love_iq, iq_wadd...|[love, iq, waddup...|(262144,[0,19,72,...|(262144,[0,19,72,...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|did you know seno...|[know, senoritaco...|[do, you, know, s...|[VBP, PRP, VBP, N...|[VBP, PRP, VBP, N...|[know, senoritaco...|[know_senoritacos...|[know, senoritaco...|(262144,[0,22,38,...|(262144,[0,22,38,...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|natures interconn...|[nature, intercon...|[nature, intercon...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[nature, intercon...|[nature_interconn...|[nature, intercon...|(262144,[0,11,29,...|(262144,[0,11,29,...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|scarlet punica gr...|[scarlet, punica,...|[scarlet, punica,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[scarlet, punica,...|[scarlet_punica, ...|[scarlet, punica,...|(262144,[0,20,162...|(262144,[0,20,162...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|vegan for all I c...|[vegan, meinetweg...|[vegan, meinetweg...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[vegan, meinetweg...|[vegan_meinetwege...|[vegan, meinetweg...|(262144,[0,25,54,...|(262144,[0,25,54,...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|nuna, thanks for ...|[nuna, thank, hel...|[nuna, thank, for...|[NN, VB, IN, PRP,...|[NN, VB, IN, PRP,...|[nuna, thank, say...|[aduh_saya, saya_...|[nuna, thank, say...|(262144,[0,106,13...|(262144,[0,106,13...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç_·¥¢…™…¥·¥Ö…¢…™, ·¥¢…™…¥·¥Ö...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|(262144,[0,19,101...|(262144,[0,19,101...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|dairy milk no lon...|dairy milk no lon...|[dairy, milk, lon...|[dairy, milk, no,...|[JJ, NN, DT, JJ, ...|[JJ, NN, DT, JJ, ...|[dairy, milk, ser...|[dairy_milk, long...|[dairy, milk, ser...|(262144,[12,22,49...|(262144,[12,22,49...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|newyorkcity schoo...|newyorkcity schoo...|[newyorkcity, sch...|[newyorkcity, sch...|[NN, NN, MD, VB, ...|[NN, NN, MD, VB, ...|[newyorkcity, sch...|[newyorkcity_scho...|[newyorkcity, sch...|(262144,[3,17,22,...|(262144,[3,17,22,...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|vegan food is dis...|vegan food is dis...|[vegan, food, dis...|[vegan, food, be,...|   [NN, NN, VB, VBG]|[NN, NN, VB, VBG,...|[vegan, food, dis...|        [vegan_food]|[vegan, food, dis...|(262144,[0,2,39,3...|(262144,[0,2,39,3...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|vegan food‚Äôs abil...|vegan food‚Äôs abil...|[vegan, food, abi...|[vegan, food, abi...|[NN, NN, NN, TO, ...|[NN, NN, NN, TO, ...|[vegan, food, abi...|[vegan_food, food...|[vegan, food, abi...|(262144,[0,2,39,5...|(262144,[0,2,39,5...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|my bad cholestero...|my bad cholestero...|[bad, cholesterol...|[i, bad, choleste...|[NNP, JJ, NN, NN,...|[NNP, JJ, NN, NN,...|[cholesterol, num...|[bad_cholesterol,...|[cholesterol, num...|(262144,[65,624,9...|(262144,[65,624,9...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|vegan festival ho...|vegan festival ho...|[vegan, festival,...|[vegan, festival,...|[NN, NN, JJ, NN, ...|[NN, NN, JJ, NN, ...|[vegan, festival,...|[vegan_festival, ...|[vegan, festival,...|(262144,[0,3,329,...|(262144,[0,3,329,...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|dont be too hard ...|dont be too hard ...|[dont, hard, does...|[dont, be, too, h...|[NN, VB, RB, JJ, ...|[NN, VB, RB, JJ, ...|[dont, hard, scar...|[be_scary, good_q...|[dont, hard, scar...|(262144,[28,29,30...|(262144,[28,29,30...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|or vegan ones bra...|or vegan ones bra...|[vegan, one, brah...|[or, vegan, one, ...|[CC, NN, CD, NN, NN]|[CC, NN, CD, NN, ...|        [one, skull]|        [brah_skull]|[one, skull, brah...|(262144,[34,1203]...|(262144,[34,1203]...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|2022-03-13 00:23:54|vita russia anima...|vita russia anima...|[vita, russia, an...|[vita, russia, an...|[NN, NNP, NN, NN,...|[NN, NNP, NN, NN,...|[vita, animal, ri...|[animal_right, ri...|[vita, animal, ri...|(262144,[12,44,11...|(262144,[12,44,11...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tfidf_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:05:35 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.>                (0 + 0) / 9]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_2223/3791242948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_idf_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetTopicDistributionCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topicDistributionCol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 3\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features').setTopicDistributionCol(\"topicDistributionCol\")\n",
    "lda_model = lda.fit(tfidf_result)\n",
    "transformed = lda_model.transform(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 13:47:07 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 301:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|         translation|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|topicDistributionCol|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|[NN, NN, NN, NN_N...|[vegan, govegan, ...|[vegan_govegan, g...|[vegan, govegan, ...|(262144,[0,22,160...|(262144,[0,22,160...|[0.00425177337382...|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|[PRP, MD, VB, JJ,...|[today, tomorrow,...|[be_open, open_am...|[today, tomorrow,...|(262144,[0,2,11,1...|(262144,[0,2,11,1...|[3.80687915953491...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|[JJ, JJ, NN, IN, ...|[daily, update, t...|[daily_update, up...|[daily, update, t...|(262144,[2,21,39,...|(262144,[2,21,39,...|[0.00142510423339...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|good news thanks ...|[mal, gute, nachr...|[mal, gute, nachr...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[mal, gute, nachr...|[mal_gute, gute_n...|[mal, gute, nachr...|(262144,[25,54,90...|(262144,[25,54,90...|[0.14198210365489...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|[NN, NN, NN, VB, ...|[love, iq, waddup...|[love_iq, iq_wadd...|[love, iq, waddup...|(262144,[0,19,72,...|(262144,[0,19,72,...|[0.15627971329825...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|did you know seno...|[know, senoritaco...|[do, you, know, s...|[VBP, PRP, VBP, N...|[VBP, PRP, VBP, N...|[know, senoritaco...|[know_senoritacos...|[know, senoritaco...|(262144,[0,22,38,...|(262144,[0,22,38,...|[0.00127026577656...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|natures interconn...|[nature, intercon...|[nature, intercon...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[nature, intercon...|[nature_interconn...|[nature, intercon...|(262144,[0,11,29,...|(262144,[0,11,29,...|[0.02851324416198...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|scarlet punica gr...|[scarlet, punica,...|[scarlet, punica,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[scarlet, punica,...|[scarlet_punica, ...|[scarlet, punica,...|(262144,[0,20,162...|(262144,[0,20,162...|[2.67128619879668...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|vegan for all I c...|[vegan, meinetweg...|[vegan, meinetweg...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[vegan, meinetweg...|[vegan_meinetwege...|[vegan, meinetweg...|(262144,[0,25,54,...|(262144,[0,25,54,...|[7.11269340272785...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|nuna, thanks for ...|[nuna, thank, hel...|[nuna, thank, for...|[NN, VB, IN, PRP,...|[NN, VB, IN, PRP,...|[nuna, thank, say...|[aduh_saya, saya_...|[nuna, thank, say...|(262144,[0,106,13...|(262144,[0,106,13...|[0.23762834100976...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç_·¥¢…™…¥·¥Ö…¢…™, ·¥¢…™…¥·¥Ö...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|(262144,[0,19,101...|(262144,[0,19,101...|[6.10996360207366...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|dairy milk no lon...|dairy milk no lon...|[dairy, milk, lon...|[dairy, milk, no,...|[JJ, NN, DT, JJ, ...|[JJ, NN, DT, JJ, ...|[dairy, milk, ser...|[dairy_milk, long...|[dairy, milk, ser...|(262144,[12,22,49...|(262144,[12,22,49...|[0.99682947173435...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|newyorkcity schoo...|newyorkcity schoo...|[newyorkcity, sch...|[newyorkcity, sch...|[NN, NN, MD, VB, ...|[NN, NN, MD, VB, ...|[newyorkcity, sch...|[newyorkcity_scho...|[newyorkcity, sch...|(262144,[3,17,22,...|(262144,[3,17,22,...|[0.06798444549860...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|vegan food is dis...|vegan food is dis...|[vegan, food, dis...|[vegan, food, be,...|   [NN, NN, VB, VBG]|[NN, NN, VB, VBG,...|[vegan, food, dis...|        [vegan_food]|[vegan, food, dis...|(262144,[0,2,39,3...|(262144,[0,2,39,3...|[0.00976189358319...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|vegan food‚Äôs abil...|vegan food‚Äôs abil...|[vegan, food, abi...|[vegan, food, abi...|[NN, NN, NN, TO, ...|[NN, NN, NN, TO, ...|[vegan, food, abi...|[vegan_food, food...|[vegan, food, abi...|(262144,[0,2,39,5...|(262144,[0,2,39,5...|[0.00176421082406...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|my bad cholestero...|my bad cholestero...|[bad, cholesterol...|[i, bad, choleste...|[NNP, JJ, NN, NN,...|[NNP, JJ, NN, NN,...|[cholesterol, num...|[bad_cholesterol,...|[cholesterol, num...|(262144,[65,624,9...|(262144,[65,624,9...|[0.00150703883739...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|vegan festival ho...|vegan festival ho...|[vegan, festival,...|[vegan, festival,...|[NN, NN, JJ, NN, ...|[NN, NN, JJ, NN, ...|[vegan, festival,...|[vegan_festival, ...|[vegan, festival,...|(262144,[0,3,329,...|(262144,[0,3,329,...|[4.11312735716878...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|dont be too hard ...|dont be too hard ...|[dont, hard, does...|[dont, be, too, h...|[NN, VB, RB, JJ, ...|[NN, VB, RB, JJ, ...|[dont, hard, scar...|[be_scary, good_q...|[dont, hard, scar...|(262144,[28,29,30...|(262144,[28,29,30...|[0.00175648611641...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|or vegan ones bra...|or vegan ones bra...|[vegan, one, brah...|[or, vegan, one, ...|[CC, NN, CD, NN, NN]|[CC, NN, CD, NN, ...|        [one, skull]|        [brah_skull]|[one, skull, brah...|(262144,[34,1203]...|(262144,[34,1203]...|[0.93431030752179...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|2022-03-13 00:23:54|vita russia anima...|vita russia anima...|[vita, russia, an...|[vita, russia, an...|[NN, NNP, NN, NN,...|[NN, NNP, NN, NN,...|[vita, animal, ri...|[animal_right, ri...|[vita, animal, ri...|(262144,[12,44,11...|(262144,[12,44,11...|[0.00109239908883...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(input_list):\n",
    "    max = input_list[0]\n",
    "    index = 0\n",
    "    for i in range(1,len(input_list)):\n",
    "        if input_list[i] > max:\n",
    "            max = input_list[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "get_topic_udf = udf(lambda z: get_topic(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn('topic', get_topic_udf(\"topicDistributionCol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 309:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 13:52:29 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 313:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:01:04 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 318:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:01:05 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:01:05 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n",
      "22/12/01 14:01:06 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n"
     ]
    }
   ],
   "source": [
    "freq_month = transformed.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "freq_month = freq_month.withColumn(\"month\", month(df[\"post_created_at\"]))\n",
    "\n",
    "freq_month = freq_month.groupBy('year', 'month', 'topic').agg(countDistinct(\"full_text\"))\\\n",
    "               .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                    .sort('year', 'month', ascending = True)\n",
    "freq_month = freq_month.select(concat_ws('_',freq_month.year, freq_month.month)\\\n",
    "                            .alias('date'), 'topic', 'freq').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>5</td>\n",
       "      <td>9472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>4</td>\n",
       "      <td>8047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>4126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>3</td>\n",
       "      <td>3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>3</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>5</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic  freq\n",
       "0   2021_10      5  9472\n",
       "1   2021_10      4  8047\n",
       "2   2021_10      2  4126\n",
       "3   2021_10      1  4224\n",
       "4   2021_10      3  3156\n",
       "..      ...    ...   ...\n",
       "73  2022_10      3   540\n",
       "74  2022_10      2   531\n",
       "75  2022_10      0   395\n",
       "76  2022_10      5   825\n",
       "77  2022_10      1   668\n",
       "\n",
       "[78 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "hoverongaps": true,
         "type": "heatmap",
         "x": [
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_12",
          "2021_12",
          "2021_12",
          "2021_12",
          "2021_12",
          "2021_12",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_10",
          "2022_10",
          "2022_10",
          "2022_10",
          "2022_10",
          "2022_10"
         ],
         "y": [
          5,
          4,
          2,
          1,
          3,
          0,
          1,
          2,
          0,
          4,
          3,
          5,
          3,
          2,
          0,
          5,
          4,
          1,
          4,
          3,
          1,
          5,
          0,
          2,
          1,
          4,
          0,
          2,
          3,
          5,
          5,
          2,
          4,
          1,
          3,
          0,
          4,
          0,
          2,
          5,
          3,
          1,
          2,
          4,
          5,
          3,
          0,
          1,
          4,
          0,
          2,
          1,
          5,
          3,
          5,
          4,
          2,
          3,
          1,
          0,
          2,
          3,
          5,
          4,
          0,
          1,
          5,
          2,
          1,
          0,
          3,
          4,
          4,
          3,
          2,
          0,
          5,
          1
         ],
         "z": [
          9472,
          8047,
          4126,
          4224,
          3156,
          5182,
          3828,
          4043,
          7262,
          7826,
          3218,
          10432,
          7654,
          9634,
          10949,
          16875,
          17329,
          8780,
          8342,
          4126,
          4339,
          6621,
          3436,
          6089,
          3528,
          5891,
          3102,
          3726,
          3746,
          5740,
          20580,
          10372,
          22023,
          11540,
          10221,
          13508,
          11383,
          7033,
          6431,
          11264,
          5570,
          7419,
          4695,
          8027,
          7429,
          4633,
          4238,
          5790,
          10689,
          6610,
          5926,
          7214,
          10558,
          5517,
          16743,
          17375,
          9590,
          8818,
          10720,
          11820,
          9372,
          8611,
          16590,
          17735,
          11533,
          10628,
          3847,
          2286,
          2910,
          2062,
          2692,
          4204,
          859,
          540,
          531,
          395,
          825,
          668
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   x=freq_month[\"date\"],\n",
    "                   y=freq_month[\"topic\"],\n",
    "                   z=freq_month[\"freq\"],\n",
    "                   hoverongaps = True))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>5</td>\n",
       "      <td>9472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>4</td>\n",
       "      <td>8047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>4126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>3</td>\n",
       "      <td>3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>3</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>5</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic  freq\n",
       "0   2021_10      5  9472\n",
       "1   2021_10      4  8047\n",
       "2   2021_10      2  4126\n",
       "3   2021_10      1  4224\n",
       "4   2021_10      3  3156\n",
       "..      ...    ...   ...\n",
       "73  2022_10      3   540\n",
       "74  2022_10      2   531\n",
       "75  2022_10      0   395\n",
       "76  2022_10      5   825\n",
       "77  2022_10      1   668\n",
       "\n",
       "[78 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 340:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592251, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((transformed.count(), len(transformed.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of top words per topic we would like to see and extract the words with our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|    0|                                   [vegan, be_vegan, im, eat, een, en, food, ik, day, die]|\n",
      "|    1|                                      [ich, und, que, das, ist, nicht, die, les, soap, es]|\n",
      "|    2|[vegan, diet, raw, twitter, atheist, healthy, instagram_facebook, twitter_instagram, in...|\n",
      "|    3|              [recipe, vegan, amp, soap, skincare, organic, check, check_mark, mark, food]|\n",
      "|    4|                     [vegan, amp, cheese, im, make, food, sob, recipe, vegetarian, burger]|\n",
      "|    5|              [heart, rofl, vegan, be_vegan, go, food, collision, rofl_rofl, animal, fire]|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 349:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "vegan\n",
      "be_vegan\n",
      "im\n",
      "eat\n",
      "een\n",
      "en\n",
      "food\n",
      "ik\n",
      "day\n",
      "die\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "ich\n",
      "und\n",
      "que\n",
      "das\n",
      "ist\n",
      "nicht\n",
      "die\n",
      "les\n",
      "soap\n",
      "es\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "vegan\n",
      "diet\n",
      "raw\n",
      "twitter\n",
      "atheist\n",
      "healthy\n",
      "instagram_facebook\n",
      "twitter_instagram\n",
      "instagram_facebook_youtube\n",
      "visit\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "recipe\n",
      "vegan\n",
      "amp\n",
      "soap\n",
      "skincare\n",
      "organic\n",
      "check\n",
      "check_mark\n",
      "mark\n",
      "food\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "vegan\n",
      "amp\n",
      "cheese\n",
      "im\n",
      "make\n",
      "food\n",
      "sob\n",
      "recipe\n",
      "vegetarian\n",
      "burger\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "heart\n",
      "rofl\n",
      "vegan\n",
      "be_vegan\n",
      "go\n",
      "food\n",
      "collision\n",
      "rofl_rofl\n",
      "animal\n",
      "fire\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
