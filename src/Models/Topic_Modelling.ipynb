{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit, pandas_udf, PandasUDFType\n",
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import emojis\n",
    "from translate import Translator\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "from sparknlp.pretrained import PretrainedPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/wouterdewitte/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/wouterdewitte/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-m1_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-06988f95-84f4-48de-8981-c17d3dac41a9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.2.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 277ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-m1_2.12;4.2.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-m1_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-06988f95-84f4-48de-8981-c17d3dac41a9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 20:24:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start(m1=True)\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "#sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "#spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "#path_json = \".././../data/Topic_vegan/*.json\"\n",
    "\n",
    "# use this if you want all the tweet files, but this is usually too large\n",
    "#df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 20:25:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1595676"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"_vegan_\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic_vegan/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "#filter on correct files via keyword\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>≈ü·ë≠ùêÑcŒπ‚í∫ùî∞ÔΩÅÔº≠ùî≤ùê¨ÔΩÖ·µà ü¶áüå≥üêíüê¥üêõ</td>\n",
       "      <td>speciesamused</td>\n",
       "      <td>Tue Sep 13 22:32:32 +0000 2022</td>\n",
       "      <td>RT @animalsavemvmt: Do you see us? Will you he...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camuka üáπüá∑</td>\n",
       "      <td>Zomorok</td>\n",
       "      <td>Tue Sep 13 22:32:26 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael Belton</td>\n",
       "      <td>38Belton</td>\n",
       "      <td>Tue Sep 13 22:32:26 +0000 2022</td>\n",
       "      <td>RT @MyVegan_Reach: Cows are forcibly impregnat...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stare Decisis ‚ìã</td>\n",
       "      <td>do_nothing_dem</td>\n",
       "      <td>Tue Sep 13 22:32:16 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mally</td>\n",
       "      <td>mizzishyde</td>\n",
       "      <td>Tue Sep 13 22:32:09 +0000 2022</td>\n",
       "      <td>RT @angie_karan: #vegan \\n     for the animals...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hiedra-vegan</td>\n",
       "      <td>vegan02131055</td>\n",
       "      <td>Tue Sep 13 22:32:07 +0000 2022</td>\n",
       "      <td>RT @Dodo_Tribe: Jared Leto - \"No More Pus\"  #G...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RadioFreeKrsna</td>\n",
       "      <td>JFave5</td>\n",
       "      <td>Tue Sep 13 22:31:54 +0000 2022</td>\n",
       "      <td>RT @veganrecipebowl: This recipe is also #vega...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VerdeVetriolo@gingerzoerescueranch</td>\n",
       "      <td>VerdeVetriolo</td>\n",
       "      <td>Tue Sep 13 22:31:54 +0000 2022</td>\n",
       "      <td>RT @animalsavemvmt: Do you see us? Will you he...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lightspeed</td>\n",
       "      <td>LightspeedSteps</td>\n",
       "      <td>Tue Sep 13 22:31:49 +0000 2022</td>\n",
       "      <td>RT @DanielleAnd15: #vegan https://t.co/a1eSrYlq23</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kaz5thlife</td>\n",
       "      <td>kaz6thlife</td>\n",
       "      <td>Tue Sep 13 22:31:49 +0000 2022</td>\n",
       "      <td>Âà∫ÊøÄÁöÑ„Å†„Åë„Å©‚Ä•ÂãïÁâ©Â•Ω„Åç„Å™„Çâ„ÄÅÂøÖ„ÅöÁêÜËß£„Åô„ÇãÁîªÂÉè„Åß„Åô„ÄÇ„Çà„Å≠‚ùóÔ∏è#vegan „ÅÆÊ∞óÊåÅ„Å°„ÅØËâØ„ÅèÁêÜËß£„Åß...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name      screen_name  \\\n",
       "0                 ≈ü·ë≠ùêÑcŒπ‚í∫ùî∞ÔΩÅÔº≠ùî≤ùê¨ÔΩÖ·µà ü¶áüå≥üêíüê¥üêõ    speciesamused   \n",
       "1                           Camuka üáπüá∑          Zomorok   \n",
       "2                      Michael Belton         38Belton   \n",
       "3                     Stare Decisis ‚ìã   do_nothing_dem   \n",
       "4                               mally       mizzishyde   \n",
       "5                        hiedra-vegan    vegan02131055   \n",
       "6                      RadioFreeKrsna           JFave5   \n",
       "7  VerdeVetriolo@gingerzoerescueranch    VerdeVetriolo   \n",
       "8                          Lightspeed  LightspeedSteps   \n",
       "9                          kaz5thlife       kaz6thlife   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Sep 13 22:32:32 +0000 2022   \n",
       "1  Tue Sep 13 22:32:26 +0000 2022   \n",
       "2  Tue Sep 13 22:32:26 +0000 2022   \n",
       "3  Tue Sep 13 22:32:16 +0000 2022   \n",
       "4  Tue Sep 13 22:32:09 +0000 2022   \n",
       "5  Tue Sep 13 22:32:07 +0000 2022   \n",
       "6  Tue Sep 13 22:31:54 +0000 2022   \n",
       "7  Tue Sep 13 22:31:54 +0000 2022   \n",
       "8  Tue Sep 13 22:31:49 +0000 2022   \n",
       "9  Tue Sep 13 22:31:49 +0000 2022   \n",
       "\n",
       "                                           full_text lang  \n",
       "0  RT @animalsavemvmt: Do you see us? Will you he...   en  \n",
       "1  RT @angie_karan: #vegan \\n     for the animals...   en  \n",
       "2  RT @MyVegan_Reach: Cows are forcibly impregnat...   en  \n",
       "3  RT @angie_karan: #vegan \\n     for the animals...   en  \n",
       "4  RT @angie_karan: #vegan \\n     for the animals...   en  \n",
       "5  RT @Dodo_Tribe: Jared Leto - \"No More Pus\"  #G...   en  \n",
       "6  RT @veganrecipebowl: This recipe is also #vega...   en  \n",
       "7  RT @animalsavemvmt: Do you see us? Will you he...   en  \n",
       "8  RT @DanielleAnd15: #vegan https://t.co/a1eSrYlq23  und  \n",
       "9  Âà∫ÊøÄÁöÑ„Å†„Åë„Å©‚Ä•ÂãïÁâ©Â•Ω„Åç„Å™„Çâ„ÄÅÂøÖ„ÅöÁêÜËß£„Åô„ÇãÁîªÂÉè„Åß„Åô„ÄÇ„Çà„Å≠‚ùóÔ∏è#vegan „ÅÆÊ∞óÊåÅ„Å°„ÅØËâØ„ÅèÁêÜËß£„Åß...   ja  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"),\n",
    "                    F.col(\"lang\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF‚Äôs are used to extend the functions of the framework and re-use these functions on multiple DataFrame‚Äôs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "# define the function\n",
    "\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "\n",
    "#df.printSchema()\n",
    "#df.count() #1340938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|    post_created_at|emoji_count|      text_tokenized|num_words|num_hashtags|num_mentions|num_exclamation_marks|\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...| und|2022-05-14 00:55:33|          0|[!\\n#vegan, #GoVe...|        4|           3|           0|                    1|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|  en|2022-01-15 07:17:18|          0|[!, We, will, be,...|       35|          21|           0|                    3|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|  en|2022-09-07 09:11:40|          0|[!!, Daily, Updat...|       31|           9|           2|                    2|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|  de|2022-07-18 10:31:26|          0|[!B, Mal, gute, N...|       30|           2|           0|                    1|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|  en|2022-02-06 13:48:33|          0|[!love, !iq, !wad...|       32|          14|           2|                    5|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|  en|2022-09-03 04:23:35|          0|[\", DID, YOU, KNO...|       18|          13|           0|                    0|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|  en|2022-07-18 20:50:34|          0|[\", Nature's, Int...|       30|          21|           2|                    0|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|  en|2022-08-02 16:30:04|          0|[\", Scarlet, Puni...|       32|          21|           0|                    0|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|  de|2022-08-04 14:36:52|          0|[\", Vegan, meinet...|       22|           1|           0|                    1|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|  in|2022-08-01 15:44:25|          2|[\", nuna, \"\\n\", T...|       20|           0|           0|                    0|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...| und|2022-09-11 16:31:24|          0|[\",  ú·¥Ä·¥ç,  ô, ·¥¢…™…¥·¥Ö…¢...|       13|           1|           0|                    0|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|  en|2022-02-06 08:36:00|          0|[\"#Dairy, milk, n...|       36|          15|           0|                    0|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|  en|2022-02-04 01:04:56|          0|[\"#NewYorkCity, s...|       40|           6|           0|                    0|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|  en|2022-05-30 18:08:58|          0|[\"#Vegan, food, i...|        5|           1|           0|                    0|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|  en|2022-07-02 16:34:40|          0|[\"#Vegan, food‚Äôs,...|       37|           2|           0|                    0|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|  en|2022-03-07 20:01:00|          0|[\"'My, bad, chole...|       22|           0|           1|                    0|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|  en|2022-06-29 20:00:17|          0|[\"'Vegan, Festiva...|       25|           1|           0|                    0|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|  en|2022-02-08 04:51:35|          0|[\"(1), Don't, be,...|       32|           7|           0|                    1|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|  en|2021-11-01 20:44:18|          1|[\"...or, vegan, o...|        5|           0|           0|                    0|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|  en|2022-03-13 00:23:54|          0|[\".@Vita_Russia, ...|       37|           3|           1|                    0|\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df = df.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words consisting out of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "\n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4ab2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate_mul_en download started this may take some time.\n",
      "Approx size to download 279,5 MB\n",
      "[ | ]translate_mul_en download started this may take some time.\n",
      "Approximate size to download 279,5 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[ ‚Äî ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 15:25:01.230748: W external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"translate_mul_en\", lang = \"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_not_eng = twitter_df.where(twitter_df.lang != 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, screen_name: string, created_at: string, full_text: string, lang: string, post_created_at: timestamp, text: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_not_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_trans = pipeline.transform(twitter_not_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_trans = twitter_trans.withColumn(\"text\", concat_ws(\" \",twitter_trans.translation.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_trans = twitter_trans.drop(\"translation\", \"document\",\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 15:26:20 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 37 (TID 2198): Attempting to kill Python Worker\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|    post_created_at|                text|\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...| und|2022-05-14 00:55:33|        little mouse|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|  de|2022-07-18 10:31:26|Thank you for all...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|  de|2022-08-04 14:36:52|but why should th...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|  in|2022-08-01 15:44:25|Thank you for you...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...| und|2022-09-11 16:31:24|. . . . . . . . ....|\n",
      "| Marilyne Chenuet üêæ|MarilyneChenuet|Wed Aug 03 17:19:...|\"5 raisons d'√™tre...|  fr|2022-08-03 17:19:26|reasons to drive ...|\n",
      "|           pn_konyha|      pn_konyha|Wed May 18 03:56:...|\"8 √©ve veg√°n, csa...|  hu|2022-05-18 03:56:06|I'm sorry I didn'...|\n",
      "|üá∫üá¶üå±Krone der E...|    Tofuwa_Bohu|Tue May 03 12:01:...|\"Aber dann k√∂nnen...|  de|2022-05-03 12:01:19|but then we can e...|\n",
      "|                Matt|        untipoo|Mon Nov 01 22:59:...|\"Allora la casa o...|  it|2021-11-01 22:59:37|then the house of...|\n",
      "|         FemmeFatale|    LadyFemme86|Tue May 17 20:32:...|\"Blut an den H√§nd...|  de|2022-05-17 20:32:02|blood on the hand...|\n",
      "|             Hans ü¶ä| LeRenardTaquin|Tue Apr 26 09:07:...|\"Coiffeur vegan\",...|  fr|2022-04-26 09:07:35|Coiffer vegan con...|\n",
      "|  Sofia Augustine L.|    VERSOIRECTO|Sat Oct 30 00:58:...|\"Dari Alex,\" Sofi...|  in|2021-10-30 00:58:58|From the Alex Sof...|\n",
      "|Alex Hauri (er/he...|     AlexHauri1|Tue Mar 15 11:21:...|\"Das echte Kriegs...|  de|2022-03-15 11:21:00|the real war menu...|\n",
      "|               DRIFT|       drifteur|Wed Aug 17 10:06:...|\"De wereld produc...|  nl|2022-08-17 10:06:49|the world is alre...|\n",
      "|        Alice Paulüå±|      MoinAlice|Sat Jan 08 16:17:...|\"Der Veganuary fi...|  de|2022-01-08 16:17:18|the veganuary is ...|\n",
      "|         ZEIT ONLINE|     zeitonline|Tue Nov 02 04:46:...|\"Die @DB_Bahn k√∂n...|  de|2021-11-02 04:46:02|the db path could...|\n",
      "|      Jens Tekhaus ‚ìã|       jtinline|Sun May 01 15:58:...|\"Die Frage ist ni...|  de|2022-05-01 15:58:10|the question is n...|\n",
      "|Denise Nadine Sch√ºrg|     DeniseNSch|Fri Dec 10 22:06:...|\"Die mutter hat v...|  de|2021-12-10 22:06:49|the mother bought...|\n",
      "|Proact.Translatio...|   JouwVertaler|Fri Oct 29 11:58:...|\"Drink je dan ook...|  nl|2021-10-29 11:58:09|Then you don't dr...|\n",
      "|     TiltedFloste üå±|   TiltedFloste|Tue May 17 06:04:...|\"Du bist schon 10...|  de|2022-05-17 06:04:03|You've been vegan...|\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_trans.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df.union(twitter_trans)\n",
    "twitter_df = twitter_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googletrans import Translator\n",
    "\n",
    "# translator = Translator() \n",
    "# translation = translator.translate(\"test\", dest='en')\n",
    "\n",
    "# def translate(tweet):\n",
    "#     if len(tweet)!=0:\n",
    "#         translator = Translator() \n",
    "#         translation = translator.translate(tweet, dest = 'en')\n",
    "#         return(translation.text)\n",
    "#     return None\n",
    "\n",
    "# translate_udf = F.udf(translate, StringType())\n",
    "\n",
    "# twitter_df = twitter_df.withColumn(\"translation\", translate_udf(\"text\"))\n",
    "\n",
    "# twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|lang|    post_created_at|                text|\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+--------------------+\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|  in|2022-08-01 15:44:25|nuna thanks for y...|\n",
      "|           pn_konyha|      pn_konyha|Wed May 18 03:56:...|\"8 √©ve veg√°n, csa...|  hu|2022-05-18 03:56:06|√©ve veg√°n csak sz...|\n",
      "|            The Beet|thebeetofficial|Tue Nov 02 18:33:...|\"A lot of people ...|  en|2021-11-02 18:33:05|lot of people say...|\n",
      "|    ISA Study Abroad|      ISAabroad|Thu Mar 10 14:30:...|\"By now, your [ve...|  en|2022-03-10 14:30:04|by now your vegan...|\n",
      "|                 kev|     bitkevcoin|Tue Apr 26 18:00:...|\"Don't stay in th...|  en|2022-04-26 18:00:01|dont stay in the ...|\n",
      "|     Rob Whitehall ‚ìã|   robdoubleyoo|Tue Jun 21 11:34:...|\"Heroes sacrifice...|  en|2022-06-21 11:34:53|heroes sacrifice ...|\n",
      "|  FAB #Veggie #Vegan| fabveggievegan|Sat Apr 30 18:58:...|\"Last year over a...|  en|2022-04-30 18:58:29|last year over mi...|\n",
      "|KaLena Bowers üìñ?...|   BowersKaLena|Mon Dec 13 18:22:...|\"Monday Mint Moti...|  en|2021-12-13 18:22:14|monday mint motiv...|\n",
      "|      FM La Patriada|   FMLaPatriada|Sat Dec 11 14:42:...|\"No nacimos vegan...|  es|2021-12-11 14:42:09|no nacimos vegana...|\n",
      "|Haruka - Former C...|    HarukaChamp|Tue Nov 02 20:54:...|\"No nut... Novemb...|  en|2021-11-02 20:54:27|no nut november m...|\n",
      "|GhostWipe - Premi...|    _GhostWipe_|Fri Apr 29 18:40:...|\"Sir, why did you...|  en|2022-04-29 18:40:00|sir why did you s...|\n",
      "|   Quadram Institute|     TheQuadram|Tue Jan 11 20:05:...|\"There are many g...|  en|2022-01-11 20:05:01|there are many go...|\n",
      "|           MetroMedi|     MetroMedi1|Wed Apr 27 07:09:...|\"Vegan Diets\" pro...|  en|2022-04-27 07:09:44|vegan diets provi...|\n",
      "|     Jamie Woodhouse| JamieWoodhouse|Sun Jul 31 16:54:...|\"We were given a ...|  en|2022-07-31 16:54:40|we were given veg...|\n",
      "|     Rob Whitehall ‚ìã|   robdoubleyoo|Wed Aug 10 16:53:...|\"Why are vegans s...|  en|2022-08-10 16:53:36|why are vegans so...|\n",
      "|              Riley!| PrivateMeRiley|Tue Aug 02 17:00:...|\"Would you ever g...|  en|2022-08-02 17:00:38|would you ever go...|\n",
      "|                 Jay|   Chxinsawenby|Mon Nov 01 21:57:...|\"You doing anythi...|  en|2021-11-01 21:57:48|you doing anythin...|\n",
      "|         BND For All|BigNonVeganDave|Tue Mar 08 18:19:...|\"defeating Russia...|  en|2022-03-08 18:19:53|defeating russia ...|\n",
      "|Erlijn van Genuch...|        ErlijnG|Thu Mar 31 12:30:...|#365sustainableDe...|  en|2022-03-31 12:30:00|sustainabledecisi...|\n",
      "|               O God|    OGodIsBlack|Wed Aug 10 01:41:...|#5k #10k #Run #ge...| und|2022-08-10 01:41:35|run getitin stren...|\n",
      "+--------------------+---------------+--------------------+--------------------+----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic Modelling\n",
    "\n",
    "https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb\n",
    "\n",
    "https://www.johnsnowlabs.com/spark-nlp/\n",
    "\n",
    "Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds some natural groups of items (topics) even when we‚Äôre not sure what we‚Äôre looking for.\n",
    "\n",
    "Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives. \n",
    "It can help with the following:\n",
    "- discovering the hidden themes in the collection.\n",
    "- classifying the documents into the discovered themes.\n",
    "- using the classification to organize/summarize/search the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Spark NLP pipeline\n",
    "\n",
    "### 3.1.1 Basic NLP pipeline\n",
    "\n",
    "DocumentAssembler converts data into Spark NLP annotation format that can be used by Spark NLP annotators. Prepares data into a format that is processable by Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "                .setInputCol(\"text\") \\\n",
    "                .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize the data with Tokenizer. Tokenizes raw text in document type columns into TokenizedSentence. Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean out the data and lower it with Normalizer. Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with LemmatizerModel.\n",
    "Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with . The dictionary can be set as a delimited text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ / ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ ‚Äî ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP doesn't provide stop word list, hence, we will use nltk package to download stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with NGramGenerator in Spark NLP. N-grams try to predict which word will have the highest probability of appearing with the other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP. We can access the model with PerceptronModel. Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "[ \\ ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "Download done! Loading the resource.\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import PerceptronModel\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'lemmatized']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with Finisher. Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to input everything into a pipeline. Pipeline functionality is accessible with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 ngrammer,\n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pipeline.fit(twitter_df).transform(twitter_df)\n",
    "processed_review = processed_review.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, screen_name: string, created_at: string, lang: string, post_created_at: timestamp, finished_unigrams: array<string>, finished_ngrams: array<string>, finished_pos: array<string>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.drop(\"full_text\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.             (200 + 8) / 400]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_14515/1681446882.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_review.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Extended NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have our data in a form of unigrams that are lemmatized, with no stop words in there. I think it is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let's filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let's restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don't use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
    "\n",
    "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing.\n",
    "\n",
    "Let's start this processing. First, we need to join all our POS tags obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize our POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate n-grams from them in the same way we did that for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are ready to get POS tags ngrams with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create this new Spark NLP pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and again fit it and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what kind of data we have to operate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'screen_name',\n",
       " 'created_at',\n",
       " 'full_text',\n",
       " 'lang',\n",
       " 'post_created_at',\n",
       " 'text',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review = processed_review.cache()\n",
    "processed_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are our word n-grams with their corresponding pos n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:============>                                             (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 14:08:30 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 90 (TID 2727): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (3 + 6) / 9]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_12047/2448479704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished_ngrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'finished_pos_ngrams'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe. WHAT ARE THE POS TAGSSSS\n",
    "\n",
    "- NN is singular noun\n",
    "- NNS is plural noun\n",
    "- VB is verb\n",
    "- VBP verb, present tense not 3rd person singular(wrap)\n",
    "- JJ is an adjective (large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
    "                                                              F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how our filtered unigrams look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:38:00 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 81 (TID 2626): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:38:02 WARN PythonUDFRunner: Detected deadlock while completing task 8.0 in stage 81 (TID 2627): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|                 [sweet, potato, amp, courgette, fritter, yoghurt, dip, vegan, plantbased]|\n",
      "|                           [creamy, mushroom, bucatini, vegan, veganforlife, forevervegan]|\n",
      "|[polypieter, veggie, al, super, vegan, quasi, onmogelijk, en, je, krijgt, onvermijdelij...|\n",
      "|                             [enrich, origin, ingredient, certify, vegan, wink, fragrance]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the function on word and POS n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
    "                                                                    F.col('finished_pos_ngrams'))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what we get after filtering for n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:38:48 WARN PythonUDFRunner: Detected deadlock while completing task 6.0 in stage 98 (TID 2787): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                           filtered_ngrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[relax_conversation, book_dontmesswithasilverback, dontmesswithasilverback_govegan, gov...|\n",
      "|[sweet_potato, potato_amp, amp_courgette, courgette_fritter, mint_yoghurt, yoghurt_dip,...|\n",
      "|[creamy_mushroom, mushroom_bucatini, bucatini_vegan, vegan_veganforlife, veganforlife_f...|\n",
      "|[polypieter_veggie, be_al, al_super, super_vegan, be_quasi, quasi_onmogelijk, onmogelij...|\n",
      "|[natural_origin, origin_ingredient, vegan_society, love_wink, wink_fragrance, fragrance...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(F.col('filtered_unigrams'), \n",
    "                                                      F.col('filtered_ngrams'))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our final look of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:=========================>                               (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:39:26 WARN PythonUDFRunner: Detected deadlock while completing task 3.0 in stage 115 (TID 2946): Attempting to kill Python Worker\n",
      "22/12/03 09:39:26 WARN PythonUDFRunner: Detected deadlock while completing task 5.0 in stage 115 (TID 2948): Attempting to kill Python Worker\n",
      "22/12/03 09:39:26 WARN PythonUDFRunner: Detected deadlock while completing task 2.0 in stage 115 (TID 2945): Attempting to kill Python Worker\n",
      "22/12/03 09:39:26 WARN PythonUDFRunner: Detected deadlock while completing task 4.0 in stage 115 (TID 2947): Attempting to kill Python Worker\n",
      "22/12/03 09:39:26 WARN PythonUDFRunner: Detected deadlock while completing task 7.0 in stage 115 (TID 2950): Attempting to kill Python Worker\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                     final|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|[sweet, potato, amp, courgette, fritter, yoghurt, dip, vegan, plantbased, sweet_potato,...|\n",
      "|[creamy, mushroom, bucatini, vegan, veganforlife, forevervegan, creamy_mushroom, mushro...|\n",
      "|[polypieter, veggie, al, super, vegan, quasi, onmogelijk, en, je, krijgt, onvermijdelij...|\n",
      "|[enrich, origin, ingredient, certify, vegan, wink, fragrance, natural_origin, origin_in...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('final').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set to vectorization of our data. First, we will proceed with TF (term frequency) vectorization with CountVectorizer in PySpark. We fit tf dictionary and then transform the data to vectors of counts.\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 15:01:19 WARN DAGScheduler: Broadcasting large task binary with size 1029.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 56:> (0 + 0) / 400]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_12721/1554251797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get TF results, we can account for words that are frequent for all the documents. We can use IDF (inverse document frequency) to lower score of such words.\n",
    "\n",
    "The inverse document frequency is a measure of whether a term is common or rare in a given document corpus. It is obtained by dividing the total number of documents by the number of documents containing the term in the corpus.\n",
    "\n",
    "While computing TF, all terms are considered equally important. However it is known that certain terms, such as ‚Äúis‚Äù, ‚Äúof‚Äù, and ‚Äúthat‚Äù, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as ‚Äúis‚Äù is present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n",
    "\n",
    "Now there are few other problems with the IDF , in case of a large corpus,say 100,000,000 , the IDF value explodes , to avoid the effect we take the log of idf .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Ftf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558&psig=AOvVaw2NphvHAPexM_UP_4UAcgLP&ust=1670086684586000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCJjOxO6z2_sCFQAAAAAdAAAAABAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:47:57 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:52:59 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2\n",
    "\n",
    "Finally, we are ready to model topics in our data with LDA (Latent Dirichlet Allocation). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 159:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 09:54:15 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|[NN, NN, NN, NN_N...|[vegan, govegan, ...|[vegan_govegan, g...|[vegan, govegan, ...|(262144,[0,22,160...|(262144,[0,22,160...|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|[PRP, MD, VB, JJ,...|[today, tomorrow,...|[be_open, open_am...|[today, tomorrow,...|(262144,[0,2,11,1...|(262144,[0,2,11,1...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|[JJ, JJ, NN, IN, ...|[daily, update, t...|[daily_update, up...|[daily, update, t...|(262144,[2,21,39,...|(262144,[2,21,39,...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|[mal, gute, nachr...|[mal, gute, nachr...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[mal, gute, nachr...|[mal_gute, gute_n...|[mal, gute, nachr...|(262144,[25,54,90...|(262144,[25,54,90...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|[NN, NN, NN, VB, ...|[love, iq, waddup...|[love_iq, iq_wadd...|[love, iq, waddup...|(262144,[0,19,72,...|(262144,[0,19,72,...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|[know, senoritaco...|[do, you, know, s...|[VBP, PRP, VBP, N...|[VBP, PRP, VBP, N...|[know, senoritaco...|[know_senoritacos...|[know, senoritaco...|(262144,[0,22,38,...|(262144,[0,22,38,...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|[nature, intercon...|[nature, intercon...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[nature, intercon...|[nature_interconn...|[nature, intercon...|(262144,[0,11,29,...|(262144,[0,11,29,...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|[scarlet, punica,...|[scarlet, punica,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[scarlet, punica,...|[scarlet_punica, ...|[scarlet, punica,...|(262144,[0,20,162...|(262144,[0,20,162...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|[vegan, meinetweg...|[vegan, meinetweg...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[vegan, meinetweg...|[vegan_meinetwege...|[vegan, meinetweg...|(262144,[0,25,54,...|(262144,[0,25,54,...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|[nuna, thank, hel...|[nuna, thank, for...|[NN, VB, IN, PRP,...|[NN, VB, IN, PRP,...|[nuna, thank, say...|[aduh_saya, saya_...|[nuna, thank, say...|(262144,[0,106,13...|(262144,[0,106,13...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç_·¥¢…™…¥·¥Ö…¢…™, ·¥¢…™…¥·¥Ö...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|(262144,[0,19,101...|(262144,[0,19,101...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|dairy milk no lon...|[dairy, milk, lon...|[dairy, milk, no,...|[JJ, NN, DT, JJ, ...|[JJ, NN, DT, JJ, ...|[dairy, milk, ser...|[dairy_milk, long...|[dairy, milk, ser...|(262144,[12,22,49...|(262144,[12,22,49...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|newyorkcity schoo...|[newyorkcity, sch...|[newyorkcity, sch...|[NN, NN, MD, VB, ...|[NN, NN, MD, VB, ...|[newyorkcity, sch...|[newyorkcity_scho...|[newyorkcity, sch...|(262144,[3,17,22,...|(262144,[3,17,22,...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|vegan food is dis...|[vegan, food, dis...|[vegan, food, be,...|   [NN, NN, VB, VBG]|[NN, NN, VB, VBG,...|[vegan, food, dis...|        [vegan_food]|[vegan, food, dis...|(262144,[0,2,39,3...|(262144,[0,2,39,3...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|vegan food‚Äôs abil...|[vegan, food, abi...|[vegan, food, abi...|[NN, NN, NN, TO, ...|[NN, NN, NN, TO, ...|[vegan, food, abi...|[vegan_food, food...|[vegan, food, abi...|(262144,[0,2,39,5...|(262144,[0,2,39,5...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|my bad cholestero...|[bad, cholesterol...|[i, bad, choleste...|[NNP, JJ, NN, NN,...|[NNP, JJ, NN, NN,...|[cholesterol, num...|[bad_cholesterol,...|[cholesterol, num...|(262144,[65,624,9...|(262144,[65,624,9...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|vegan festival ho...|[vegan, festival,...|[vegan, festival,...|[NN, NN, JJ, NN, ...|[NN, NN, JJ, NN, ...|[vegan, festival,...|[vegan_festival, ...|[vegan, festival,...|(262144,[0,3,329,...|(262144,[0,3,329,...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|dont be too hard ...|[dont, hard, does...|[dont, be, too, h...|[NN, VB, RB, JJ, ...|[NN, VB, RB, JJ, ...|[dont, hard, scar...|[be_scary, good_q...|[dont, hard, scar...|(262144,[28,29,30...|(262144,[28,29,30...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|or vegan ones bra...|[vegan, one, brah...|[or, vegan, one, ...|[CC, NN, CD, NN, NN]|[CC, NN, CD, NN, ...|        [one, skull]|        [brah_skull]|[one, skull, brah...|(262144,[34,1203]...|(262144,[34,1203]...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|2022-03-13 00:23:54|vita russia anima...|[vita, russia, an...|[vita, russia, an...|[NN, NNP, NN, NN,...|[NN, NNP, NN, NN,...|[vita, animal, ri...|[animal_right, ri...|[vita, animal, ri...|(262144,[12,44,11...|(262144,[12,44,11...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tfidf_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 345:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:17:30 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:08 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:08 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:09 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 361:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:13 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:13 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:14 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:15 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:15 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:16 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 379:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:17 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:17 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:18 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 388:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:19 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:19 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:20 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:20 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:20 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:21 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:21 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:22 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:22 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:22 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:23 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:23 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 424:============>                                            (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:25 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 433:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:25 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:26 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/12/03 10:24:26 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 442:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:24:27 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 3\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features').setTopicDistributionCol(\"topicDistributionCol\")\n",
    "lda_model = lda.fit(tfidf_result)\n",
    "transformed = lda_model.transform(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:02:11 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 289:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|    post_created_at|                text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|         tf_features|     tf_idf_features|topicDistributionCol|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|2022-05-14 00:55:33| vegan govegan dairy|[vegan, govegan, ...|[vegan, govegan, ...|        [NN, NN, NN]|[NN, NN, NN, NN_N...|[vegan, govegan, ...|[vegan_govegan, g...|[vegan, govegan, ...|(262144,[0,22,160...|(262144,[0,22,160...|[0.98410772339944...|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|2022-01-15 07:17:18|we will be open a...|[open, ampm, toda...|[we, will, be, op...|[PRP, MD, VB, JJ,...|[PRP, MD, VB, JJ,...|[today, tomorrow,...|[be_open, open_am...|[today, tomorrow,...|(262144,[0,2,11,1...|(262144,[0,2,11,1...|[0.47587353547659...|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|2022-09-07 09:11:40|daily updates tas...|[daily, update, t...|[daily, update, t...|[JJ, JJ, NN, IN, ...|[JJ, JJ, NN, IN, ...|[daily, update, t...|[daily_update, up...|[daily, update, t...|(262144,[2,21,39,...|(262144,[2,21,39,...|[0.69292878539306...|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|2022-07-18 10:31:26|mal gute nachrich...|[mal, gute, nachr...|[mal, gute, nachr...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[mal, gute, nachr...|[mal_gute, gute_n...|[mal, gute, nachr...|(262144,[25,54,90...|(262144,[25,54,90...|[0.99831835310588...|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|2022-02-06 13:48:33|love iq waddup bu...|[love, iq, waddup...|[love, iq, waddup...|[NN, NN, NN, VB, ...|[NN, NN, NN, VB, ...|[love, iq, waddup...|[love_iq, iq_wadd...|[love, iq, waddup...|(262144,[0,19,72,...|(262144,[0,19,72,...|[0.79211895717557...|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|2022-09-03 04:23:35|did you know seno...|[know, senoritaco...|[do, you, know, s...|[VBP, PRP, VBP, N...|[VBP, PRP, VBP, N...|[know, senoritaco...|[know_senoritacos...|[know, senoritaco...|(262144,[0,22,38,...|(262144,[0,22,38,...|[0.80153216423167...|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|2022-07-18 20:50:34|natures interconn...|[nature, intercon...|[nature, intercon...|[NN, NN, NN, IN, ...|[NN, NN, NN, IN, ...|[nature, intercon...|[nature_interconn...|[nature, intercon...|(262144,[0,11,29,...|(262144,[0,11,29,...|[0.99700842419109...|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|2022-08-02 16:30:04|scarlet punica gr...|[scarlet, punica,...|[scarlet, punica,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[scarlet, punica,...|[scarlet_punica, ...|[scarlet, punica,...|(262144,[0,20,162...|(262144,[0,20,162...|[8.06049224515436...|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|2022-08-04 14:36:52|vegan meinetwegen...|[vegan, meinetweg...|[vegan, meinetweg...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[vegan, meinetweg...|[vegan_meinetwege...|[vegan, meinetweg...|(262144,[0,25,54,...|(262144,[0,25,54,...|[0.99779252254811...|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|2022-08-01 15:44:25|nuna thanks for y...|[nuna, thank, hel...|[nuna, thank, for...|[NN, VB, IN, PRP,...|[NN, VB, IN, PRP,...|[nuna, thank, say...|[aduh_saya, saya_...|[nuna, thank, say...|(262144,[0,106,13...|(262144,[0,106,13...|[0.99601186047938...|\n",
      "|    sukhkarm dhillon|SukhkarmDhillon|Sun Sep 11 16:31:...|\"  ú·¥Ä·¥ç  ô ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™...|2022-09-11 16:31:24| ú·¥Ä·¥ç ·¥¢…™…¥·¥Ö…¢…™ ·¥ã…™ ·¥õ·¥Ä Ä...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|[ ú·¥Ä·¥ç_·¥¢…™…¥·¥Ö…¢…™, ·¥¢…™…¥·¥Ö...|[ ú·¥Ä·¥ç, ·¥¢…™…¥·¥Ö…¢…™, ·¥ã…™,...|(262144,[0,19,101...|(262144,[0,19,101...|[0.99826478675428...|\n",
      "|            VeganRoo|       VeganRoo|Sun Feb 06 08:36:...|\"#Dairy milk no l...|2022-02-06 08:36:00|dairy milk no lon...|[dairy, milk, lon...|[dairy, milk, no,...|[JJ, NN, DT, JJ, ...|[JJ, NN, DT, JJ, ...|[dairy, milk, ser...|[dairy_milk, long...|[dairy, milk, ser...|(262144,[12,22,49...|(262144,[12,22,49...|[0.00174909179188...|\n",
      "|        Pauline Park|    paulinepark|Fri Feb 04 01:04:...|\"#NewYorkCity sch...|2022-02-04 01:04:56|newyorkcity schoo...|[newyorkcity, sch...|[newyorkcity, sch...|[NN, NN, MD, VB, ...|[NN, NN, MD, VB, ...|[newyorkcity, sch...|[newyorkcity_scho...|[newyorkcity, sch...|(262144,[3,17,22,...|(262144,[3,17,22,...|[0.21282598741116...|\n",
      "|  üìñThat Wordy Oneüìö|  Herofthewords|Mon May 30 18:08:...|\"#Vegan food is d...|2022-05-30 18:08:58|vegan food is dis...|[vegan, food, dis...|[vegan, food, be,...|   [NN, NN, VB, VBG]|[NN, NN, VB, VBG,...|[vegan, food, dis...|        [vegan_food]|[vegan, food, dis...|(262144,[0,2,39,3...|(262144,[0,2,39,3...|[0.02857722762531...|\n",
      "|               NiCHE|   NiCHE_Canada|Sat Jul 02 16:34:...|\"#Vegan food‚Äôs ab...|2022-07-02 16:34:40|vegan food‚Äôs abil...|[vegan, food, abi...|[vegan, food, abi...|[NN, NN, NN, TO, ...|[NN, NN, NN, TO, ...|[vegan, food, abi...|[vegan_food, food...|[vegan, food, abi...|(262144,[0,2,39,5...|(262144,[0,2,39,5...|[0.00560045401333...|\n",
      "|   Mercy For Animals|MercyForAnimals|Mon Mar 07 20:01:...|\"'My bad choleste...|2022-03-07 20:01:00|my bad cholestero...|[bad, cholesterol...|[i, bad, choleste...|[NNP, JJ, NN, NN,...|[NNP, JJ, NN, NN,...|[cholesterol, num...|[bad_cholesterol,...|[cholesterol, num...|(262144,[65,624,9...|(262144,[65,624,9...|[0.00424598520677...|\n",
      "|What's On South W...|   tixSouthWest|Wed Jun 29 20:00:...|\"'Vegan Festival'...|2022-06-29 20:00:17|vegan festival ho...|[vegan, festival,...|[vegan, festival,...|[NN, NN, JJ, NN, ...|[NN, NN, JJ, NN, ...|[vegan, festival,...|[vegan_festival, ...|[vegan, festival,...|(262144,[0,3,329,...|(262144,[0,3,329,...|[0.00108954779826...|\n",
      "|        Gayforbillie| MarleyRose0725|Tue Feb 08 04:51:...|\"(1) Don't be too...|2022-02-08 04:51:35|dont be too hard ...|[dont, hard, does...|[dont, be, too, h...|[NN, VB, RB, JJ, ...|[NN, VB, RB, JJ, ...|[dont, hard, scar...|[be_scary, good_q...|[dont, hard, scar...|(262144,[28,29,30...|(262144,[28,29,30...|[0.00600870751384...|\n",
      "|ùôèùôùùôöùòøùôöùôóùô§ùô£?...|       thee_sxs|Mon Nov 01 20:44:...|\"...or vegan ones...|2021-11-01 20:44:18|or vegan ones bra...|[vegan, one, brah...|[or, vegan, one, ...|[CC, NN, CD, NN, NN]|[CC, NN, CD, NN, ...|        [one, skull]|        [brah_skull]|[one, skull, brah...|(262144,[34,1203]...|(262144,[34,1203]...|[0.04218614871231...|\n",
      "|             Krasnov|   greennomad61|Sun Mar 13 00:23:...|\".@Vita_Russia An...|2022-03-13 00:23:54|vita russia anima...|[vita, russia, an...|[vita, russia, an...|[NN, NNP, NN, NN,...|[NN, NNP, NN, NN,...|[vita, animal, ri...|[animal_right, ri...|[vita, animal, ri...|(262144,[12,44,11...|(262144,[12,44,11...|[0.09810146192606...|\n",
      "+--------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(input_list):\n",
    "    max = input_list[0]\n",
    "    index = 0\n",
    "    for i in range(1,len(input_list)):\n",
    "        if input_list[i] > max:\n",
    "            max = input_list[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "get_topic_udf = udf(lambda z: get_topic(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn('topic', get_topic_udf(\"topicDistributionCol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 451:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:25:01 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 455:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:36:07 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 460:=========================>                               (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:36:09 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:36:09 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "22/12/03 10:36:10 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n"
     ]
    }
   ],
   "source": [
    "freq_month = transformed.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "freq_month = freq_month.withColumn(\"month\", month(df[\"post_created_at\"]))\n",
    "\n",
    "freq_month = freq_month.groupBy('year', 'month', 'topic').agg(countDistinct(\"full_text\"))\\\n",
    "               .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                    .sort('year', 'month', ascending = True)\n",
    "freq_month = freq_month.select(concat_ws('_',freq_month.year, freq_month.month)\\\n",
    "                            .alias('date'), 'topic', 'freq').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>10736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>15727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>0</td>\n",
       "      <td>7744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>1</td>\n",
       "      <td>16664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>2</td>\n",
       "      <td>11935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>0</td>\n",
       "      <td>8010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>2</td>\n",
       "      <td>18196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>0</td>\n",
       "      <td>20318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>1</td>\n",
       "      <td>32707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>1</td>\n",
       "      <td>14449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>2</td>\n",
       "      <td>7136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>1</td>\n",
       "      <td>10908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>2</td>\n",
       "      <td>6582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>2</td>\n",
       "      <td>24232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>1</td>\n",
       "      <td>39916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>0</td>\n",
       "      <td>24096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>0</td>\n",
       "      <td>13320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>2</td>\n",
       "      <td>13522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>1</td>\n",
       "      <td>22258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>2</td>\n",
       "      <td>10058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>0</td>\n",
       "      <td>10668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>1</td>\n",
       "      <td>14086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>0</td>\n",
       "      <td>13119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>2</td>\n",
       "      <td>13753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>1</td>\n",
       "      <td>19642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>2</td>\n",
       "      <td>22797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>1</td>\n",
       "      <td>31378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>0</td>\n",
       "      <td>20891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>2</td>\n",
       "      <td>21494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>0</td>\n",
       "      <td>20743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>1</td>\n",
       "      <td>32232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>2</td>\n",
       "      <td>5579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>1</td>\n",
       "      <td>6633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>0</td>\n",
       "      <td>5789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic   freq\n",
       "0   2021_10      2  10736\n",
       "1   2021_10      1  15727\n",
       "2   2021_10      0   7744\n",
       "3   2021_11      1  16664\n",
       "4   2021_11      2  11935\n",
       "5   2021_11      0   8010\n",
       "6   2021_12      2  18196\n",
       "7   2021_12      0  20318\n",
       "8   2021_12      1  32707\n",
       "9    2022_1      1  14449\n",
       "10   2022_1      0  11368\n",
       "11   2022_1      2   7136\n",
       "12   2022_2      1  10908\n",
       "13   2022_2      0   8243\n",
       "14   2022_2      2   6582\n",
       "15   2022_3      2  24232\n",
       "16   2022_3      1  39916\n",
       "17   2022_3      0  24096\n",
       "18   2022_4      0  13320\n",
       "19   2022_4      2  13522\n",
       "20   2022_4      1  22258\n",
       "21   2022_5      2  10058\n",
       "22   2022_5      0  10668\n",
       "23   2022_5      1  14086\n",
       "24   2022_6      0  13119\n",
       "25   2022_6      2  13753\n",
       "26   2022_6      1  19642\n",
       "27   2022_7      2  22797\n",
       "28   2022_7      1  31378\n",
       "29   2022_7      0  20891\n",
       "30   2022_8      2  21494\n",
       "31   2022_8      0  20743\n",
       "32   2022_8      1  32232\n",
       "33   2022_9      2   5579\n",
       "34   2022_9      1   6633\n",
       "35   2022_9      0   5789\n",
       "36  2022_10      2   1102\n",
       "37  2022_10      0   1167\n",
       "38  2022_10      1   1549"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "hoverongaps": true,
         "type": "heatmap",
         "x": [
          "2021_10",
          "2021_10",
          "2021_10",
          "2021_11",
          "2021_11",
          "2021_11",
          "2021_12",
          "2021_12",
          "2021_12",
          "2022_1",
          "2022_1",
          "2022_1",
          "2022_2",
          "2022_2",
          "2022_2",
          "2022_3",
          "2022_3",
          "2022_3",
          "2022_4",
          "2022_4",
          "2022_4",
          "2022_5",
          "2022_5",
          "2022_5",
          "2022_6",
          "2022_6",
          "2022_6",
          "2022_7",
          "2022_7",
          "2022_7",
          "2022_8",
          "2022_8",
          "2022_8",
          "2022_9",
          "2022_9",
          "2022_9",
          "2022_10",
          "2022_10",
          "2022_10"
         ],
         "y": [
          2,
          1,
          0,
          1,
          2,
          0,
          2,
          0,
          1,
          1,
          0,
          2,
          1,
          0,
          2,
          2,
          1,
          0,
          0,
          2,
          1,
          2,
          0,
          1,
          0,
          2,
          1,
          2,
          1,
          0,
          2,
          0,
          1,
          2,
          1,
          0,
          2,
          0,
          1
         ],
         "z": [
          10736,
          15727,
          7744,
          16664,
          11935,
          8010,
          18196,
          20318,
          32707,
          14449,
          11368,
          7136,
          10908,
          8243,
          6582,
          24232,
          39916,
          24096,
          13320,
          13522,
          22258,
          10058,
          10668,
          14086,
          13119,
          13753,
          19642,
          22797,
          31378,
          20891,
          21494,
          20743,
          32232,
          5579,
          6633,
          5789,
          1102,
          1167,
          1549
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   x=freq_month[\"date\"],\n",
    "                   y=freq_month[\"topic\"],\n",
    "                   z=freq_month[\"freq\"],\n",
    "                   hoverongaps = True))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>2</td>\n",
       "      <td>10736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>1</td>\n",
       "      <td>15727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021_10</td>\n",
       "      <td>0</td>\n",
       "      <td>7744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>1</td>\n",
       "      <td>16664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>2</td>\n",
       "      <td>11935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021_11</td>\n",
       "      <td>0</td>\n",
       "      <td>8010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>2</td>\n",
       "      <td>18196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>0</td>\n",
       "      <td>20318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021_12</td>\n",
       "      <td>1</td>\n",
       "      <td>32707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>1</td>\n",
       "      <td>14449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022_1</td>\n",
       "      <td>2</td>\n",
       "      <td>7136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>1</td>\n",
       "      <td>10908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022_2</td>\n",
       "      <td>2</td>\n",
       "      <td>6582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>2</td>\n",
       "      <td>24232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>1</td>\n",
       "      <td>39916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022_3</td>\n",
       "      <td>0</td>\n",
       "      <td>24096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>0</td>\n",
       "      <td>13320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>2</td>\n",
       "      <td>13522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022_4</td>\n",
       "      <td>1</td>\n",
       "      <td>22258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>2</td>\n",
       "      <td>10058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>0</td>\n",
       "      <td>10668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022_5</td>\n",
       "      <td>1</td>\n",
       "      <td>14086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>0</td>\n",
       "      <td>13119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>2</td>\n",
       "      <td>13753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022_6</td>\n",
       "      <td>1</td>\n",
       "      <td>19642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>2</td>\n",
       "      <td>22797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>1</td>\n",
       "      <td>31378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022_7</td>\n",
       "      <td>0</td>\n",
       "      <td>20891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>2</td>\n",
       "      <td>21494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>0</td>\n",
       "      <td>20743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022_8</td>\n",
       "      <td>1</td>\n",
       "      <td>32232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>2</td>\n",
       "      <td>5579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>1</td>\n",
       "      <td>6633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022_9</td>\n",
       "      <td>0</td>\n",
       "      <td>5789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>2</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>0</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022_10</td>\n",
       "      <td>1</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  topic   freq\n",
       "0   2021_10      2  10736\n",
       "1   2021_10      1  15727\n",
       "2   2021_10      0   7744\n",
       "3   2021_11      1  16664\n",
       "4   2021_11      2  11935\n",
       "5   2021_11      0   8010\n",
       "6   2021_12      2  18196\n",
       "7   2021_12      0  20318\n",
       "8   2021_12      1  32707\n",
       "9    2022_1      1  14449\n",
       "10   2022_1      0  11368\n",
       "11   2022_1      2   7136\n",
       "12   2022_2      1  10908\n",
       "13   2022_2      0   8243\n",
       "14   2022_2      2   6582\n",
       "15   2022_3      2  24232\n",
       "16   2022_3      1  39916\n",
       "17   2022_3      0  24096\n",
       "18   2022_4      0  13320\n",
       "19   2022_4      2  13522\n",
       "20   2022_4      1  22258\n",
       "21   2022_5      2  10058\n",
       "22   2022_5      0  10668\n",
       "23   2022_5      1  14086\n",
       "24   2022_6      0  13119\n",
       "25   2022_6      2  13753\n",
       "26   2022_6      1  19642\n",
       "27   2022_7      2  22797\n",
       "28   2022_7      1  31378\n",
       "29   2022_7      0  20891\n",
       "30   2022_8      2  21494\n",
       "31   2022_8      0  20743\n",
       "32   2022_8      1  32232\n",
       "33   2022_9      2   5579\n",
       "34   2022_9      1   6633\n",
       "35   2022_9      0   5789\n",
       "36  2022_10      2   1102\n",
       "37  2022_10      0   1167\n",
       "38  2022_10      1   1549"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 482:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592251, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((transformed.count(), len(transformed.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of top words per topic we would like to see and extract the words with our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------+\n",
      "|topic|                                                                  topicWords|\n",
      "+-----+----------------------------------------------------------------------------+\n",
      "|    0|[recipe, healthy, vegan, heart, food, amp, face, green, organic, vegetarian]|\n",
      "|    1|            [vegan, go, heart, go_vegan, amp, be_vegan, eat, im, make, food]|\n",
      "|    2|               [vegan, und, ich, die, es, ist, das, nicht, be_vegan, animal]|\n",
      "+-----+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "recipe\n",
      "healthy\n",
      "vegan\n",
      "heart\n",
      "food\n",
      "amp\n",
      "face\n",
      "green\n",
      "organic\n",
      "vegetarian\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "vegan\n",
      "go\n",
      "heart\n",
      "go_vegan\n",
      "amp\n",
      "be_vegan\n",
      "eat\n",
      "im\n",
      "make\n",
      "food\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "vegan\n",
      "und\n",
      "ich\n",
      "die\n",
      "es\n",
      "ist\n",
      "das\n",
      "nicht\n",
      "be_vegan\n",
      "animal\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
