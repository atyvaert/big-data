{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c0693c",
   "metadata": {},
   "source": [
    "# Initialize pyspark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "#path = \"/Users/konstantinlazarov/Desktop/Big_Data/PySpark/Week_5/spark\"\n",
    "path = \"/Users/Artur/spark\"\n",
    "findspark.init(path) \n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b1cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 21:12:21 WARN Utils: Your hostname, MacBook-Pro-de-Elena.local resolves to a loopback address: 127.0.0.1; using 192.168.1.22 instead (on interface en0)\n",
      "22/12/08 21:12:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 21:12:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/08 21:12:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a6ad9",
   "metadata": {},
   "source": [
    "# Import necessary packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1fed3",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5968b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pytz\n",
    "import emojis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from pandas.tseries.holiday import nearest_workday, \\\n",
    "    AbstractHolidayCalendar, Holiday, \\\n",
    "    USMartinLutherKingJr, USPresidentsDay, GoodFriday, \\\n",
    "    USMemorialDay, USLaborDay, USThanksgivingDay\n",
    "from datetime import date\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b37bb",
   "metadata": {},
   "source": [
    "#### Import the twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03afc613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1827680"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a25396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46530a1c",
   "metadata": {},
   "source": [
    "# Predict the engagement rate of a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2304e11",
   "metadata": {},
   "source": [
    "## 1. Goal of our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00b73b",
   "metadata": {},
   "source": [
    "In this notebook, we are going to predict the engagement rate of tweets. Further, it will be interesting to see the driving factors behind the engagement rate. This can be valuable information when creating an own social media brand or when you want to increase the reach of your tweets.\n",
    "\n",
    "We start by selecting the interesting variables for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "849d5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the interesting variables\n",
    "basetable_engr = df_json.select(F.col('created_at').alias('tweet_created'), \\\n",
    "                                   F.col('entities.symbols').alias('symbols'), \\\n",
    "                                   F.col('display_text_range').alias('text_range'), \\\n",
    "                                   F.col('extended_entities.media.type').alias('media_type'), \\\n",
    "                                   F.col('favorite_count'), \\\n",
    "                                   F.col('full_text'), \\\n",
    "                                   F.col('is_quote_status').alias('quoted'), \\\n",
    "                                   F.col('lang').alias('language'), \\\n",
    "                                   F.col('retweet_count'),\\\n",
    "                                   F.col('user.created_at').alias('user_created'), \\\n",
    "                                   F.col('user.followers_count').alias('user_followers'), \\\n",
    "                                   F.col('user.friends_count').alias('user_following'), \\\n",
    "                                   F.col('user.verified').alias('user_verified'), \\\n",
    "                                   F.col(\"user.screen_name\"), \\\n",
    "                                   F.col('user.statuses_count').alias('nr_tweets_by_user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68aafc",
   "metadata": {},
   "source": [
    "Next, we perform some basic preprocessing steps on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8e6a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we  convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "basetable_engr = basetable_engr.withColumn('tweet_created', F.to_utc_timestamp(date_udf(\"tweet_created\"), \"UTC\"))\n",
    "basetable_engr = basetable_engr.withColumn('user_created', F.to_utc_timestamp(date_udf(\"user_created\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "772d6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "basetable_engr = basetable_engr.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "basetable_engr = basetable_engr.sort(\"tweet_created\", ascending=False)\n",
    "\n",
    "#removing spam accounts \n",
    "basetable_engr = basetable_engr.drop_duplicates([\"full_text\", \"screen_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1259f",
   "metadata": {},
   "source": [
    "Before we start the feature engineering, we need to filter our data. As we will use the vader package to determine the sentiment later in this notebook, we will only work with English tweets. As our insights in the data will be primarily for European and American companies and most of our tweets are in English, we do not see this as a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45b73aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data on language\n",
    "basetable_engr = basetable_engr.filter(F.col(\"language\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "447587fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4597"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing if we can compile the model for a smaller data set \n",
    "basetable_engr = basetable_engr.sample(0.01) #takiong 10 procent of the data \n",
    "basetable_engr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b26ba",
   "metadata": {},
   "source": [
    "# 2. Create the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca291115",
   "metadata": {},
   "source": [
    "AAN TE PASSEN\n",
    "\n",
    "First, we start by defining our dependent variable. The engagement rate has already been discussed in the data exploration section. We will use the same definition in order to create a model to predict the engagement rate. Below, we repeat this definition:\n",
    "\n",
    "Engagement on Twitter is measured by the number of retweets, follows, replies, favorites, and other people’s reactions to your tweets, including the clicks on the links and hashtags in those tweets. Your Twitter engagement rate is your engagement figure divided by the number of impressions on the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "123ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate engagement rate\n",
    "def engagement_rate(favorite_count, retweet_count, user_followers):\n",
    "    if(user_followers == 0):\n",
    "        eng_rate = 0\n",
    "    elif(user_followers != 0):    \n",
    "        eng_rate = (favorite_count + retweet_count)/user_followers\n",
    "    else:  \n",
    "        eng_rate = 0\n",
    "    \n",
    "    return eng_rate\n",
    "\n",
    "engagement_rate_UDF = F.udf(engagement_rate, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3146953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable_engr = basetable_engr.withColumn(\"eng_rate\", \n",
    "                                           engagement_rate_UDF( \"favorite_count\", \n",
    "                                                               \"retweet_count\", \n",
    "                                                                \"user_followers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "adbdd20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+----------+--------------+--------------------+------+--------+-------------+-------------------+--------------+--------------+-------------+---------------+-----------------+--------+\n",
      "|      tweet_created|symbols|text_range|media_type|favorite_count|           full_text|quoted|language|retweet_count|       user_created|user_followers|user_following|user_verified|    screen_name|nr_tweets_by_user|eng_rate|\n",
      "+-------------------+-------+----------+----------+--------------+--------------------+------+--------+-------------+-------------------+--------------+--------------+-------------+---------------+-----------------+--------+\n",
      "|2021-11-02 05:03:16|     []| [13, 143]|   [photo]|             0|@KevinTMaher Can ...| false|      en|            0|2009-09-03 02:26:51|             0|             0|        false|      smapple11|                9|    null|\n",
      "|2022-04-27 23:42:53|     []| [15, 128]|      null|             0|@MsBlaireWhite A ...| false|      en|            0|2022-04-25 20:19:41|             0|            16|        false| mooseknuckle47|               14|    null|\n",
      "|2022-07-01 02:30:57|     []|  [31, 76]|      null|             0|@absolute_vegan @...| false|      en|            0|2022-06-09 13:00:16|             0|            17|        false|   Boondoggle42|               40|    null|\n",
      "|2022-05-28 12:47:19|     []| [19, 270]|      null|             0|@olseadoo @CTVNew...| false|      en|            0|2022-01-06 17:16:34|             0|             6|        false|    c_urmudgeon|              243|    null|\n",
      "|2022-02-08 07:27:13|     []| [16, 249]|      null|             0|@vegan_cooking3 S...| false|      en|            0|2021-11-09 08:17:37|             0|             2|        false|    trebetheric|                8|    null|\n",
      "|2021-11-01 12:31:01|     []|  [0, 242]|   [photo]|             0|All Round CBD 300...| false|      en|            0|2021-10-18 11:41:21|             0|             1|        false|  AlphaPharmaUk|               45|    null|\n",
      "|2022-06-30 05:09:20|     []|  [0, 257]|   [photo]|             0|Can't Get Diabete...| false|      en|            0|2022-06-08 08:50:11|             0|             0|        false|sugardrmedicare|               17|    null|\n",
      "|2022-07-31 06:47:56|     []|   [0, 53]|      null|             0|I saw some dude t...| false|      en|            0|2022-07-31 06:43:52|             0|             5|        false| doyenatanotori|                3|    null|\n",
      "|2022-07-05 13:41:02|     []|  [0, 217]|      null|             0|This is why vegan...|  true|      en|            0|2022-07-05 12:47:40|             0|            12|        false|      Crishae94|                8|    null|\n",
      "|2022-06-29 17:47:53|     []|  [0, 165]|      null|             0|What's more, seei...| false|      en|            0|2020-07-06 17:28:51|             0|             3|        false|   wordsweutter|              182|    null|\n",
      "|2022-07-03 16:58:28|     []|   [0, 37]|   [photo]|             0|Who said vegan fo...| false|      en|            0|2022-05-14 18:46:18|             0|             0|        false|        slaycow|                5|    null|\n",
      "|2022-03-05 23:04:52|     []| [24, 222]|      null|             0|@paul__tully @vga...| false|      en|            0|2022-01-27 21:44:17|             0|             0|        false|      Imafrog69|                9|    null|\n",
      "|2021-11-03 01:16:20|     []|  [0, 147]|   [photo]|             0|Crispy Vegan Chic...| false|      en|            0|2021-10-24 03:56:05|             0|            25|        false|VRevolution_803|                6|    null|\n",
      "|2022-09-12 06:13:50|     []|  [0, 271]|   [photo]|             0|Use Happy Million...| false|      en|            0|2022-07-27 11:35:16|             0|             5|        false| millions_happy|               30|    null|\n",
      "|2022-07-31 01:17:32|     []|   [0, 70]|      null|             0|Why are Vegan wom...| false|      en|            0|2022-07-31 01:10:19|             0|            12|        false|Diretucatafine1|                3|    null|\n",
      "|2021-11-04 03:10:42|     []| [60, 183]|      null|             0|@GlimmersYT @WetC...| false|      en|            0|2021-10-26 03:57:52|             0|            14|        false|        alt_bop|               18|    null|\n",
      "|2022-04-27 16:47:46|     []|   [0, 58]|      null|             0|You aren't truly ...| false|      en|            0|2022-01-27 04:50:54|             0|            34|        false|JuicyintheskyyJ|               70|    null|\n",
      "|2022-07-01 22:15:40|     []|  [0, 206]|      null|             0|#vegans are sick,...| false|      en|            0|2017-04-29 21:02:49|             0|             3|        false|JamesHa74458806|             1263|    null|\n",
      "|2021-11-01 21:04:48|     []|   [7, 55]|      null|             0|@hmusa yesssss ve...| false|      en|            0|2020-10-14 16:08:22|             0|            15|        false|      ryanh4500|               19|    null|\n",
      "|2021-10-31 10:21:04|     []|  [42, 94]|      null|             0|@oneshotbobby @si...| false|      en|            0|2021-09-23 11:19:32|             0|             2|        false|Paulhaw20721284|              547|    null|\n",
      "+-------------------+-------+----------+----------+--------------+--------------------+------+--------+-------------+-------------------+--------------+--------------+-------------+---------------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basetable_engr.filter(F.col(\"eng_rate\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e9c83",
   "metadata": {},
   "source": [
    "**!! WHICH VALUE OF FOLLOWERS_COUNT HAS TO BE USED??**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4ab37",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88cde2",
   "metadata": {},
   "source": [
    "In order to predict the engagement of a tweet, we will create some additional features in order to improve the performance of our model. For most of these features, we will first creata a function. The following features will be created:\n",
    "\n",
    "    1) number of words\n",
    "    2) number of hashtags\n",
    "    3) number of tags\n",
    "    4) number of emojis\n",
    "    5) get the number of exclamation marks\n",
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "    10) The number of upper case words\n",
    "    11) tweeted quote\n",
    "    12) presence of a symbol\n",
    "    13) The age of the account\n",
    "    14) The number of media elements\n",
    "    15) The media type present\n",
    "    16) The number of text characters in the tweet\n",
    "    17) Indicator if the account is verified\n",
    "\n",
    "These features will be used next to some variables that are already present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675584",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfebde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "671e09af",
   "metadata": {},
   "source": [
    "    1) For the number of words, we can just use the function F.size() and apply it to the tokenized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e58af41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define function to count hashtags\n",
    "def get_hashtags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"#\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "419c458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define function to count tags\n",
    "def get_tags(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter == \"@\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "945d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Define a function to get the number of emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94c3885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Define function to count exclamation marks\n",
    "def get_exclamation_marks(text):\n",
    "    counter = 0\n",
    "    for letter in text:\n",
    "        if letter ==  \"!\":\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609aa6b",
   "metadata": {},
   "source": [
    "    6) the month\n",
    "    7) day of the month\n",
    "    8) day of the week\n",
    "    9) hour of the day\n",
    "\n",
    "We saw how to create these variables when solving the questions for this assignment. The same code will be used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e807c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7e25698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Define number of upper case words\n",
    "def get_upper_case_words(text):\n",
    "    counter = 0\n",
    "    \n",
    "    ## Tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    ## Check for uppercase words\n",
    "    for word in word_tokens:\n",
    "        if word.isupper():\n",
    "            counter += 1\n",
    "    return(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "37602e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Define a function that indicates if the tweet was a quote\n",
    "def tweeted_quote_indicator(quoted):\n",
    "    quote = 0\n",
    "    if quoted == True:\n",
    "        quote = 1\n",
    "    return quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c7987bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) define a function that indicates the presence of a symbol\n",
    "def symbol_indicator(symbols):\n",
    "    symbol = 0\n",
    "    if(symbols > 0):\n",
    "        symbol = 1\n",
    "    return symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea625a6",
   "metadata": {},
   "source": [
    "    13) Define the age of the account. This is defined as the number of days since the account has been created and the last day of scraping (2022-10-11). The last day of scraping was calculated in the exploration phase of the data. For this variable, we will use the function datediff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c208ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) define a function to get help get the number of media types included in the tweet\n",
    "def adjust_nr_media(number):\n",
    "    if number == -1:\n",
    "        number = 0\n",
    "        \n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "650393c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) define a function to get the first media element\n",
    "def get_media_type(media):\n",
    "    if media == None:\n",
    "        media = 'no_media'\n",
    "    else:\n",
    "        media = media[0]\n",
    "       \n",
    "    return media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0210dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) define a function to get the first media element\n",
    "def get_nr_text_characters(text_range):\n",
    "    number = text_range[1] - text_range[0]  \n",
    "    return number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4d61bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Look if the user is a verified user\n",
    "def verified_ind(verified):\n",
    "    indicator = 0\n",
    "    if verified == True:\n",
    "        indicator = 1\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "591c78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the functions as an udf\n",
    "get_upper_case_words_UDF = F.udf(get_upper_case_words, IntegerType()) \n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n",
    "get_hashtags_udf = F.udf(get_hashtags, IntegerType())\n",
    "get_tags_udf = F.udf(get_tags, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "tweeted_quote_indicator_UDF = F.udf(tweeted_quote_indicator, IntegerType())\n",
    "symbol_indicator_udf = F.udf(symbol_indicator, IntegerType())\n",
    "adjust_nr_media_udf = F.udf(adjust_nr_media, IntegerType())\n",
    "get_media_type_udf = F.udf(get_media_type, StringType())\n",
    "get_nr_text_characters_udf = F.udf(get_nr_text_characters, IntegerType())\n",
    "verified_ind_udf = F.udf(verified_ind, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ac2b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already define some text cleaning steps in order to define the correct number of words.\n",
    "\n",
    "# define puncutation and stopwords\n",
    "PUNCTUATION = [char for char in punctuation if char not in [\"!\", \"@\", \"#\"]]\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "# define function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    ## Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in PUNCTUATION])\n",
    "    return(text)\n",
    "\n",
    "# define function to remove stopwords\n",
    "def remove_stops(text_tokenized):\n",
    "    # remove stopwords\n",
    "    text_tokenized = [word for word in text_tokenized if word not in STOPWORDS]\n",
    "    return(text_tokenized)\n",
    "\n",
    "# register as udf\n",
    "remove_punct_UDF = F.udf(remove_punct, StringType())\n",
    "remove_stops_UDF = F.udf(remove_stops, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02854ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr.withColumn(\"num_emojis\", emoji_counter_udf(F.col(\"full_text\")))\\\n",
    "                            .withColumn('upper_case_words', get_upper_case_words_UDF('full_text'))\\\n",
    "                            .withColumn(\"text_lower\", F.lower(\"full_text\")) \\\n",
    "                            .withColumn(\"text_cleaned\", remove_punct_UDF(\"text_lower\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"text_cleaned\", \" \")) \\\n",
    "                            .withColumn(\"text_tokenized_no_stops\", remove_stops_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_tags_udf(\"text_tokenized_no_stops\")) \\\n",
    "                            .withColumn('nr_exlcamations', get_exclamation_marks_UDF('text_tokenized_no_stops'))\\\n",
    "                            .withColumn(\"week_day\", F.date_format(F.col(\"tweet_created\"), \"E\"))\\\n",
    "                            .withColumn(\"hour\", F.date_format(F.col(\"tweet_created\"), \"H\").cast('string'))\\\n",
    "                            .withColumn(\"month\", F.date_format(F.col(\"tweet_created\"), \"M\"))\\\n",
    "                            .withColumn(\"day_month\", F.date_format(F.col(\"tweet_created\"), \"d\"))\\\n",
    "                            .withColumn('quoted_ind', tweeted_quote_indicator_UDF('quoted'))\\\n",
    "                            .withColumn('symbol_ind', F.size('symbols'))\\\n",
    "                            .withColumn('symbol_ind', symbol_indicator_udf('symbol_ind'))\\\n",
    "                            .withColumn('user_age_days', F.datediff(F.lit(\"2022-10-11\"), F.col(\"user_created\")))\\\n",
    "                            .withColumn('verified', verified_ind_udf('user_verified'))\\\n",
    "                            .withColumn(\"nr_media_elements\", F.size(\"media_type\"))\\\n",
    "                            .withColumn(\"nr_media_elements\", adjust_nr_media_udf(\"nr_media_elements\"))\\\n",
    "                            .withColumn(\"media_type\", get_media_type_udf('media_type'))\\\n",
    "                            .withColumn(\"nr_text_char\", get_nr_text_characters_udf('text_range'))\\\n",
    "                            .drop('tweet_created')\\\n",
    "                            .drop('quoted')\\\n",
    "                            .drop('symbols')\\\n",
    "                            .drop('user_created')\\\n",
    "                            .drop('user_verified')\\\n",
    "                            .drop('display_text_range')\\\n",
    "                            .drop('text_lower')\\\n",
    "                            .drop('text_cleaned')\\\n",
    "                            .drop('text_tokenized')\\\n",
    "                            .drop('text_tokenized_no_stops')\\\n",
    "                            .drop('text_range')\\\n",
    "                            .filter(\"num_words > 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f2a7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|media_type|favorite_count|           full_text|language|retweet_count|user_followers|user_following| screen_name|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "|     photo|             7|#BeforeIWasVegan ...|      en|            2|          1074|          2497|OliviaBaboon|            15906|0.008379888268156424|         0|               6|       34|           0|           0|              0|     Mon|   9|   11|        1|         1|         0|         2566|       0|                1|         308|\n",
      "|  no_media|             1|#CocaCola #cocaco...|      en|            0|           463|          1217|    KimPWilt|             1986|0.002159827213822...|         0|               0|       18|           0|           0|              0|     Fri|  18|    4|       29|         1|         0|          943|       0|                0|         224|\n",
      "|  no_media|             2|#Gunpowder &amp; ...|      en|            0|           965|          1789| eobathnbody|            17254|0.002072538860103627|         0|               0|       28|           0|           0|              0|     Mon|   3|    8|       29|         0|         0|         4630|       0|                0|         171|\n",
      "|  no_media|             0|#ICYMI from Folio...|      en|            1|            95|           328|   Folio_YVR|             3338|0.010526315789473684|         0|               1|       21|           0|           0|              0|     Wed|  10|    7|        6|         0|         0|         1107|       0|                0|         217|\n",
      "|  no_media|             0|#ICYMI ☆  NUDESTI...|      en|            0|           158|           728| EcoLux_Life|             4603|                 0.0|         0|               2|       15|           0|           0|              0|     Thu|  18|    6|       30|         0|         0|         1107|       0|                0|         141|\n",
      "+----------+--------------+--------------------+--------+-------------+--------------+--------------+------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95e2cc",
   "metadata": {},
   "source": [
    "# 3. Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bba8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6790c",
   "metadata": {},
   "source": [
    "Now, we are going to clean the text of the twitter data. Then, we can use this cleaned text to extract features about the sensitivity of the tweet.\n",
    "\n",
    "Here, we remove numbers, punctuation, urls... Further, we transform the emojis to words. Emojis are at the very core of communication over social channels. One small image can completely describe one or more human emotions. A naive thing to do during pre-processing would be to remove all emojis. This could result in significant loss of meaning.\n",
    "A good way to achieve this is to replace the emoji with corresponding text explaining the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30919066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emoji.demojize(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5a6dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to udf\n",
    "clean_text_udf = F.udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "137d917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c70e267f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#BeforeIWasVegan I ate dairy cheese &amp;amp; Cadb...</td>\n",
       "      <td>beforeiwasvegan ate dairy cheese amp cadbury m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#CocaCola #cocacolacruelty #cocacoladairyfarm ...</td>\n",
       "      <td>cocacola cocacolacruelty cocacoladairyfarm fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Gunpowder &amp;amp; Lead #ManCave #SoyCandle | Ha...</td>\n",
       "      <td>gunpowder amp lead mancave soycandle hand pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#ICYMI from Folio.YVR Magazine: Folio.YVR Issu...</td>\n",
       "      <td>icymi from folioyvr magazine folioyvr issue lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#ICYMI ☆  NUDESTIX: On-the-Go Palettes &amp;amp; P...</td>\n",
       "      <td>icymi nudestix onthego palettes amp products f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  \\\n",
       "0  #BeforeIWasVegan I ate dairy cheese &amp; Cadb...   \n",
       "1  #CocaCola #cocacolacruelty #cocacoladairyfarm ...   \n",
       "2  #Gunpowder &amp; Lead #ManCave #SoyCandle | Ha...   \n",
       "3  #ICYMI from Folio.YVR Magazine: Folio.YVR Issu...   \n",
       "4  #ICYMI ☆  NUDESTIX: On-the-Go Palettes &amp; P...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  beforeiwasvegan ate dairy cheese amp cadbury m...  \n",
       "1  cocacola cocacolacruelty cocacoladairyfarm fai...  \n",
       "2  gunpowder amp lead mancave soycandle hand pour...  \n",
       "3  icymi from folioyvr magazine folioyvr issue lo...  \n",
       "4  icymi nudestix onthego palettes amp products f...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basetable_engr_final.select(\"full_text\", \"cleaned_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951b7b",
   "metadata": {},
   "source": [
    "Next, we tokenize the text. Then, we remove stop words. Finally, we use a spelling correction library to correct the spelling of the tweet data. This library from the textblob package is based on the Levenshtein distance. To correct the spelling, we first define a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "081c910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "# define helper function for spelling\n",
    "#correct_spelling_udf = F.udf(lambda tokens: [TextBlob(token).correct() for token in tokens], ArrayType(StringType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd55c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the cleaned_text variable \n",
    "#tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
    "#basetable_engr_final = tokenizer.transform(basetable_engr_final)\n",
    "\n",
    "#remove stop words \n",
    "#remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\")\n",
    "#basetable_engr_final = remover.transform(basetable_engr_final)\n",
    "#basetable_engr_final.select('tokens', 'clean_tokens').show()\n",
    "\n",
    "# correct spelling\n",
    "#basetable_engr_final = basetable_engr_final.withColumn(\"tokens_stemmed\", correct_spelling_udf(\"clean_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edc95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.select('clean_tokens', 'tokens_stemmed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecacd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fa5eb",
   "metadata": {},
   "source": [
    "Now that the text is cleaned, we can derive the sentiment of the text. In this next section, we derive the sentiment, the subjectivity and the polarity. These are the tree final features we add to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "21641bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the sentiment\n",
    "def get_sentiment(sentence):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # get sentiment dict\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    # get positive sentiment score\n",
    "    pos_sentiment = sentiment_dict[\"pos\"]\n",
    "    \n",
    "    # return positive sentiment score\n",
    "    return(pos_sentiment)\n",
    "\n",
    "# define function to get polarity score of text \n",
    "def get_polarity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[0]\n",
    "\n",
    "# define function to get subjectivity score of text \n",
    "def get_subjectivity(row):\n",
    "    textBlob_review = TextBlob(row)\n",
    "    return textBlob_review.sentiment[1]\n",
    "\n",
    "\n",
    "# register the functions as udf\n",
    "get_sentiment_udf = F.udf(get_sentiment, DoubleType())\n",
    "get_polarity_udf = F.udf(get_polarity, DoubleType())\n",
    "get_subjectivity_udf = F.udf(get_subjectivity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c764d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final basetable for our analysis\n",
    "basetable_engr_final = basetable_engr_final.withColumn(\"sentiment\", get_sentiment_udf(F.col(\"cleaned_text\")))\\\n",
    "                                .withColumn('polarity', get_polarity_udf(F.col('cleaned_text')))\\\n",
    "                                .withColumn('subjectivity', get_subjectivity_udf(F.col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d1e0f",
   "metadata": {},
   "source": [
    "# 4. Basetable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cd4d1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- user_followers: long (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = true)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the basetable:\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a4803",
   "metadata": {},
   "source": [
    "    - Check for missing values. If there are some - handle them (delete, impute,..). In this exercise: write code to handle them even if they aren't present.\n",
    "    - Adjust datatypes where needed\n",
    "    - Remove unnecessary columns\n",
    "    - Add data pre-processing steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be68364",
   "metadata": {},
   "source": [
    "## 4.1 Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "289f4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop (the 3 last variables were used to define the dependent variable)\n",
    "basetable_engr_final = basetable_engr_final.drop('full_text')\\\n",
    "                                .drop('screen_name')\\\n",
    "                                .drop('language')\\\n",
    "                                .drop('cleaned_text')\\\n",
    "                                .drop('retweet_count')\\\n",
    "                                .drop('tokens_stemmed')\\\n",
    "                                .drop('favorite_count')\\\n",
    "                                .drop('user_followers')\n",
    "\n",
    "#.drop('tokens')\\\n",
    "#.drop('clean_tokens')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1da23c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- eng_rate: double (nullable = true)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f169a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|media_type|user_following|nr_tweets_by_user|            eng_rate|num_emojis|upper_case_words|num_words|num_hashtags|num_mentions|nr_exlcamations|week_day|hour|month|day_month|quoted_ind|symbol_ind|user_age_days|verified|nr_media_elements|nr_text_char|sentiment|           polarity|      subjectivity|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "|     photo|          2497|            15906|0.008379888268156424|         0|               6|       34|           0|           0|              0|     Mon|   9|   11|        1|         1|         0|         2566|       0|                1|         308|    0.237| 0.2397959183673469| 0.573469387755102|\n",
      "|  no_media|          1217|             1986|0.002159827213822...|         0|               0|       18|           0|           0|              0|     Fri|  18|    4|       29|         1|         0|          943|       0|                0|         224|    0.183|0.09999999999999999|0.3666666666666667|\n",
      "|  no_media|          1789|            17254|0.002072538860103627|         0|               0|       28|           0|           0|              0|     Mon|   3|    8|       29|         0|         0|         4630|       0|                0|         171|    0.167|                0.0|               0.0|\n",
      "|  no_media|           328|             3338|0.010526315789473684|         0|               1|       21|           0|           0|              0|     Wed|  10|    7|        6|         0|         0|         1107|       0|                0|         217|    0.089|                0.0|               0.0|\n",
      "|  no_media|           728|             4603|                 0.0|         0|               2|       15|           0|           0|              0|     Thu|  18|    6|       30|         0|         0|         1107|       0|                0|         141|      0.0|                0.0|               0.0|\n",
      "+----------+--------------+-----------------+--------------------+----------+----------------+---------+------------+------------+---------------+--------+----+-----+---------+----------+----------+-------------+--------+-----------------+------------+---------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "#basetable_engr_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4f1f8",
   "metadata": {},
   "source": [
    "## 4.2 Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37067394",
   "metadata": {},
   "source": [
    "We see that there are no missing values in our dataset, so this step is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea47faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of missing values per column\n",
    "'''\n",
    "for col in basetable_engr_final.columns:\n",
    "    \n",
    "    # look at the perecentage of null values\n",
    "    print(\"Number of None values for variable\", col,\":\", basetable_engr_final.filter(F.col(col) == None).count())\n",
    "    print(\"Number of Null values for variable\", col,\":\", basetable_engr_final.filter(F.col(col).isNull()).count())\n",
    "\n",
    "'''                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cd53b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_type</th>\n",
       "      <th>user_following</th>\n",
       "      <th>nr_tweets_by_user</th>\n",
       "      <th>eng_rate</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>upper_case_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>nr_exlcamations</th>\n",
       "      <th>...</th>\n",
       "      <th>day_month</th>\n",
       "      <th>quoted_ind</th>\n",
       "      <th>symbol_ind</th>\n",
       "      <th>user_age_days</th>\n",
       "      <th>verified</th>\n",
       "      <th>nr_media_elements</th>\n",
       "      <th>nr_text_char</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   media_type  user_following  nr_tweets_by_user  eng_rate  num_emojis  \\\n",
       "0           0               0                  0         0           0   \n",
       "\n",
       "   upper_case_words  num_words  num_hashtags  num_mentions  nr_exlcamations  \\\n",
       "0                 0          0             0             0                0   \n",
       "\n",
       "   ...  day_month  quoted_ind  symbol_ind  user_age_days  verified  \\\n",
       "0  ...          0           0           0              0         0   \n",
       "\n",
       "   nr_media_elements  nr_text_char  sentiment  polarity  subjectivity  \n",
       "0                  0             0          0         0             0  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "basetable_engr_final.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in basetable_engr_final.columns]).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1616eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_type</th>\n",
       "      <th>user_following</th>\n",
       "      <th>nr_tweets_by_user</th>\n",
       "      <th>label</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>upper_case_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>nr_exlcamations</th>\n",
       "      <th>...</th>\n",
       "      <th>day_month</th>\n",
       "      <th>quoted_ind</th>\n",
       "      <th>symbol_ind</th>\n",
       "      <th>user_age_days</th>\n",
       "      <th>verified</th>\n",
       "      <th>nr_media_elements</th>\n",
       "      <th>nr_text_char</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   media_type  user_following  nr_tweets_by_user  label  num_emojis  \\\n",
       "0           0               0                  0     52           0   \n",
       "\n",
       "   upper_case_words  num_words  num_hashtags  num_mentions  nr_exlcamations  \\\n",
       "0                 0          0             0             0                0   \n",
       "\n",
       "   ...  day_month  quoted_ind  symbol_ind  user_age_days  verified  \\\n",
       "0  ...          0           0           0              0         0   \n",
       "\n",
       "   nr_media_elements  nr_text_char  sentiment  polarity  subjectivity  \n",
       "0                  0             0          0         0             0  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_engr_final.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in basetable_engr_final.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37062864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basetable_engr_final = basetable_engr_final.withColumnRenamed(\"eng_rate\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8006276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- media_type: string (nullable = true)\n",
      " |-- user_following: long (nullable = true)\n",
      " |-- nr_tweets_by_user: long (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- num_emojis: integer (nullable = true)\n",
      " |-- upper_case_words: integer (nullable = true)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- nr_exlcamations: integer (nullable = true)\n",
      " |-- week_day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_month: string (nullable = true)\n",
      " |-- quoted_ind: integer (nullable = true)\n",
      " |-- symbol_ind: integer (nullable = true)\n",
      " |-- user_age_days: integer (nullable = true)\n",
      " |-- verified: integer (nullable = true)\n",
      " |-- nr_media_elements: integer (nullable = true)\n",
      " |-- nr_text_char: integer (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- polarity: double (nullable = true)\n",
      " |-- subjectivity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basetable_engr_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "142f3a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('media_type', 'string'),\n",
       " ('user_following', 'bigint'),\n",
       " ('nr_tweets_by_user', 'bigint'),\n",
       " ('label', 'double'),\n",
       " ('num_emojis', 'int'),\n",
       " ('upper_case_words', 'int'),\n",
       " ('num_words', 'int'),\n",
       " ('num_hashtags', 'int'),\n",
       " ('num_mentions', 'int'),\n",
       " ('nr_exlcamations', 'int'),\n",
       " ('week_day', 'string'),\n",
       " ('hour', 'string'),\n",
       " ('month', 'string'),\n",
       " ('day_month', 'string'),\n",
       " ('quoted_ind', 'int'),\n",
       " ('symbol_ind', 'int'),\n",
       " ('user_age_days', 'int'),\n",
       " ('verified', 'int'),\n",
       " ('nr_media_elements', 'int'),\n",
       " ('nr_text_char', 'int'),\n",
       " ('sentiment', 'double'),\n",
       " ('polarity', 'double'),\n",
       " ('subjectivity', 'double')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_engr_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37742c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['media_type',\n",
       " 'user_following',\n",
       " 'nr_tweets_by_user',\n",
       " 'label',\n",
       " 'num_emojis',\n",
       " 'upper_case_words',\n",
       " 'num_words',\n",
       " 'num_hashtags',\n",
       " 'num_mentions',\n",
       " 'nr_exlcamations',\n",
       " 'week_day',\n",
       " 'hour',\n",
       " 'month',\n",
       " 'day_month',\n",
       " 'quoted_ind',\n",
       " 'symbol_ind',\n",
       " 'user_age_days',\n",
       " 'verified',\n",
       " 'nr_media_elements',\n",
       " 'nr_text_char',\n",
       " 'sentiment',\n",
       " 'polarity',\n",
       " 'subjectivity']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable_engr_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976b529",
   "metadata": {},
   "source": [
    "**splitting in test and train** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "503f179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "train, test = basetable_engr_final.randomSplit([0.70, 0.30], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bd43b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.cache()\n",
    "test = test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7c45c",
   "metadata": {},
   "source": [
    "Further, we see that our variables all have the correct datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0557",
   "metadata": {},
   "source": [
    "# 4.3 Add pre-processing steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450db7",
   "metadata": {},
   "source": [
    "**Import required transformers and estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a18c486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark ml packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, StandardScaler, Word2Vec\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "256ddd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the numeric features\n",
    "\n",
    "numFeatureCols = [\n",
    " 'user_following',\n",
    " 'nr_tweets_by_user',\n",
    " 'num_emojis',\n",
    " 'upper_case_words',\n",
    " 'num_words',\n",
    " 'num_hashtags',\n",
    " 'num_mentions',\n",
    " 'nr_exlcamations',\n",
    " 'user_age_days',\n",
    " 'verified',\n",
    " 'nr_media_elements',\n",
    " 'nr_text_char']\n",
    "\n",
    "# define vector assembler\n",
    "VA_num = VectorAssembler(inputCols=numFeatureCols, outputCol=\"numeric_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc652b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standard scaler (SS)\n",
    "SS = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785317d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables \n",
    "#https://masum-math8065.medium.com/how-to-convert-categorical-data-into-numeric-in-pyspark-2202407f5fac\n",
    "('media_type', 'string')\n",
    "('week_day', 'string'),\n",
    " ('hour', 'string'),\n",
    " ('month', 'string'),\n",
    " ('day_month', 'string'),\n",
    "    quoted_ind \n",
    "    symbol_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f5ce10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector assembler\n",
    "VA_all = VectorAssembler(inputCols=[\"scaled_numeric_features\", \"sentiment\",\"polarity\", \"subjectivity\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "01309ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model (RF)\n",
    "RF = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f399669",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb096ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ==================================>              (144 + 8) / 200]=========>       (172 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 23:00:27 ERROR Executor: Exception in task 22.0 in stage 601.0 (TID 11882)\n",
      "scala.MatchError: [null,1.0,(15,[0,1,3,4,8,11],[0.0027829459326121873,2.5241885171139194E-5,0.5711183903247351,0.4410160850234674,0.18989272432957316,0.32793849119407875])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/08 23:00:27 ERROR Executor: Exception in task 16.0 in stage 601.0 (TID 11876)\n",
      "scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 16.0 in stage 601.0 (TID 11876) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/12/08 23:00:27 ERROR TaskSetManager: Task 16 in stage 601.0 failed 1 times; aborting job\n",
      "22/12/08 23:00:27 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 601.0 failed 1 times, most recent failure: Lost task 16.0 in stage 601.0 (TID 11876) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 22.0 in stage 601.0 (TID 11882) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(15,[0,1,3,4,8,11],[0.0027829459326121873,2.5241885171139194E-5,0.5711183903247351,0.4410160850234674,0.18989272432957316,0.32793849119407875])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 29.0 in stage 601.0 (TID 11889) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 31.0 in stage 601.0 (TID 11891) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 32.0 in stage 601.0 (TID 11892) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 30.0 in stage 601.0 (TID 11890) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 23:00:27 WARN TaskSetManager: Lost task 33.0 in stage 601.0 (TID 11893) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1793.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 601.0 failed 1 times, most recent failure: Lost task 16.0 in stage 601.0 (TID 11876) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define pipeline model and fit on training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m RF_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetStages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mVA_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVA_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRF\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1793.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 601.0 failed 1 times, most recent failure: Lost task 16.0 in stage 601.0 (TID 11876) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: scala.MatchError: [null,1.0,(15,[0,1,4,8,10,11,12],[0.0011595608052550781,0.001501892167682782,2.0948264038614703,0.15250759422718846,1.369525483289042,2.2487210824736827,0.116])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# define pipeline model and fit on training data\n",
    "RF_pipeline = Pipeline().setStages([VA_num, SS, VA_all, RF]).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607f2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f287b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.silect.is/blog/random-forest-models-in-spark-ml/ \n",
    "# get predictions on test set\n",
    "predictions = RF_pipeline.transform(test) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rfPred = cvModel.transform(df)\n",
    "rfResult = rfPred.toPandas()\n",
    "plt.plot(rfResult.label, rfResult.prediction, 'bo')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Prediction')\n",
    "plt.suptitle(\"Model Performance RMSE: %f\" % rmse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1957db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d7e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27dc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e477d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100300da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "766099c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stages of preprocessing pipeline\n",
    "stagesPreprocessingPipeline = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53156f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f28f38",
   "metadata": {},
   "source": [
    "**Create a vector for each type of features (numeric, categorical)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60accc3d",
   "metadata": {},
   "source": [
    "#### handle numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "676921ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the numeric variables\n",
    "num_cols = ['user_following', 'nr_tweets_by_user', 'num_emojis', 'upper_case_words',\n",
    "           'num_words', 'num_hashtags', 'num_mentions', 'nr_exlcamations', 'quoted_ind',\n",
    "           'symbol_ind', 'user_age_days', 'verified', 'nr_media_elements', 'nr_text_char', \n",
    "           'sentiment', 'polarity', 'subjectivity']\n",
    "\n",
    "# define the assembler\n",
    "num_vec_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(num_vec_assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42297141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a33263",
   "metadata": {},
   "source": [
    "#### handle categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe911a",
   "metadata": {},
   "source": [
    "First, we transform the text variables in our dataset into numerical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef799e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dc251be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# define the categorical variables\\ncat_cols = [\\'media_type\\', \\'hour\\', \\'week_day\\', \\'month\\', \\'day_month\\']\\n\\n# create an object of StringIndexer class for each categorical variable\\nSI_media = StringIndexer(inputCol= \\'media_type\\', outputCol= \\'media_type_index\\').setHandleInvalid(\"skip\")\\nSI_hour = StringIndexer(inputCol= \\'hour\\', outputCol= \\'hour_index\\').setHandleInvalid(\"skip\")\\nSI_week_day = StringIndexer(inputCol= \\'week_day\\', outputCol= \\'week_day_index\\').setHandleInvalid(\"skip\")\\nSI_month = StringIndexer(inputCol= \\'month\\', outputCol= \\'month_index\\').setHandleInvalid(\"skip\")\\nSI_day_month = StringIndexer(inputCol= \\'day_month\\', outputCol= \\'day_month_index\\').setHandleInvalid(\"skip\")\\n\\n# add to pipeline stage\\n\\nstagesPreprocessingPipeline.append(SI_media)\\nstagesPreprocessingPipeline.append(SI_hour)\\nstagesPreprocessingPipeline.append(SI_week_day)\\nstagesPreprocessingPipeline.append(SI_month)\\nstagesPreprocessingPipeline.append(SI_day_month)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define the categorical variables\n",
    "cat_cols = ['media_type', 'hour', 'week_day', 'month', 'day_month']\n",
    "\n",
    "# create an object of StringIndexer class for each categorical variable\n",
    "SI_media = StringIndexer(inputCol= 'media_type', outputCol= 'media_type_index').setHandleInvalid(\"skip\")\n",
    "SI_hour = StringIndexer(inputCol= 'hour', outputCol= 'hour_index').setHandleInvalid(\"skip\")\n",
    "SI_week_day = StringIndexer(inputCol= 'week_day', outputCol= 'week_day_index').setHandleInvalid(\"skip\")\n",
    "SI_month = StringIndexer(inputCol= 'month', outputCol= 'month_index').setHandleInvalid(\"skip\")\n",
    "SI_day_month = StringIndexer(inputCol= 'day_month', outputCol= 'day_month_index').setHandleInvalid(\"skip\")\n",
    "\n",
    "# add to pipeline stage\n",
    "\n",
    "stagesPreprocessingPipeline.append(SI_media)\n",
    "stagesPreprocessingPipeline.append(SI_hour)\n",
    "stagesPreprocessingPipeline.append(SI_week_day)\n",
    "stagesPreprocessingPipeline.append(SI_month)\n",
    "stagesPreprocessingPipeline.append(SI_day_month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39b48d",
   "metadata": {},
   "source": [
    "Next, we use a VectorAssembler to assemble all indexed variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5429a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the assembler\n",
    "cat_cols_indexed = ['media_type_index', 'hour_index', 'week_day_index', 'month_index', 'day_month_index']\n",
    "cat_vec_assembler = VectorAssembler(inputCols= cat_cols_indexed, outputCol=\"cat_features\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(cat_vec_assembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a188e",
   "metadata": {},
   "source": [
    "VRAAG KONSTANTIN: VECTOR INDEXER NOG NODIG NA STRINGINDEXER?\n",
    "Finally ,we use the VectorIndexer that helps index categorical features in datasets of vectors. This is required for tree based methods, which we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79e5f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexer\n",
    "indexer = VectorIndexer(inputCol= \"cat_features\", outputCol=\"cat_features_indexed\")\n",
    "\n",
    "# add to pipeline stage\n",
    "stagesPreprocessingPipeline.append(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "801d6b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_3a6f0afdaf21]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bcdf780",
   "metadata": {},
   "source": [
    "We will also look at the words that were used in the text with the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33999f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word2vec model\n",
    "#W2V = Word2Vec(inputCol = 'tokens_stemmed', outputCol = 'text_features')\n",
    "\n",
    "# add to pipeline stage\n",
    "#stagesPreprocessingPipeline.append(W2V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7d00f",
   "metadata": {},
   "source": [
    "**Define, fit and apply Pipeline on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b865e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model and fit on data\n",
    "pipeline_model_preprocess = Pipeline().setStages(stagesPreprocessingPipeline).fit(basetable_engr_final)\n",
    "\n",
    "# transform data by applying pipeline model on data\n",
    "preprocessed_data = pipeline_model_preprocess.transform(basetable_engr_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a3065",
   "metadata": {},
   "source": [
    "**Select features and label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a864c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and labels\n",
    "#preprocessed_data = preprocessed_data.select([\"num_features\", \"cat_features_indexed\", \"eng_rate\"])\n",
    "preprocessed_data = preprocessed_data.select([\"num_features\", \"eng_rate\"])\n",
    "\n",
    "# rename engagement rate to label\n",
    "preprocessed_data = preprocessed_data.withColumnRenamed(\"eng_rate\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9be8c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        num_features|               label|\n",
      "+--------------------+--------------------+\n",
      "|(17,[0,1,4,10,12,...|                 0.0|\n",
      "|(17,[0,1,3,4,10,1...|3.619254433586681E-4|\n",
      "|(17,[0,1,4,10,13,...|7.874015748031496E-4|\n",
      "|(17,[0,1,3,4,10,1...|0.003115264797507788|\n",
      "|(17,[0,1,4,10,12,...|0.025477707006369428|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# look at the data\n",
    "preprocessed_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a60e7a",
   "metadata": {},
   "source": [
    "## 4.5 Split dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2257052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "train, test = preprocessed_data.randomSplit([0.7, 0.3], seed= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42c5b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training set: 32657 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 242:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the test set: 14287 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check number of observations in both datasets\n",
    "#print(\"Number of observations in the training set: %s \" % train.count())\n",
    "#print(\"Number of observations in the test set: %s \" %test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e5336",
   "metadata": {},
   "source": [
    "**Define scaler to standardize numeric features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c787472",
   "metadata": {},
   "source": [
    "The `StandardScaler` should only be performed on the trainingset, because an equal `mean` and `standard deviation` between the training- and testset need to be assumed to avoid methodological mistakes. However, as we will only train a random forest model it is not needed to standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148dc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5abfb88",
   "metadata": {},
   "source": [
    "**Create final feature vector containing the indexed categorical features and scaled numeric features**\n",
    "\n",
    "Most ML algorithms can only handle one vector as input, so we add all preprocessed columns (here two vectors) into one vector, using the ``VectorAssembler`` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2885feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define assembler\n",
    "#vec_assembler = VectorAssembler(inputCols=[\"cat_features_indexed\", \"num_features\"], outputCol=\"features\")\n",
    "vec_assembler = VectorAssembler(inputCols=[\"num_features\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550228c",
   "metadata": {},
   "source": [
    "**Define, fit and apply Pipeline on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7f5eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline model stages\n",
    "stages = [vec_assembler]\n",
    "\n",
    "# define pipeline model\n",
    "pipeline_model_split = Pipeline().setStages(stages)\n",
    "\n",
    "# fit pipeline model on training data\n",
    "pipeline_model_split = pipeline_model_split.fit(train)\n",
    "\n",
    "# transform training set\n",
    "train_final = pipeline_model_split.transform(train).select([\"label\", \"features\"])\n",
    "\n",
    "# transform test set\n",
    "test_final = pipeline_model_split.transform(test).select([\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb801858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "|(17,[0,1,2,3,4,10...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# inspect the final data\n",
    "train_final.select('features').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff619af6",
   "metadata": {},
   "source": [
    "# 5. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cea52",
   "metadata": {},
   "source": [
    "In this part, we are going to train a random forest model on the training data. This is an advanced machine learning model of which we expect good performance. After we train this model, we will evaluate it. As the goal of our analysis was to find the driving factors behind the engagement rate, we will use the shap values to determine variable importance. This way, we know in which direction the variable affects the engagement rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af18d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and evaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3413885",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8759b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rf model\n",
    "RF = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", numTrees=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3920129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 148:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:28 ERROR Executor: Exception in task 5.0 in stage 148.0 (TID 3356)\n",
      "scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/08 20:58:29 WARN TaskSetManager: Lost task 5.0 in stage 148.0 (TID 3356) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/12/08 20:58:29 ERROR TaskSetManager: Task 5 in stage 148.0 failed 1 times; aborting job\n",
      "22/12/08 20:58:29 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 148.0 failed 1 times, most recent failure: Lost task 5.0 in stage 148.0 (TID 3356) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:29 WARN TaskSetManager: Lost task 8.0 in stage 148.0 (TID 3359) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 148:>                                                        (0 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:31 WARN PythonUDFRunner: Incomplete task 0.0 in stage 148 (TID 3351) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:31 WARN PythonUDFRunner: Incomplete task 0.0 in stage 148 (TID 3351) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:31 WARN PythonUDFRunner: Incomplete task 0.0 in stage 148 (TID 3351) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:31 WARN PythonUDFRunner: Incomplete task 4.0 in stage 148 (TID 3355) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:31 WARN PythonUDFRunner: Incomplete task 4.0 in stage 148 (TID 3355) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:31 WARN PythonUDFRunner: Incomplete task 0.0 in stage 148 (TID 3351) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:31 WARN TaskSetManager: Lost task 4.0 in stage 148.0 (TID 3355) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 20:58:31 WARN TaskSetManager: Lost task 0.0 in stage 148.0 (TID 3351) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 148:>                                                        (0 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 2.0 in stage 148 (TID 3353) interrupted: Attempting to kill Python Worker\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o704.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 148.0 failed 1 times, most recent failure: Lost task 5.0 in stage 148.0 (TID 3356) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fit model on training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mRF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_final\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o704.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 148.0 failed 1 times, most recent failure: Lost task 5.0 in stage 148.0 (TID 3356) (192.168.1.22 executor driver): scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: scala.MatchError: [null,1.0,(17,[0,1,3,4,10,13,14,15,16],[14.0,69.0,10.0,9.0,718.0,66.0,0.452,0.5,0.7])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 2.0 in stage 148 (TID 3353) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 2.0 in stage 148 (TID 3353) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN TaskSetManager: Lost task 2.0 in stage 148.0 (TID 3353) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 6.0 in stage 148 (TID 3357) interrupted: Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\r",
      "[Stage 148:>                                                        (0 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 1.0 in stage 148 (TID 3352) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 6.0 in stage 148 (TID 3357) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 1.0 in stage 148 (TID 3352) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 3.0 in stage 148 (TID 3354) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 3.0 in stage 148 (TID 3354) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 3.0 in stage 148 (TID 3354) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN TaskSetManager: Lost task 3.0 in stage 148.0 (TID 3354) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\r",
      "[Stage 148:>                                                        (0 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 6.0 in stage 148 (TID 3357) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 1.0 in stage 148 (TID 3352) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN TaskSetManager: Lost task 6.0 in stage 148.0 (TID 3357) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 1.0 in stage 148 (TID 3352) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN TaskSetManager: Lost task 1.0 in stage 148.0 (TID 3352) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 7.0 in stage 148 (TID 3358) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 7.0 in stage 148 (TID 3358) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN PythonUDFRunner: Incomplete task 7.0 in stage 148 (TID 3358) interrupted: Attempting to kill Python Worker\n",
      "22/12/08 20:58:32 WARN TaskSetManager: Lost task 7.0 in stage 148.0 (TID 3358) (192.168.1.22 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# fit model on training set\n",
    "rf_model = RF.fit(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test set\n",
    "rf_preds = rf_model.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluator object\n",
    "rfEvaluator = RegressionEvaluator(labelCol = 'label', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get different metrics using your created evaluator object\n",
    "rfsq = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'r2'})\n",
    "rfmae = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'mae'})\n",
    "rfrmse = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'rmse'})\n",
    "rfmse = rfEvaluator.evaluate(rf_preds, {rfEvaluator.metricName: 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30252ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect evaluation metrics\n",
    "print('R^2  : %g' % rfsq)\n",
    "print('MAE  : %g' % rfmae)\n",
    "print('RMSE : %g' % rfrmse)\n",
    "print('MSE  : %g' % rfmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263b4fb",
   "metadata": {},
   "source": [
    "#### Use cross validation to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter space\n",
    "rfparamGrid = (ParamGridBuilder().addGrid(RF.maxDepth, [3, 7, 10])\n",
    "                                   .addGrid(RF.maxBins, [15, 25, 40])\n",
    "                                   .addGrid(RF.numTrees, [5, 25, 60])\n",
    "                                   .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ab519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross validation\n",
    "rfcv_model = CrossValidator(estimator=RF, #random forest model we created before\n",
    "                          estimatorParamMaps=rfparamGrid, \n",
    "                          evaluator=rfEvaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cross validation on trainig set\n",
    "rfcv_model = rfcv_model.fit(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect best params\n",
    "print(\"best max depth: %s\" %rfcv_model.bestModel._java_obj.getMaxDepth())\n",
    "print(\"best max bins: %s\" %rfcv_model.bestModel._java_obj.getMaxBins())\n",
    "print(\"best num trees: %s\" %rfcv_model.bestModel._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions of best model of on test set (cv_model automatically uses best model)\n",
    "rfcv_preds = rfcv_model.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "rfcv_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca17e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ee57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which of both algorithms is the best:\n",
    "print(\"RANDOM FOREST WITHOUT CV:\")\n",
    "print('  R^2  : %g' % rfsq)\n",
    "print('  MAE  : %g' % rfmae)\n",
    "print('  RMSE : %g' % rfrmse)\n",
    "print('  MSE  : %g' % rfmse)\n",
    "print(\"------------------\")\n",
    "print(\"RANDOM FOREST WITH CV:\")\n",
    "print('  R^2  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'r2'}))\n",
    "print('  MAE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mae'}))\n",
    "print('  RMSE : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'rmse'}))\n",
    "print('  MSE  : %g' % rfcv_evaluator.evaluate(rfcv_preds, {rfcv_evaluator.metricName: 'mse'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189e975",
   "metadata": {},
   "source": [
    "# 6. Interpretation Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee61ec",
   "metadata": {},
   "source": [
    "As we want use the random forest model, we would also like some insights into the drivers behind the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install shap\n",
    "#!pip3 install numpy\n",
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values,  X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0c040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31414fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44474fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70addd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d80d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
