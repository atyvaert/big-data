{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e9ec2f-dd80-4b6a-9c7b-f239ccd9ac0c",
   "metadata": {},
   "source": [
    "# Reading in the files and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15a1795-9538-404d-90f1-5378932d6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pandas as pd \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "import os \n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3be841-af34-4b6d-b83d-5c364a1bda54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:11:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:12:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "path_json = \".././../data/Topic_vegan/*.json\" \n",
    "\n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51ae01c-2e05-4620-8cc9-b313ac0f1c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.45:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107cdd180>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fb514-15ac-47a2-b692-67c761b13302",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting the correct columns, converting types and checking for doubles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344b4f0e-a251-47d5-a755-fc732d01af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3428559"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twitter represents likes as hearts \n",
    "# the number of likes different topics receive over time \n",
    "# use favorites_count \n",
    "\n",
    "#plot aantal tweets over een topic over time \n",
    "\n",
    "df_json_sub = df_json.select(F.col(\"user.name\"),\n",
    "                                F.col(\"user.screen_name\"),\n",
    "                                F.col(\"created_at\"), \n",
    "                                F.col(\"full_text\"),\n",
    "                                F.col(\"entities.hashtags\"),\n",
    "                                F.col(\"lang\"),\n",
    "                                F.col(\"favorite_count\"),\n",
    "                                F.col(\"user.followers_count\"),\n",
    "                                F.col(\"user.friends_count\"),\n",
    "                                F.col(\"user.favourites_count\"),\n",
    "                                F.col(\"entities.urls\"),\n",
    "                                F.col(\"entities.symbols\"), \n",
    "                                )\n",
    "#date --> time stamp variable \n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "df_json_sub = df_json_sub.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))\n",
    "\n",
    "#extract year, month, day \n",
    "\n",
    "df_json_sub = df_json_sub.withColumn('year', year('post_created_at'))\n",
    "df_json_sub = df_json_sub.withColumn('month', month('post_created_at'))\n",
    "df_json_sub = df_json_sub.withColumn('day', dayofmonth('post_created_at')) #number from 1 to 31 \n",
    "\n",
    "df_json_sub.count() #3428559 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfab48-373e-4ff6-a8d7-ef897a6280bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      " |-- friends_count: long (nullable = true)\n",
      " |-- favourites_count: long (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- symbols: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop duplicates and retweets \n",
    "df_json_sub = df_json_sub.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates().cache()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df_json_sub = df_json_sub.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df_json_sub = df_json_sub.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "                        \n",
    "df_json_sub.printSchema()\n",
    "#df_json_sub.count() #1340938 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14662d-55d1-400c-b52c-e06d568e34ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c006eb-78a9-4bcf-b319-c71755ed1cec",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d4cecc-aa3f-40f8-8a4b-1fa214d9f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e72eba-6c0f-4039-90e7-f84785d90c62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      " |-- friends_count: long (nullable = true)\n",
      " |-- favourites_count: long (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- symbols: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- emoji_count: integer (nullable = true)\n",
      " |-- text_tokenized: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- num_words: integer (nullable = false)\n",
      " |-- num_hashtags: integer (nullable = true)\n",
      " |-- num_mentions: integer (nullable = true)\n",
      " |-- num_exclamation_marks: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_df = df_json_sub.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f24281-01e2-47c4-adff-cc8dcefc8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emoji.demojize(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ac4174-fa4d-44ae-8d7e-ec0101738015",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c458e296-7b50-4b62-befb-ca52f79cc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.createOrReplaceTempView('twitter_df') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e297cda-b918-4fc0-a6ab-e34c55963585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=========================>                              (90 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_90 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:36 WARN BlockManager: Persisting block rdd_23_90 to disk instead.\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_94 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:36 WARN BlockManager: Persisting block rdd_23_94 to disk instead.\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_96 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_94 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_90 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:36 WARN BlockManager: Persisting block rdd_23_96 to disk instead.\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_96 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_101 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:36 WARN BlockManager: Persisting block rdd_23_101 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===========================>                            (99 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_100 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:36 WARN BlockManager: Persisting block rdd_23_100 to disk instead.\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_105 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:36 WARN BlockManager: Persisting block rdd_23_105 to disk instead.\n",
      "22/11/25 16:14:36 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_100 in memory.\n",
      "22/11/25 16:14:36 WARN MemoryStore: Not enough space to cache rdd_23_100 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_106 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_106 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_105 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_106 in memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================>                          (104 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_107 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_107 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_106 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_107 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_108 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_108 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_110 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_110 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_110 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_112 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_112 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_114 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_113 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_114 to disk instead.\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_113 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_113 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_114 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_115 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_115 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_116 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_118 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_118 to disk instead.\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_116 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:================================>                      (119 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_117 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_117 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_115 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_121 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_121 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_119 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_119 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_120 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_120 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_119 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_124 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_124 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_121 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_129 in memory.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_125 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_125 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_124 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_123 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_123 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_129 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_129 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_125 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_123 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_130 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_130 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_128 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_128 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================>                  (132 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_131 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_131 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_138 in memory.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_133 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_134 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_134 to disk instead.\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_133 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_131 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_133 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_134 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_138 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_138 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_137 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_137 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_135 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_135 to disk instead.\n",
      "22/11/25 16:14:37 WARN MemoryStore: Not enough space to cache rdd_23_136 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:37 WARN BlockManager: Persisting block rdd_23_136 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_139 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_139 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_141 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_141 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_140 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_140 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_139 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_143 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_143 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_140 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_145 in memory! (computed 3.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================>                (140 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_145 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_146 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_146 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_146 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_147 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_147 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_148 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_148 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_151 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_151 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_147 in memory! (computed 3.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==========================================>            (154 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_151 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_154 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_154 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_154 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_158 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_158 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_158 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_161 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_161 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_162 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_163 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_163 to disk instead.\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_162 to disk instead.\n",
      "22/11/25 16:14:38 WARN MemoryStore: Not enough space to cache rdd_23_164 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:38 WARN BlockManager: Persisting block rdd_23_164 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_168 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_168 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_167 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_167 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_164 in memory.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_164 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================================>         (166 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_167 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_168 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_169 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_169 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_170 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_170 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_170 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_172 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_172 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_177 in memory.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_180 in memory.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_172 in memory.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_174 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_174 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_172 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_177 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_177 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_175 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_175 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===============================================>       (172 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_180 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_180 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_179 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_179 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_178 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_178 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_178 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:================================================>      (177 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_181 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_181 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_185 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_185 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================================>   (188 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_188 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_188 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_188 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_186 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_186 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_191 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_191 to disk instead.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_197 in memory.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_196 in memory.\n",
      "22/11/25 16:14:39 WARN MemoryStore: Not enough space to cache rdd_23_192 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:39 WARN BlockManager: Persisting block rdd_23_192 to disk instead.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_192 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_194 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:40 WARN BlockManager: Persisting block rdd_23_194 to disk instead.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_198 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_196 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_199 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_193 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN BlockManager: Persisting block rdd_23_193 to disk instead.\n",
      "22/11/25 16:14:40 WARN BlockManager: Persisting block rdd_23_196 to disk instead.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_197 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN BlockManager: Persisting block rdd_23_197 to disk instead.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_194 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_198 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN BlockManager: Persisting block rdd_23_198 to disk instead.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_194 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_199 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN BlockManager: Persisting block rdd_23_199 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_4 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_2 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_0 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_3 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_4 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_1 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_5 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_6 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_2 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_0 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_7 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_3 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_1 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_9 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_8 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_10 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_12 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_12 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_11 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_13 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_14 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_14 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_13 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_15 in memory.\n",
      "22/11/25 16:14:40 WARN MemoryStore: Not enough space to cache rdd_23_15 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==>                                                      (8 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_16 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_17 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_18 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_19 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_19 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_20 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_21 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_22 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_21 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_23 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_22 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_23 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====>                                                  (20 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_24 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_26 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_25 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_27 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_28 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_29 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_28 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_29 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_30 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_31 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_30 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_31 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======>                                                 (24 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_32 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_33 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_34 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_35 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_37 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_38 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_37 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_39 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_38 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_36 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_39 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==========>                                             (38 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_40 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_41 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_42 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_43 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_44 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_45 in memory.\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_45 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_46 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:41 WARN MemoryStore: Not enough space to cache rdd_23_47 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_48 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_51 in memory.\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_49 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_50 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_51 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_52 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_53 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_54 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============>                                           (46 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_55 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_56 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_57 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_58 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_59 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_60 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_23_61 in memory.\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_61 in memory! (computed 384.0 B so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_62 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============>                                         (53 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_63 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_64 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:42 WARN MemoryStore: Not enough space to cache rdd_23_68 in memory! (computed 3.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                   (74 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_90 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_94 in memory! (computed 3.8 MiB so far)\n",
      "[206.587s][warning][gc,alloc] dag-scheduler-event-loop: Retried waiting for GCLocker too often allocating 256 words\n",
      "[206.587s][warning][gc,alloc] task-result-getter-3: Retried waiting for GCLocker too often allocating 256 words\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_96 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_100 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==========================>                             (94 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_105 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_106 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_107 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_110 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================>                          (102 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_113 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_114 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_115 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_119 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_121 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_123 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_124 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:43 WARN MemoryStore: Not enough space to cache rdd_23_125 in memory! (computed 3.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:================================>                      (118 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_131 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_133 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_134 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_139 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================>                    (126 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_140 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================================>                  (134 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_146 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_147 in memory! (computed 3.8 MiB so far)\n",
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_151 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:========================================>              (148 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_154 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:44 WARN MemoryStore: Not enough space to cache rdd_23_158 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===========================================>           (158 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_164 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_167 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_168 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_170 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_172 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================================>         (165 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_178 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>     (179 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_188 in memory! (computed 3.9 MiB so far)\n",
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_192 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================> (196 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:45 WARN MemoryStore: Not enough space to cache rdd_23_194 in memory! (computed 3.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 16:14:52 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'emojis'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/25 16:14:52 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'emojis'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/25 16:14:52 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 4758)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'emojis'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "22/11/25 16:14:52 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 4758) (192.168.1.45 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'emojis'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "\n",
      "22/11/25 16:14:52 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 679, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'emojis'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect * from twitter_df\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 679, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'emojis'\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from twitter_df\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a5aa49-cb74-43e4-9df3-3120c821291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5d84b-3d0c-4d8e-8813-a112dd7c1c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
