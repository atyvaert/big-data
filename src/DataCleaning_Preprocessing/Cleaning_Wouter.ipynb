{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning for Google Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import emojis\n",
    "from translate import Translator\n",
    "\n",
    "import sparknlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark NLP, master=local[*]) created by getOrCreate at /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/sparknlp/__init__.py:164 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_55802/2477715280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# create spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# create spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    431\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark NLP, master=local[*]) created by getOrCreate at /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/sparknlp/__init__.py:164 "
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "#path_json = \".././../data/Topic_vegan/*.json\"\n",
    "\n",
    "# use this if you want all the tweet files, but this is usually too large\n",
    "#df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 16:04:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3428559"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_brands = [\"healthyfood\",\n",
    "               \"healthylifestyle\",\n",
    "               \"vegan\",\n",
    "               \"keto\",\n",
    "               \"ketodiet\",\n",
    "               \"ketolifestyle\",\n",
    "               \"veganism\",\n",
    "               \"vegetarian\"]\n",
    "from re import search\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \".././../data/Topic_vegan/\"\n",
    "tweet_files = [os.path.join(data_dir, obs) for obs in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "\n",
    "files_brand = [file for file in tweet_files if (file.find(list_brands[2]) != -1)]\n",
    "files_brand               \n",
    "               \n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(files_brand)  \n",
    "df_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:55 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>139</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "      <td>655</td>\n",
       "      <td>3837</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karen Reed üå∏</td>\n",
       "      <td>kandk670</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>@trudiebakescake Organic coconut oil in a jar ...</td>\n",
       "      <td>711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>„Éè„É´):)</td>\n",
       "      <td>patlnwza55</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>236</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>655</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ÔæåÔΩßÔæôÔæÑÔæûÔæó„Å£Â≠êorganicÊúâÊ©üüíôüíª</td>\n",
       "      <td>organic_yusai</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>„Éû„Ç∏„Åß„Éî„É≥„ÉÅÂä©„Åë„Å¶Ëá™Ëª¢Ëªä„Ç¨„Ç¨„Ç¨„Ç¨</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "      <td>139</td>\n",
       "      <td>3837</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ÔΩ°‚óï‚Äø‚óïÔΩ°ùë±ùíÜ ùíï'ùíÇùíäùíéùíÜ üê∂üß°‚ú®</td>\n",
       "      <td>MyFnlovely97</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>245</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sang‚Ñ¢</td>\n",
       "      <td>asan_gk</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @NotechAna: Am I the only one who types in ...</td>\n",
       "      <td>2065</td>\n",
       "      <td>374</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trysia ):)‚ñ™Ô∏énever let me go‚ñ™Ô∏é</td>\n",
       "      <td>Winnie_thephuu</td>\n",
       "      <td>Mon Apr 04 10:09:48 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>379</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name     screen_name  \\\n",
       "0                        „ÅÆ„Çä/Nori      nori_k_629   \n",
       "1                          alice   myn4meizalize   \n",
       "2                   Karen Reed üå∏        kandk670   \n",
       "3                          „Éè„É´):)      patlnwza55   \n",
       "4                          alice   myn4meizalize   \n",
       "5            ÔæåÔΩßÔæôÔæÑÔæûÔæó„Å£Â≠êorganicÊúâÊ©üüíôüíª   organic_yusai   \n",
       "6                        „ÅÆ„Çä/Nori      nori_k_629   \n",
       "7             ÔΩ°‚óï‚Äø‚óïÔΩ°ùë±ùíÜ ùíï'ùíÇùíäùíéùíÜ üê∂üß°‚ú®    MyFnlovely97   \n",
       "8                          Sang‚Ñ¢         asan_gk   \n",
       "9  Trysia ):)‚ñ™Ô∏énever let me go‚ñ™Ô∏é  Winnie_thephuu   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Mon Apr 04 10:09:55 +0000 2022   \n",
       "1  Mon Apr 04 10:09:54 +0000 2022   \n",
       "2  Mon Apr 04 10:09:54 +0000 2022   \n",
       "3  Mon Apr 04 10:09:52 +0000 2022   \n",
       "4  Mon Apr 04 10:09:52 +0000 2022   \n",
       "5  Mon Apr 04 10:09:52 +0000 2022   \n",
       "6  Mon Apr 04 10:09:50 +0000 2022   \n",
       "7  Mon Apr 04 10:09:50 +0000 2022   \n",
       "8  Mon Apr 04 10:09:50 +0000 2022   \n",
       "9  Mon Apr 04 10:09:48 +0000 2022   \n",
       "\n",
       "                                           full_text  followers_count  \\\n",
       "0  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              139   \n",
       "1  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...              655   \n",
       "2  @trudiebakescake Organic coconut oil in a jar ...              711   \n",
       "3  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              236   \n",
       "4  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              655   \n",
       "5                                   „Éû„Ç∏„Åß„Éî„É≥„ÉÅÂä©„Åë„Å¶Ëá™Ëª¢Ëªä„Ç¨„Ç¨„Ç¨„Ç¨              291   \n",
       "6  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...              139   \n",
       "7  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              245   \n",
       "8  RT @NotechAna: Am I the only one who types in ...             2065   \n",
       "9  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              379   \n",
       "\n",
       "   retweet_count  favorite_count hashtags  \n",
       "0           3582               0       []  \n",
       "1           3837               0       []  \n",
       "2              0               0       []  \n",
       "3           3582               0       []  \n",
       "4           3582               0       []  \n",
       "5              0               1       []  \n",
       "6           3837               0       []  \n",
       "7           3582               0       []  \n",
       "8            374               0       []  \n",
       "9           3582               0       []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"),\n",
    "                    F.col(\"user.followers_count\"),\n",
    "                    F.col(\"retweet_count\"),\n",
    "                    F.col(\"favorite_count\"),\n",
    "                    F.col(\"entities.hashtags\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "\n",
    "#df.printSchema()\n",
    "#df.count() #1340938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+---------------+-------------+--------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|followers_count|retweet_count|favorite_count|            hashtags|    post_created_at|emoji_count|      text_tokenized|num_words|num_hashtags|num_mentions|num_exclamation_marks|\n",
      "+--------------------+---------------+--------------------+--------------------+---------------+-------------+--------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|           4285|            1|             6|[{[2, 8], vegan},...|2022-05-14 00:55:33|          0|[!\\n#vegan, #GoVe...|        4|           3|           0|                    1|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|            947|            0|             0|[{[69, 79], vegan...|2022-01-15 07:17:18|          0|[!, We, will, be,...|       35|          21|           0|                    3|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|          10745|            3|             8|[{[52, 67], TheMo...|2022-09-07 09:11:40|          0|[!!, Daily, Updat...|       31|           9|           2|                    2|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|           1558|            2|             1|[{[205, 211], Nep...|2022-01-25 01:00:20|          2|[!!Fresh, Vegetab...|       34|           6|           0|                    2|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|            858|            0|             0|                  []|2022-02-03 15:32:38|          0|[!00%, Organic, C...|       12|           0|           1|                    1|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|            148|            1|             2|[{[81, 87], vegan...|2022-07-18 10:31:26|          0|[!B, Mal, gute, N...|       30|           2|           0|                    1|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|            181|            3|             2|[{[135, 142], twi...|2022-02-06 13:48:33|          0|[!love, !iq, !wad...|       32|          14|           2|                    5|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|             96|            0|             1|[{[6, 25], cocina...|2022-09-25 17:30:05|          4|[!‚¨ÜÔ∏èüß°üéºüôå#cocina...|       13|          12|           0|                    1|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|            105|            0|             0|                  []|2021-12-09 21:32:21|          0|[\", 'A√ßougue, veg...|       19|           0|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|           1137|            3|             5|[{[107, 122], ket...|2022-02-13 21:02:14|          0|[\", Cacao, butter...|       20|           1|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|           1151|            0|             7|[{[86, 96], ketos...|2022-02-10 17:53:01|          0|[\", Coconut, maca...|       20|           5|           0|                    0|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|              2|            1|             0|[{[19, 37], senor...|2022-09-03 04:23:35|          0|[\", DID, YOU, KNO...|       18|          13|           0|                    0|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|             87|            0|             0|[{[92, 98], inari...|2022-07-18 20:50:34|          0|[\", Nature's, Int...|       30|          21|           2|                    0|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|           3547|            1|             0|[{[58, 65], Canva...|2022-08-02 16:30:04|          0|[\", Scarlet, Puni...|       32|          21|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|           1151|            1|             2|[{[125, 134], ket...|2022-02-09 12:15:43|          0|[\", Spanish-style...|       20|           1|           0|                    0|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|           1336|            1|             2|[{[167, 181], ket...|2022-02-26 16:40:54|          2|[\", Vegan, BLT, s...|       49|           3|           0|                    0|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|              1|            1|             0|[{[137, 156], Veg...|2022-08-04 14:36:52|          0|[\", Vegan, meinet...|       22|           1|           0|                    1|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|           1137|            1|             4|[{[121, 130], ket...|2022-02-09 19:00:20|          0|[\", Vegetable, fr...|       21|           1|           0|                    0|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|           1151|            0|             4|[{[126, 141], ket...|2022-02-10 22:02:09|          0|[\", chocolate, br...|       23|           4|           0|                    0|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|            240|            0|             1|                  []|2022-08-01 15:44:25|          2|[\", nuna, \"\\n\", T...|       20|           0|           0|                    0|\n",
      "+--------------------+---------------+--------------------+--------------------+---------------+-------------+--------------+--------------------+-------------------+-----------+--------------------+---------+------------+------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "twitter_df = df.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emojis.decode(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "\n",
    "    #translate to English\n",
    "    translator = Translator(to_lang=\"en\")\n",
    "    cleaned_string = translator.translate(cleaned_string)\n",
    "\n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4ab2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb\n",
    "\n",
    "https://www.johnsnowlabs.com/spark-nlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start(m1=True)\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark NLP pipeline\n",
    "\n",
    "## Basic NLP pipeline\n",
    "\n",
    "DocumentAssembler converts data into Spark NLP annotation format that can be used by Spark NLP annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "                .setInputCol(\"cleaned_text\") \\\n",
    "                .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize the data with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean out the data and lower it with Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with LemmatizerModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[ ‚Äî ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP doesn't provide stop word list, hence, we will use nltk package to download stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with NGramGenerator in Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP. We can access the model with PerceptronModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "[ \\ ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3,9 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import PerceptronModel\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'lemmatized']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to input everything into a pipeline. Pipeline functionality is accessible with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 ngrammer,\n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = twitter_df.select(\"cleaned_text\")\n",
    "\n",
    "processed_review = pipeline.fit(review_text).transform(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|this afternoon li...|[afternoon, lives...|[this, afternoon,...|[DT, NN, VBG, JJ,...|\n",
      "|sweet potato amp ...|[sweet, potato, a...|[sweet, potato, a...|[JJ, NN, NN, NN, ...|\n",
      "|creamy mushroom b...|[creamy, mushroom...|[creamy, mushroom...|[NN, NN, NN, NN, ...|\n",
      "|polypieter veggie...|[polypieter, vegg...|[polypieter, vegg...|[NN, NN, VB, NN, ...|\n",
      "|enriched with nat...|[enrich, natural,...|[enrich, with, na...|[NN, IN, JJ, NN, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have our data in a form of unigrams that are lemmatized, with no stop words in there. I think it is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let's filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let's restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don't use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
    "\n",
    "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing.\n",
    "\n",
    "Let's start this processing. First, we need to join all our POS tags obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize our POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate n-grams from them in the same way we did that for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are ready to get POS tags ngrams with Finisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create this new Spark NLP pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and again fit it and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what kind of data we have to operate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are our word n-grams with their corresponding pos n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     finished_ngrams| finished_pos_ngrams|\n",
      "+--------------------+--------------------+\n",
      "|[this, afternoon,...|[DT, NN, VBG, JJ,...|\n",
      "|[sweet, potato, a...|[JJ, NN, NN, NN, ...|\n",
      "|[creamy, mushroom...|[NN, NN, NN, NN, ...|\n",
      "|[polypieter, vegg...|[NN, NN, VB, NN, ...|\n",
      "|[enrich, with, na...|[NN, IN, JJ, NN, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
    "                                                              F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how our filtered unigrams look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|                 [sweet, potato, amp, courgette, fritter, yoghurt, dip, vegan, plantbased]|\n",
      "|                           [creamy, mushroom, bucatini, vegan, veganforlife, forevervegan]|\n",
      "|[polypieter, veggie, al, super, vegan, quasi, onmogelijk, en, je, krijgt, onvermijdelij...|\n",
      "|                             [enrich, origin, ingredient, certify, vegan, wink, fragrance]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the function on word and POS n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
    "                                                                    F.col('finished_pos_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what we get after filtering for n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                           filtered_ngrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[relax_conversation, book_dontmesswithasilverback, dontmesswithasilverback_govegan, gov...|\n",
      "|[sweet_potato, potato_amp, amp_courgette, courgette_fritter, mint_yoghurt, yoghurt_dip,...|\n",
      "|[creamy_mushroom, mushroom_bucatini, bucatini_vegan, vegan_veganforlife, veganforlife_f...|\n",
      "|[polypieter_veggie, be_al, al_super, super_vegan, be_quasi, quasi_onmogelijk, onmogelij...|\n",
      "|[natural_origin, origin_ingredient, vegan_society, love_wink, wink_fragrance, fragrance...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(F.col('filtered_unigrams'), \n",
    "                                                      F.col('filtered_ngrams')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our final look of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                     final|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[livestreaming, conversation, asefrid, book, govegan, livelikeagorilla, veganfortheanim...|\n",
      "|[sweet, potato, amp, courgette, fritter, yoghurt, dip, vegan, plantbased, sweet_potato,...|\n",
      "|[creamy, mushroom, bucatini, vegan, veganforlife, forevervegan, creamy_mushroom, mushro...|\n",
      "|[polypieter, veggie, al, super, vegan, quasi, onmogelijk, en, je, krijgt, onvermijdelij...|\n",
      "|[enrich, origin, ingredient, certify, vegan, wink, fragrance, natural_origin, origin_in...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('final').limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set to vectorization of our data. First, we will proceed with TF (term frequency) vectorization with CountVectorizer in PySpark. We fit tf dictionary and then transform the data to vectors of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get TF results, we can account for words that are frequent for all the documents. We can use IDF (inverse document frequency) to lower score of such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 16:35:20 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 16:46:58 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to model topics in our data with LDA (Latent Dirichlet Allocation). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:============>                                            (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 16:49:57 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 16:59:59 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:00 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:01 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/11/28 17:00:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:08 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:09 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:09 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 180:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:12 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:13 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:13 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 189:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:15 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:16 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:17 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 198:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:19 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:19 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:20 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:21 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:22 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:22 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:============>                                            (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 225:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:26 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:26 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:27 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:28 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:28 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:28 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 243:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:30 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:30 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/11/28 17:00:30 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 252:=========================>                               (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 17:00:32 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 6\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of top words per topic we would like to see and extract the words with our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|    0|                               [organic, keto, vegan, make, amp, rofl, joy, im, get, diet]|\n",
      "|    1|[blue_diamond, small_blue_diamond, small_blue, small, blue, diamond, vegan, diamond_sma...|\n",
      "|    2|                               [vegan, und, ich, die, soap, ist, das, nicht, der, organic]|\n",
      "|    3|                             [que, vegano, en, organic, vegan, el, face, de, spice, heart]|\n",
      "|    4|   [vegan, organic, vegano, healthyfood, food, healthy, healthylifestyle, eu, health, que]|\n",
      "|    5|                [vegan, organic, go, go_vegan, tea, food, animal, keto, check, check_mark]|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "organic\n",
      "keto\n",
      "vegan\n",
      "make\n",
      "amp\n",
      "rofl\n",
      "joy\n",
      "im\n",
      "get\n",
      "diet\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "blue_diamond\n",
      "small_blue_diamond\n",
      "small_blue\n",
      "small\n",
      "blue\n",
      "diamond\n",
      "vegan\n",
      "diamond_small\n",
      "organic\n",
      "keto\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "vegan\n",
      "und\n",
      "ich\n",
      "die\n",
      "soap\n",
      "ist\n",
      "das\n",
      "nicht\n",
      "der\n",
      "organic\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "que\n",
      "vegano\n",
      "en\n",
      "organic\n",
      "vegan\n",
      "el\n",
      "face\n",
      "de\n",
      "spice\n",
      "heart\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "vegan\n",
      "organic\n",
      "vegano\n",
      "healthyfood\n",
      "food\n",
      "healthy\n",
      "healthylifestyle\n",
      "eu\n",
      "health\n",
      "que\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "vegan\n",
      "organic\n",
      "go\n",
      "go_vegan\n",
      "tea\n",
      "food\n",
      "animal\n",
      "keto\n",
      "check\n",
      "check_mark\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'describeTopics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_59316/2720286453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribeTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \"\"\"\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'describeTopics'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
