{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning for Google Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:37:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/26 11:37:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "import os\n",
    "path_json = \".././../data/Topic_vegan/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:38:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:55 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>139</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "      <td>655</td>\n",
       "      <td>3837</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karen Reed üå∏</td>\n",
       "      <td>kandk670</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>@trudiebakescake Organic coconut oil in a jar ...</td>\n",
       "      <td>711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>„Éè„É´):)</td>\n",
       "      <td>patlnwza55</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>236</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>655</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ÔæåÔΩßÔæôÔæÑÔæûÔæó„Å£Â≠êorganicÊúâÊ©üüíôüíª</td>\n",
       "      <td>organic_yusai</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>„Éû„Ç∏„Åß„Éî„É≥„ÉÅÂä©„Åë„Å¶Ëá™Ëª¢Ëªä„Ç¨„Ç¨„Ç¨„Ç¨</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "      <td>139</td>\n",
       "      <td>3837</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ÔΩ°‚óï‚Äø‚óïÔΩ°ùë±ùíÜ ùíï'ùíÇùíäùíéùíÜ üê∂üß°‚ú®</td>\n",
       "      <td>MyFnlovely97</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>245</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sang‚Ñ¢</td>\n",
       "      <td>asan_gk</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @NotechAna: Am I the only one who types in ...</td>\n",
       "      <td>2065</td>\n",
       "      <td>374</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trysia ):)‚ñ™Ô∏énever let me go‚ñ™Ô∏é</td>\n",
       "      <td>Winnie_thephuu</td>\n",
       "      <td>Mon Apr 04 10:09:48 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>379</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name     screen_name  \\\n",
       "0                        „ÅÆ„Çä/Nori      nori_k_629   \n",
       "1                          alice   myn4meizalize   \n",
       "2                   Karen Reed üå∏        kandk670   \n",
       "3                          „Éè„É´):)      patlnwza55   \n",
       "4                          alice   myn4meizalize   \n",
       "5            ÔæåÔΩßÔæôÔæÑÔæûÔæó„Å£Â≠êorganicÊúâÊ©üüíôüíª   organic_yusai   \n",
       "6                        „ÅÆ„Çä/Nori      nori_k_629   \n",
       "7             ÔΩ°‚óï‚Äø‚óïÔΩ°ùë±ùíÜ ùíï'ùíÇùíäùíéùíÜ üê∂üß°‚ú®    MyFnlovely97   \n",
       "8                          Sang‚Ñ¢         asan_gk   \n",
       "9  Trysia ):)‚ñ™Ô∏énever let me go‚ñ™Ô∏é  Winnie_thephuu   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Mon Apr 04 10:09:55 +0000 2022   \n",
       "1  Mon Apr 04 10:09:54 +0000 2022   \n",
       "2  Mon Apr 04 10:09:54 +0000 2022   \n",
       "3  Mon Apr 04 10:09:52 +0000 2022   \n",
       "4  Mon Apr 04 10:09:52 +0000 2022   \n",
       "5  Mon Apr 04 10:09:52 +0000 2022   \n",
       "6  Mon Apr 04 10:09:50 +0000 2022   \n",
       "7  Mon Apr 04 10:09:50 +0000 2022   \n",
       "8  Mon Apr 04 10:09:50 +0000 2022   \n",
       "9  Mon Apr 04 10:09:48 +0000 2022   \n",
       "\n",
       "                                           full_text  followers_count  \\\n",
       "0  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              139   \n",
       "1  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...              655   \n",
       "2  @trudiebakescake Organic coconut oil in a jar ...              711   \n",
       "3  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              236   \n",
       "4  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              655   \n",
       "5                                   „Éû„Ç∏„Åß„Éî„É≥„ÉÅÂä©„Åë„Å¶Ëá™Ëª¢Ëªä„Ç¨„Ç¨„Ç¨„Ç¨              291   \n",
       "6  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...              139   \n",
       "7  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              245   \n",
       "8  RT @NotechAna: Am I the only one who types in ...             2065   \n",
       "9  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...              379   \n",
       "\n",
       "   retweet_count  favorite_count hashtags  \n",
       "0           3582               0       []  \n",
       "1           3837               0       []  \n",
       "2              0               0       []  \n",
       "3           3582               0       []  \n",
       "4           3582               0       []  \n",
       "5              0               1       []  \n",
       "6           3837               0       []  \n",
       "7           3582               0       []  \n",
       "8            374               0       []  \n",
       "9           3582               0       []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"),\n",
    "                    F.col(\"user.followers_count\"),\n",
    "                    F.col(\"retweet_count\"),\n",
    "                    F.col(\"favorite_count\"),\n",
    "                    F.col(\"entities.hashtags\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====>                                                  (16 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_23 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_18 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:======>                                                (24 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_29 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_31 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_32 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_34 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=========>                                             (33 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_39 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:40:58 WARN MemoryStore: Not enough space to cache rdd_30_40 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==========>                                            (38 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_43 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_48 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:============>                                          (47 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_53 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_55 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_54 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_59 in memory! (computed 3.3 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============>                                        (53 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_62 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_61 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_63 in memory.\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_63 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:40:59 WARN MemoryStore: Not enough space to cache rdd_30_64 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=================>                                     (64 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:00 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_70 in memory.\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_69 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_70 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_71 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_72 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================>                                  (73 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_76 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_83 in memory.\n",
      "22/11/26 11:41:00 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_84 in memory.\n",
      "22/11/26 11:41:00 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_81 in memory.\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_83 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_84 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_81 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_82 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=======================>                               (84 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_92 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_93 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:00 WARN MemoryStore: Not enough space to cache rdd_30_94 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_95 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_97 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=========================>                             (93 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_101 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_102 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_105 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_104 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_106 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================>                          (100 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:01 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_110 in memory.\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_110 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_111 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_115 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_113 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:============================>                         (105 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:01 WARN MemoryStore: Not enough space to cache rdd_30_118 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============================>                       (114 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_127 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_128 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:02 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_125 in memory.\n",
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_129 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_125 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_130 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===================================>                  (130 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_137 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:02 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_143 in memory.\n",
      "22/11/26 11:41:02 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_142 in memory.\n",
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_142 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================>                 (136 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_144 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:02 WARN MemoryStore: Not enough space to cache rdd_30_143 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_147 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:======================================>               (141 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:03 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_150 in memory.\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_150 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_152 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_153 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=======================================>              (146 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_154 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==========================================>           (157 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_160 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_159 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_158 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_162 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_161 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_165 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:03 WARN MemoryStore: Not enough space to cache rdd_30_168 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_171 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================================>        (168 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_175 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_177 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_176 in memory! (computed 3.3 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:================================================>     (179 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_181 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_183 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_186 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_185 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_187 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=================================================>    (182 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_194 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_195 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:04 WARN MemoryStore: Not enough space to cache rdd_30_197 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+---------------+-------------+--------------+--------------------+-------------------+\n",
      "|                name|    screen_name|          created_at|           full_text|followers_count|retweet_count|favorite_count|            hashtags|    post_created_at|\n",
      "+--------------------+---------------+--------------------+--------------------+---------------+-------------+--------------+--------------------+-------------------+\n",
      "| Follow the Vegans ‚ìã|  vegan_v_vegan|Sat May 14 00:55:...|!\\n#vegan #GoVega...|           4285|            1|             6|[{[2, 8], vegan},...|2022-05-14 00:55:33|\n",
      "|üå±Veg-In-Out Mark...| veginoutmarket|Sat Jan 15 07:17:...|! We will be open...|            947|            0|             0|[{[69, 79], vegan...|2022-01-15 07:17:18|\n",
      "|         Mix 93.8 FM|       Mix938FM|Wed Sep 07 09:11:...|!! Daily Updates ...|          10745|            3|             8|[{[52, 67], TheMo...|2022-09-07 09:11:40|\n",
      "|         GreenGrowth|  Greengrowth01|Tue Jan 25 01:00:...|!!Fresh Vegetable...|           1558|            2|             1|[{[205, 211], Nep...|2022-01-25 01:00:20|\n",
      "|           EJBroyles|      EJBroyles|Thu Feb 03 15:32:...|!00% Organic Cott...|            858|            0|             0|                  []|2022-02-03 15:32:38|\n",
      "|             Caroona|       Caroona_|Mon Jul 18 10:31:...|!B Mal gute Nachr...|            148|            1|             2|[{[81, 87], vegan...|2022-07-18 10:31:26|\n",
      "|             Bugatea|       Bugateax|Sun Feb 06 13:48:...|!love !iq !waddup...|            181|            3|             2|[{[135, 142], twi...|2022-02-06 13:48:33|\n",
      "|‚ú®‚íπ‚ìî‚ì¢‚ì£‚ìî‚ìõ‚ìõ‚ìû‚ì¢üå∏‚ìà‚ìê‚ìö‚ì§‚ì°...|DestellosSakura|Sun Sep 25 17:30:...|!‚¨ÜÔ∏èüß°üéºüôå#cocinav...|             96|            0|             1|[{[6, 25], cocina...|2022-09-25 17:30:05|\n",
      "|       Meia noite üåÉ| joaopedro00021|Thu Dec 09 21:32:...|\" 'A√ßougue vegano...|            105|            0|             0|                  []|2021-12-09 21:32:21|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Sun Feb 13 21:02:...|\" Cacao butter co...|           1137|            3|             5|[{[107, 122], ket...|2022-02-13 21:02:14|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 17:53:...|\" Coconut macaroo...|           1151|            0|             7|[{[86, 96], ketos...|2022-02-10 17:53:01|\n",
      "|  Senorita Cosmetics| senorita_amigo|Sat Sep 03 04:23:...|\" DID YOU KNOW ?\"...|              2|            1|             0|[{[19, 37], senor...|2022-09-03 04:23:35|\n",
      "|       Jacob Dorsett| dorsett_tattoo|Mon Jul 18 20:50:...|\" Nature's Interc...|             87|            0|             0|[{[92, 98], inari...|2022-07-18 20:50:34|\n",
      "|              taiche|       taicheUK|Tue Aug 02 16:30:...|\" Scarlet Punica ...|           3547|            1|             0|[{[58, 65], Canva...|2022-08-02 16:30:04|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 12:15:...|\" Spanish-style s...|           1142|            1|             2|[{[125, 134], ket...|2022-02-09 12:15:43|\n",
      "|       Low Carb Jiji| EatHealthyGood|Sat Feb 26 16:40:...|\" Vegan BLT sandw...|           1336|            1|             2|[{[167, 181], ket...|2022-02-26 16:40:54|\n",
      "|   Mittelalter Gouda|mittelaltermark|Thu Aug 04 14:36:...|\" Vegan meinetweg...|              1|            1|             0|[{[137, 156], Veg...|2022-08-04 14:36:52|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Wed Feb 09 19:00:...|\" Vegetable fritt...|           1137|            1|             4|[{[121, 130], ket...|2022-02-09 19:00:20|\n",
      "|      Low Carb Jiji¬Æ| EatHealthyGood|Thu Feb 10 22:02:...|\" chocolate brown...|           1142|            0|             4|[{[126, 141], ket...|2022-02-10 22:02:09|\n",
      "|               ·¥ã·¥ÄÍú±·¥á è|    sanadoongie|Mon Aug 01 15:44:...|\" nuna \"\\n\" Thank...|            240|            0|             1|                  []|2022-08-01 15:44:25|\n",
      "+--------------------+---------------+--------------------+--------------------+---------------+-------------+--------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:======>                                                (22 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:07 WARN MemoryStore: Not enough space to cache rdd_30_29 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:07 WARN MemoryStore: Not enough space to cache rdd_30_32 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:07 WARN MemoryStore: Not enough space to cache rdd_30_33 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=======>                                               (28 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_39 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_41 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_40 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_42 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=========>                                             (35 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_49 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_50 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_47 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_53 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_54 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_52 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==============>                                        (54 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_57 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:41:08 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_30_62 in memory.\n",
      "22/11/26 11:41:08 WARN MemoryStore: Not enough space to cache rdd_30_62 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=================>                                     (65 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:41:09 WARN MemoryStore: Not enough space to cache rdd_30_63 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:09 WARN MemoryStore: Not enough space to cache rdd_30_71 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:41:09 WARN MemoryStore: Not enough space to cache rdd_30_69 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:41:09 WARN MemoryStore: Not enough space to cache rdd_30_72 in memory! (computed 3.5 MiB so far)\n",
      "[215.492s][warning][gc,alloc] Executor task launch worker for task 73.0 in stage 16.0 (TID 8345): Retried waiting for GCLocker too often allocating 7219 words\n",
      "[215.495s][warning][gc,alloc] Executor task launch worker for task 70.0 in stage 16.0 (TID 8342): Retried waiting for GCLocker too often allocating 256 words\n",
      "[215.495s][warning][gc,alloc] Executor task launch worker for task 71.0 in stage 16.0 (TID 8343): Retried waiting for GCLocker too often allocating 4098 words\n",
      "[215.504s][warning][gc,alloc] Executor task launch worker for task 72.0 in stage 16.0 (TID 8344): Retried waiting for GCLocker too often allocating 4119 words\n",
      "22/11/26 11:41:09 ERROR Executor: Exception in task 73.0 in stage 16.0 (TID 8345)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2132)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1732)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2168)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1732)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2617)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2468)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2268)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1744)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:514)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:472)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1664)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:953)\n",
      "\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:1258)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1325)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "22/11/26 11:41:09 ERROR Executor: Exception in task 71.0 in stage 16.0 (TID 8343)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n",
      "\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n",
      "\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n",
      "\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/26 11:41:09 ERROR Executor: Exception in task 72.0 in stage 16.0 (TID 8344)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/26 11:41:09 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 71.0 in stage 16.0 (TID 8343),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n",
      "\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n",
      "\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n",
      "\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/26 11:41:09 WARN TaskSetManager: Lost task 71.0 in stage 16.0 (TID 8343) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n",
      "\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n",
      "\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n",
      "\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "22/11/26 11:41:09 ERROR TaskSetManager: Task 71 in stage 16.0 failed 1 times; aborting job\n",
      "22/11/26 11:41:09 WARN TaskSetManager: Lost task 76.0 in stage 16.0 (TID 8348) (192.168.1.13 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/26 11:41:09 WARN TaskSetManager: Lost task 75.0 in stage 16.0 (TID 8347) (192.168.1.13 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/26 11:41:09 WARN TaskSetManager: Lost task 69.0 in stage 16.0 (TID 8341) (192.168.1.13 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/26 11:41:09 WARN TaskSetManager: Lost task 74.0 in stage 16.0 (TID 8346) (192.168.1.13 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/26 11:41:09 WARN TaskSetManager: Lost task 68.0 in stage 16.0 (TID 8340) (192.168.1.13 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/26 11:41:09 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 72.0 in stage 16.0 (TID 8344),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/26 11:41:09 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4652/0x0000000801cd1b40@9e565ac rejected from java.util.concurrent.ThreadPoolExecutor@18e3dc26[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 8346]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/26 11:41:09 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 73.0 in stage 16.0 (TID 8345),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2132)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1732)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2168)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1732)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2617)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2468)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2268)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1744)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:514)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:472)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1664)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:953)\n",
      "\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:1258)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1325)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/12/temp_shuffle_8bdafdf1-88f1-4231-ad73-1e37f64df0fc\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/2f/temp_shuffle_265846b7-b081-400c-a219-5f6b22f354f0\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/2a/temp_shuffle_904c8d81-f634-4234-903d-019b1a813664\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/12/temp_shuffle_0ee67525-75dd-4a2e-8dc7-6fedc6146c79\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/34/temp_shuffle_5a08bb00-3268-41a2-ae58-07abb8e629b5\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/14/temp_shuffle_a99e1aea-d02b-484e-b40c-4552fea008e0\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/0a/temp_shuffle_0ea5176c-b9c5-4f41-945b-7190a461b174\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/2f/temp_shuffle_bdf947fd-20cd-4da0-85e3-7f45701a46a9\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/00/temp_shuffle_b54ca15a-9dee-47a6-abc6-6107669c68f0\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/0c/temp_shuffle_3a276b19-cd1d-4179-a8ec-bb9710d44085\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/2d/temp_shuffle_6cb54294-b87d-40ae-9152-2430620d17e1\n",
      "22/11/26 11:41:09 WARN DiskBlockObjectWriter: Error deleting /private/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/blockmgr-15e39c2c-c438-4c6c-8bcd-471354579f11/2f/temp_shuffle_6b21a01e-2ef1-44dc-acf7-b2f24272e651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================>                                    (67 + 9) / 200]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o95.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 71 in stage 16.0 failed 1 times, most recent failure: Lost task 71.0 in stage 16.0 (TID 8343) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_43015/653067393.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"full_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"screen_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#df.printSchema()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#df.count() #1340938\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o95.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 71 in stage 16.0 failed 1 times, most recent failure: Lost task 71.0 in stage 16.0 (TID 8343) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat net.jpountz.lz4.LZ4BlockOutputStream.<init>(LZ4BlockOutputStream.java:102)\n\tat org.apache.spark.io.LZ4CompressionCodec.compressedOutputStream(CompressionCodec.scala:144)\n\tat org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:159)\n\tat org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:163)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1512/0x00000008014cb510.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates().cache()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "#df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "\n",
    "df.show()             \n",
    "#df.printSchema()\n",
    "#df.count() #1340938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to count hashtags\n",
    "def get_hashtags(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"#\" in word:\n",
    "            counter += 1\n",
    "    return(counter) \n",
    "\n",
    "# define function to count mentions\n",
    "def get_mentions(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"@\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count exclamation marks\n",
    "def get_exclamation_marks(tokenized_text):\n",
    "    counter = 0\n",
    "    for word in tokenized_text:\n",
    "        if \"!\" in word:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "# define function to count number of emojis used\n",
    "import emojis\n",
    "def emoji_counter(text):\n",
    "    nr_emojis = emojis.count(text)\n",
    "    return(nr_emojis)\n",
    "# register functions as udf\n",
    "get_hashtags_UDF = F.udf(get_hashtags, IntegerType())\n",
    "get_mentions_UDF = F.udf(get_mentions, IntegerType())\n",
    "get_exclamation_marks_UDF = F.udf(get_exclamation_marks, IntegerType())\n",
    "emoji_counter_udf = F.udf(emoji_counter, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"emoji_count\", emoji_counter_udf(\"full_text\")) \\\n",
    "                            .withColumn(\"text_tokenized\", F.split(\"full_text\", \" \")) \\\n",
    "                            .withColumn(\"num_words\", F.size(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_hashtags\", get_hashtags_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_mentions\", get_mentions_UDF(\"text_tokenized\")) \\\n",
    "                            .withColumn(\"num_exclamation_marks\", get_exclamation_marks_UDF(\"text_tokenized\"))\n",
    "twitter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_text(string):\n",
    "    \n",
    "    # define numbers\n",
    "    NUMBERS = '0123456789'\n",
    "    PUNCT = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    \n",
    "    # convert text to lower case\n",
    "    cleaned_string = string.lower()\n",
    "    \n",
    "    # remove URLS\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    \n",
    "    # replace emojis by words\n",
    "    cleaned_string = emoji.demojize(cleaned_string)\n",
    "    cleaned_string = cleaned_string.replace(\":\",\" \").replace(\"_\",\" \")\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in NUMBERS])\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = \"\".join([char for char in cleaned_string if char not in PUNCT])\n",
    "    \n",
    "    # remove words conisting of one character (or less)\n",
    "    cleaned_string = ' '.join([w for w in cleaned_string.split() if len(w) > 1])\n",
    "    \n",
    "    # return\n",
    "    return(cleaned_string) \n",
    "clean_text_udf = F.udf(clean_text, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ab2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = df.withColumn(\"cleaned_text\", clean_text_udf(F.col(\"full_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==================================>                    (125 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_127 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_124 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:01 WARN BlockManager: Persisting block rdd_24_127 to disk instead.\n",
      "22/11/26 11:13:01 WARN BlockManager: Persisting block rdd_24_124 to disk instead.\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_127 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================>                   (129 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:01 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_138 in memory.\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_131 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:01 WARN BlockManager: Persisting block rdd_24_131 to disk instead.\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_133 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:01 WARN BlockManager: Persisting block rdd_24_133 to disk instead.\n",
      "22/11/26 11:13:01 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_140 in memory.\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:01 WARN BlockManager: Persisting block rdd_24_134 to disk instead.\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_133 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:01 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:01 WARN BlockManager: Persisting block rdd_24_138 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================>                  (133 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:02 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_143 in memory.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_140 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:02 WARN BlockManager: Persisting block rdd_24_140 to disk instead.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_143 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:02 WARN BlockManager: Persisting block rdd_24_143 to disk instead.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_137 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:02 WARN BlockManager: Persisting block rdd_24_137 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================>                (140 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:02 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_148 in memory.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_151 in memory.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_145 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:02 WARN BlockManager: Persisting block rdd_24_145 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================>               (145 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_148 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:02 WARN BlockManager: Persisting block rdd_24_148 to disk instead.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_151 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:02 WARN BlockManager: Persisting block rdd_24_151 to disk instead.\n",
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_145 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==========================================>            (154 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:02 WARN MemoryStore: Not enough space to cache rdd_24_148 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_153 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_154 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_154 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_154 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_155 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_155 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_157 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_157 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_155 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_157 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_166 in memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===========================================>           (158 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_161 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_161 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_161 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_166 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_166 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===============================================>       (171 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_166 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_175 in memory.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_171 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_175 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_175 to disk instead.\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_171 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:================================================>      (175 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:03 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_175 in memory.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_176 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_176 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_178 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:03 WARN BlockManager: Persisting block rdd_24_178 to disk instead.\n",
      "22/11/26 11:13:03 WARN MemoryStore: Not enough space to cache rdd_24_175 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_178 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_176 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_181 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN BlockManager: Persisting block rdd_24_181 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>   (186 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_181 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_189 in memory.\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_189 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:04 WARN BlockManager: Persisting block rdd_24_189 to disk instead.\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_189 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_192 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN BlockManager: Persisting block rdd_24_192 to disk instead.\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_192 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_1 in memory.\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_5 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_4 in memory.\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_6 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_3 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_0 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_2 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_7 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_1 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:04 WARN MemoryStore: Not enough space to cache rdd_24_4 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_8 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_10 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_11 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_12 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_13 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_14 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_14 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_15 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_15 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_16 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_18 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_17 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_20 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_19 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_20 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_21 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_21 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_22 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_23 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_24 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_24 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======>                                                (25 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_25 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_26 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_29 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_27 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_28 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_30 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_28 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_29 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_30 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_31 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_31 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_35 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_34 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_33 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_32 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_36 in memory.\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_32 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_36 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:05 WARN MemoryStore: Not enough space to cache rdd_24_37 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_39 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_39 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_38 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=========>                                              (33 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_40 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_41 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_42 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_45 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_43 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_45 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_44 in memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==========>                                             (39 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_44 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_46 in memory! (computed 3.6 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_47 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_50 in memory! (computed 3.5 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_48 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_52 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_49 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_51 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_52 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_53 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_51 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_53 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============>                                          (48 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_55 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_54 in memory.\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_54 in memory! (computed 384.0 B so far)\n",
      "22/11/26 11:13:06 WARN MemoryStore: Not enough space to cache rdd_24_55 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============>                                          (49 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:07 WARN MemoryStore: Not enough space to cache rdd_24_63 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============================>                        (112 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:08 WARN MemoryStore: Not enough space to cache rdd_24_127 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================>                     (122 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_133 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=====================================>                 (135 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.3 MiB so far)\n",
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_145 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_148 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:========================================>              (146 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_155 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_157 in memory! (computed 3.4 MiB so far)\n",
      "22/11/26 11:13:09 WARN MemoryStore: Not enough space to cache rdd_24_154 in memory! (computed 3.4 MiB so far)\n",
      "[184.817s][warning][gc,alloc] Executor task launch worker for task 151.0 in stage 7.0 (TID 8220): Retried waiting for GCLocker too often allocating 4098 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=========================================>             (151 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:13:10 ERROR Executor: Exception in task 151.0 in stage 7.0 (TID 8220)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/26 11:13:10 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 151.0 in stage 7.0 (TID 8220),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/26 11:13:10 WARN TaskSetManager: Lost task 151.0 in stage 7.0 (TID 8220) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "22/11/26 11:13:10 ERROR TaskSetManager: Task 151 in stage 7.0 failed 1 times; aborting job\n",
      "22/11/26 11:13:10 WARN TaskSetManager: Lost task 152.0 in stage 7.0 (TID 8221) (192.168.1.13 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/26 11:13:10 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4649/0x0000000801c00eb8@2f7c590a rejected from java.util.concurrent.ThreadPoolExecutor@27285e46[Shutting down, pool size = 3, active threads = 0, queued tasks = 0, completed tasks = 8227]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o160.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 151 in stage 7.0 failed 1 times, most recent failure: Lost task 151.0 in stage 7.0 (TID 8220) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_42390/1766637516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o160.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 151 in stage 7.0 failed 1 times, most recent failure: Lost task 151.0 in stage 7.0 (TID 8220) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1523/0x00000008014ceaa8.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "twitter_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
