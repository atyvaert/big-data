{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ec65ef",
   "metadata": {},
   "source": [
    "# Data Exploration Wouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd32da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87226276-9d83-48bc-9bc3-9219a428de2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_20156/2477715280.py:12 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_20156/2477715280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# create spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# create spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    431\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_20156/2477715280.py:12 "
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "findspark.init(\"/Users/wouterdewitte/spark/\")\n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd03ea-50af-4201-96b3-5ead4b26470d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.13:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f79f8b6cd90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cba5c-ff9a-4055-84a2-024b7a67f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "import os\n",
    "path_json = \".././../data/Topic_vegan/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1fd47-245e-4ddb-a7df-e7c715ee78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.             (111 + 8) / 266]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/wouterdewitte/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 2:=======================>                               (112 + 8) / 266]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_20156/1179559656.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_json = spark.read.json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc20a0-5ccc-4fb5-b1d9-f23610061c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>のり/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:55 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...</td>\n",
       "      <td>139</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: คิดถึงกันป่าว ถ้าคิดถึงต้อง...</td>\n",
       "      <td>655</td>\n",
       "      <td>3837</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karen Reed 🌸</td>\n",
       "      <td>kandk670</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>@trudiebakescake Organic coconut oil in a jar ...</td>\n",
       "      <td>711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ハル):)</td>\n",
       "      <td>patlnwza55</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...</td>\n",
       "      <td>236</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...</td>\n",
       "      <td>655</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ﾌｧﾙﾄﾞﾗっ子organic有機💙💻</td>\n",
       "      <td>organic_yusai</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>マジでピンチ助けて自転車ガガガガ</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>のり/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: คิดถึงกันป่าว ถ้าคิดถึงต้อง...</td>\n",
       "      <td>139</td>\n",
       "      <td>3837</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>｡◕‿◕｡𝑱𝒆 𝒕'𝒂𝒊𝒎𝒆 🐶🧡✨</td>\n",
       "      <td>MyFnlovely97</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...</td>\n",
       "      <td>245</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sang™</td>\n",
       "      <td>asan_gk</td>\n",
       "      <td>Mon Apr 04 10:09:50 +0000 2022</td>\n",
       "      <td>RT @NotechAna: Am I the only one who types in ...</td>\n",
       "      <td>2065</td>\n",
       "      <td>374</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trysia ):)▪︎never let me go▪︎</td>\n",
       "      <td>Winnie_thephuu</td>\n",
       "      <td>Mon Apr 04 10:09:48 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...</td>\n",
       "      <td>379</td>\n",
       "      <td>3582</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name     screen_name  \\\n",
       "0                        のり/Nori      nori_k_629   \n",
       "1                          alice   myn4meizalize   \n",
       "2                   Karen Reed 🌸        kandk670   \n",
       "3                          ハル):)      patlnwza55   \n",
       "4                          alice   myn4meizalize   \n",
       "5            ﾌｧﾙﾄﾞﾗっ子organic有機💙💻   organic_yusai   \n",
       "6                        のり/Nori      nori_k_629   \n",
       "7             ｡◕‿◕｡𝑱𝒆 𝒕'𝒂𝒊𝒎𝒆 🐶🧡✨    MyFnlovely97   \n",
       "8                          Sang™         asan_gk   \n",
       "9  Trysia ):)▪︎never let me go▪︎  Winnie_thephuu   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Mon Apr 04 10:09:55 +0000 2022   \n",
       "1  Mon Apr 04 10:09:54 +0000 2022   \n",
       "2  Mon Apr 04 10:09:54 +0000 2022   \n",
       "3  Mon Apr 04 10:09:52 +0000 2022   \n",
       "4  Mon Apr 04 10:09:52 +0000 2022   \n",
       "5  Mon Apr 04 10:09:52 +0000 2022   \n",
       "6  Mon Apr 04 10:09:50 +0000 2022   \n",
       "7  Mon Apr 04 10:09:50 +0000 2022   \n",
       "8  Mon Apr 04 10:09:50 +0000 2022   \n",
       "9  Mon Apr 04 10:09:48 +0000 2022   \n",
       "\n",
       "                                           full_text  followers_count  \\\n",
       "0  RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...              139   \n",
       "1  RT @mynameisnanon: คิดถึงกันป่าว ถ้าคิดถึงต้อง...              655   \n",
       "2  @trudiebakescake Organic coconut oil in a jar ...              711   \n",
       "3  RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...              236   \n",
       "4  RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...              655   \n",
       "5                                   マジでピンチ助けて自転車ガガガガ              291   \n",
       "6  RT @mynameisnanon: คิดถึงกันป่าว ถ้าคิดถึงต้อง...              139   \n",
       "7  RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...              245   \n",
       "8  RT @NotechAna: Am I the only one who types in ...             2065   \n",
       "9  RT @ohmpawatt: เพื่อนๆคิดถึงผมมั้ยยย ถ้าคิดถึง...              379   \n",
       "\n",
       "   retweet_count  favorite_count hashtags  \n",
       "0           3582               0       []  \n",
       "1           3837               0       []  \n",
       "2              0               0       []  \n",
       "3           3582               0       []  \n",
       "4           3582               0       []  \n",
       "5              0               1       []  \n",
       "6           3837               0       []  \n",
       "7           3582               0       []  \n",
       "8            374               0       []  \n",
       "9           3582               0       []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df_json.select(F.col(\"user.name\"),\n",
    "                    F.col(\"user.screen_name\"),\n",
    "                    F.col(\"created_at\"), \n",
    "                    F.col(\"full_text\"),\n",
    "                    F.col(\"user.followers_count\"),\n",
    "                    F.col(\"retweet_count\"),\n",
    "                    F.col(\"favorite_count\"),\n",
    "                    F.col(\"entities.hashtags\"))\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb69a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/twitter-ads-api/timezones\n",
    "# function to convert Twitter date string format\n",
    "def getDate(date):\n",
    "    if date is not None:\n",
    "        return str(datetime.strptime(date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# UDF declaration\n",
    "date_udf = F.udf(getDate, StringType())\n",
    "\n",
    "# apply udf\n",
    "df = df.withColumn('post_created_at', F.to_utc_timestamp(date_udf(\"created_at\"), \"UTC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- post_created_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop duplicates and retweets \n",
    "df = df.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates().cache()\n",
    "#sorting such when dropping later we only keep the most recent post \n",
    "df = df.sort(\"post_created_at\", ascending=False)\n",
    "#removing spam accounts \n",
    "df = df.drop_duplicates([\"full_text\", \"screen_name\"])\n",
    "                        \n",
    "df.printSchema()\n",
    "#df.count() #1340938 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ed624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=================================>                     (122 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_126 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_126 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_126 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_135 in memory.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_135 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_135 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================>               (143 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_134 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_136 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_136 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_138 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_140 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_140 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_140 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_149 in memory.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_144 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_144 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_149 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_149 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_144 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_147 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_147 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_154 in memory.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_147 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_149 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_154 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_154 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_152 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_152 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_152 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_153 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=========================================>             (152 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_158 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_158 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_155 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_155 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_158 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_160 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_160 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_160 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_164 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_164 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>          (160 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_163 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN BlockManager: Persisting block rdd_24_163 to disk instead.\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_163 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:28 WARN MemoryStore: Not enough space to cache rdd_24_164 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_166 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_166 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_170 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_170 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_166 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_169 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_169 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_170 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_172 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_172 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_171 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_171 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============================================>        (169 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_172 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_171 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_177 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_176 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_176 to disk instead.\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_177 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_176 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_177 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:================================================>      (178 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_182 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_182 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_182 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_185 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_185 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_185 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_187 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_187 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_187 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_191 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_191 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_190 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_190 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_191 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>   (189 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_190 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_192 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN BlockManager: Persisting block rdd_24_192 to disk instead.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_192 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_6 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_3 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_5 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_4 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_0 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_7 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_2 in memory.\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_1 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:29 WARN MemoryStore: Not enough space to cache rdd_24_2 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_10 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_11 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_12 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_9 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_13 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_8 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_14 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_16 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_17 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_18 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_19 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_20 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_20 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_21 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_22 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_22 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_21 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_23 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_26 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_26 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_24 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_27 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_28 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_28 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_27 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_25 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_29 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_30 in memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:====>                                                   (16 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_30 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_32 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_33 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_35 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_35 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_36 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_36 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_34 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_37 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_38 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_31 in memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:========>                                               (30 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_39 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_31 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_39 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_40 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_41 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_42 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_43 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_45 in memory.\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_45 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_44 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_47 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_46 in memory! (computed 3.6 MiB so far)\n",
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_49 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=========>                                              (35 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:30 WARN MemoryStore: Not enough space to cache rdd_24_50 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                         (108 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_126 in memory! (computed 3.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================>                    (127 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_140 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_144 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_147 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:========================================>              (149 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_149 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:32 WARN MemoryStore: Not enough space to cache rdd_24_152 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_158 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_160 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_163 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_164 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_166 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_170 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_172 in memory.\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_171 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_172 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================================>         (166 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_176 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_177 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_182 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_187 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_185 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>     (181 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_190 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_191 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:33 WARN MemoryStore: Not enough space to cache rdd_24_192 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>    (185 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/12/04 08:49:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           earliest|             latest|\n",
      "+-------------------+-------------------+\n",
      "|2021-10-12 16:08:51|2022-10-11 23:17:33|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get first post\n",
    "first_post = F.min('post_created_at').alias('earliest')\n",
    "# get latest post\n",
    "latest_post = F.max('post_created_at').alias('latest')\n",
    "# show tweet period in our dataset\n",
    "df.select(first_post, latest_post).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfa305",
   "metadata": {},
   "source": [
    "# The evolution of tweet activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf6f74",
   "metadata": {},
   "source": [
    "Look at the frequency of tweets per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_activity_monthly(keyword, df):\n",
    "    \n",
    "    df = df.filter(df.full_text.contains(keyword))\n",
    "\n",
    "    # freq_month\n",
    "    freq_month = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_month = freq_month.withColumn(\"month\", month(df[\"post_created_at\"]))\n",
    "\n",
    "    freq_month = freq_month.groupBy('year', 'month').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('year', 'month', ascending = True)\n",
    "    freq_month = freq_month.select(concat_ws('_',freq_month.year, freq_month.month)\\\n",
    "                            .alias('date'), 'freq').toPandas()\n",
    "\n",
    "    fig = px.bar(freq_month, x='date', y='freq')\n",
    "\n",
    "    # Add figure title\n",
    "    fig.update_layout(\n",
    "        title_text=\"Tweet Activity Monthly \" + keyword,\n",
    "        title_x = 0.5\n",
    "    )\n",
    "\n",
    "    # add axes\n",
    "    fig.update_xaxes(title_text=\"<b>Month</b>\")\n",
    "    fig.update_yaxes(title_text=\"<b>Amount of tweets</b>\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_66 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_64 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_68 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_61 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_69 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_70 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_71 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_72 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_73 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_75 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_77 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_67 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_76 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_62 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_78 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_80 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_79 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_82 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_81 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_80 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_74 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_83 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_84 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_85 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_87 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_86 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_89 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_88 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_90 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_91 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_94 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_93 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_95 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_98 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_97 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_96 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_99 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_100 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_101 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_102 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_105 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_92 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_103 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_105 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_104 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_106 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_108 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_109 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_110 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_107 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_126 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_65 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_134 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_140 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_144 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_152 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_152 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_149 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_158 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_153 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_147 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_158 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_160 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_163 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_164 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_171 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_166 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_171 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_172 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_176 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_177 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_182 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_177 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_185 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_190 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_192 in memory.\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_187 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_191 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_192 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_190 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:38 WARN MemoryStore: Not enough space to cache rdd_24_170 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:============>                                          (45 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_61 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_62 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_64 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_64 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_65 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_65 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_66 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_69 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_70 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_67 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_69 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_68 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================>                                   (72 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_71 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_70 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_73 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_72 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_73 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_71 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_72 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_74 in memory.\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_74 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_76 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:39 WARN MemoryStore: Not enough space to cache rdd_24_75 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_77 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_79 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_77 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_80 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_79 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_78 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_80 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_78 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_81 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_81 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_83 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_82 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_85 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_84 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_85 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_88 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_88 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_87 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_86 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_89 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_90 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_89 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_91 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_92 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_91 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_90 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_92 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_93 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_94 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_96 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_95 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_96 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_97 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_98 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_97 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_99 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_100 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_98 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_100 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_95 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=======================>                               (85 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_101 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_102 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_102 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_103 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_103 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_104 in memory! (computed 3.5 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_105 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_105 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_106 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_108 in memory.\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_107 in memory! (computed 3.4 MiB so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_108 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_110 in memory.\n",
      "[131.781s][warning][gc,alloc] Executor task launch worker for task 105.0 in stage 23.0 (TID 8593): Retried waiting for GCLocker too often allocating 21242 words\n",
      "[131.781s][warning][gc,alloc] Executor task launch worker for task 107.0 in stage 23.0 (TID 8595): Retried waiting for GCLocker too often allocating 4098 words\n",
      "[131.781s][warning][gc,alloc] Executor task launch worker for task 106.0 in stage 23.0 (TID 8594): Retried waiting for GCLocker too often allocating 14090 words\n",
      "[131.781s][warning][gc,alloc] dag-scheduler-event-loop: Retried waiting for GCLocker too often allocating 424 words\n",
      "[131.781s][warning][gc,alloc] Executor task launch worker for task 103.0 in stage 23.0 (TID 8591): Retried waiting for GCLocker too often allocating 28013 words\n",
      "[131.781s][warning][gc,alloc] Executor task launch worker for task 104.0 in stage 23.0 (TID 8592): Retried waiting for GCLocker too often allocating 27284 words\n",
      "[131.781s][warning][gc,alloc] Executor task launch worker for task 108.0 in stage 23.0 (TID 8596): Retried waiting for GCLocker too often allocating 4098 words\n",
      "[131.785s][warning][gc,alloc] Executor task launch worker for task 104.0 in stage 23.0 (TID 8592): Retried waiting for GCLocker too often allocating 7 words\n",
      "[131.785s][warning][gc,alloc] dag-scheduler-event-loop: Retried waiting for GCLocker too often allocating 3 words\n",
      "[131.785s][warning][gc,alloc] Executor task launch worker for task 103.0 in stage 23.0 (TID 8591): Retried waiting for GCLocker too often allocating 3 words\n",
      "[131.785s][warning][gc,alloc] Executor task launch worker for task 106.0 in stage 23.0 (TID 8594): Retried waiting for GCLocker too often allocating 3 words\n",
      "[131.785s][warning][gc,alloc] Executor task launch worker for task 108.0 in stage 23.0 (TID 8596): Retried waiting for GCLocker too often allocating 10243 words\n",
      "[131.785s][warning][gc,alloc] Executor task launch worker for task 107.0 in stage 23.0 (TID 8595): Retried waiting for GCLocker too often allocating 6735 words\n",
      "[131.785s][warning][gc,alloc] Executor task launch worker for task 105.0 in stage 23.0 (TID 8593): Retried waiting for GCLocker too often allocating 3 words\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_110 in memory! (computed 384.0 B so far)\n",
      "22/12/04 08:49:40 WARN MemoryStore: Not enough space to cache rdd_24_109 in memory! (computed 3.3 MiB so far)\n",
      "22/12/04 08:49:40 ERROR Executor: Exception in task 106.0 in stage 23.0 (TID 8594)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR Executor: Exception in task 105.0 in stage 23.0 (TID 8593)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR Executor: Exception in task 104.0 in stage 23.0 (TID 8592)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR Executor: Exception in task 103.0 in stage 23.0 (TID 8591)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR Executor: Exception in task 107.0 in stage 23.0 (TID 8595)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1531/0x00000008014cf6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR Executor: Exception in task 108.0 in stage 23.0 (TID 8596)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1531/0x00000008014cf6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 107.0 in stage 23.0 (TID 8595),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1531/0x00000008014cf6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 108.0 in stage 23.0 (TID 8596),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1531/0x00000008014cf6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 106.0 in stage 23.0 (TID 8594),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"dag-scheduler-event-loop\" java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 104.0 in stage 23.0 (TID 8592),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 103.0 in stage 23.0 (TID 8591),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 105.0 in stage 23.0 (TID 8593),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext\n",
      "java.lang.IllegalStateException: dag-scheduler-event-loop has already been stopped accidentally.\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskSetFailed(DAGScheduler.scala:366)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.abort(TaskSetManager.scala:990)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3(TaskSchedulerImpl.scala:293)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$3$adapted(TaskSchedulerImpl.scala:292)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2(TaskSchedulerImpl.scala:292)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$cancelTasks$2$adapted(TaskSchedulerImpl.scala:291)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:291)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cancelRunningIndependentStages$2(DAGScheduler.scala:2649)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cancelRunningIndependentStages(DAGScheduler.scala:2635)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR DAGScheduler: No stages registered for job 10\n",
      "22/12/04 08:49:40 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job 10 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1067)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1066)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:2878)\n",
      "\tat org.apache.spark.util.EventLoop.post(EventLoop.scala:107)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.taskStarted(DAGScheduler.scala:296)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:546)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:483)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:459)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:589)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:584)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:584)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:557)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:557)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@749d6e21 rejected from java.util.concurrent.ThreadPoolExecutor@67623e64[Shutting down, pool size = 3, active threads = 3, queued tasks = 0, completed tasks = 8596]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:305)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 WARN TaskSetManager: Lost task 107.0 in stage 23.0 (TID 8595) (192.168.1.13 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:149)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:151)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:159)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:306)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1531/0x00000008014cf6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "22/12/04 08:49:40 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@62e01833 rejected from java.util.concurrent.ThreadPoolExecutor@21adae64[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 8592]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:819)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/04 08:49:40 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@657d4279 rejected from java.util.concurrent.ThreadPoolExecutor@21adae64[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 8592]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:819)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o126.collectToPython.\n: org.apache.spark.SparkException: Job 10 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_20017/1862784111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_activity_monthly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"healthylifestyle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_20017/1138862505.py\u001b[0m in \u001b[0;36mtweet_activity_monthly\u001b[0;34m(keyword, df)\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count(full_text)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"freq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     freq_month = freq_month.select(concat_ws('_',freq_month.year, freq_month.month)\\\n\u001b[0m\u001b[1;32m     13\u001b[0m                             .alias('date'), 'freq').toPandas()\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \"\"\"\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o126.collectToPython.\n: org.apache.spark.SparkException: Job 10 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "tweet_activity_monthly(\"healthylifestyle\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5f47d",
   "metadata": {},
   "source": [
    "Look at the frequency of tweets per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45827219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_activity_weekly(keyword, df):\n",
    "    df = df.filter(df.full_text.contains(keyword))\n",
    "\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select(concat_ws('_',freq_week.year, freq_week.week)\\\n",
    "                            .alias('date'), 'freq').toPandas()\n",
    "\n",
    "    fig = px.line(freq_week, x='date', y='freq')\n",
    "\n",
    "    # Add figure title\n",
    "    fig.update_layout(\n",
    "        title_text=\"Tweet Activity Weekly\",\n",
    "        title_x = 0.5\n",
    "    )\n",
    "\n",
    "    # add axes\n",
    "    fig.update_xaxes(title_text=\"<b>Week</b>\")\n",
    "    fig.update_yaxes(title_text=\"<b>Amount of tweets</b>\")\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754da4c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error while sending or receiving.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             raise Py4JNetworkError(\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \"Error while sending\", e, proto.ERROR_ON_SEND)\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_19903/664389679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_activity_weekly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vegan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/x2/hymzscwj7pg_t42hpfwjm20m0000gn/T/ipykernel_19903/1022886521.py\u001b[0m in \u001b[0;36mtweet_activity_weekly\u001b[0;34m(keyword, df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtweet_activity_weekly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfreq_week\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"post_created_at\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfreq_week\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_week\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'week'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweekofyear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_created_at'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             )\n\u001b[0;32m-> 1991\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpne\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exception while sending command.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 logging.exception(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "tweet_activity_weekly(\"vegan\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc21078",
   "metadata": {},
   "source": [
    "# The evolution of tweet engagement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_weekly(keyword, df):\n",
    "    df = df.filter(df.full_text.contains(keyword))\n",
    "\n",
    "    eng_weekly = df.withColumn('year', year(df[\"post_created_at\"]))\n",
    "    eng_weekly = eng_weekly.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    eng_weekly = eng_weekly.groupBy(\"year\", \"week\") \\\n",
    "                        .agg(sum(\"retweet_count\").alias(\"retweets\"), sum(\"favorite_count\").alias(\"favorites\")) \\\n",
    "                        .sort(\"year\", \"week\", ascending = True)\n",
    "\n",
    "    eng_weekly = eng_weekly.select(concat_ws('_',eng_weekly.year, eng_weekly.week)\\\n",
    "                            .alias('week'), 'retweets', 'favorites').toPandas()\n",
    "\n",
    "    fig = px.line(eng_weekly, x='week', y=eng_weekly.columns[1:3])\n",
    "\n",
    "    # Add figure title\n",
    "    fig.update_layout(\n",
    "        title_text=\"Retweets vs favorites\",\n",
    "        title_x = 0.5\n",
    "    )\n",
    "\n",
    "    # add axes\n",
    "    fig.update_xaxes(title_text=\"<b>Week</b>\")\n",
    "    fig.update_yaxes(title_text=\"<b>Amount</b>\")\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb0143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "hovertemplate": "variable=retweets<br>week=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "retweets",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "retweets",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2021_41",
          "2021_42",
          "2021_43",
          "2021_44",
          "2021_46",
          "2021_47",
          "2021_48",
          "2021_49",
          "2021_50",
          "2021_51",
          "2021_52",
          "2022_1",
          "2022_2",
          "2022_3",
          "2022_4",
          "2022_5",
          "2022_6",
          "2022_7",
          "2022_8",
          "2022_9",
          "2022_10",
          "2022_11",
          "2022_12",
          "2022_13",
          "2022_14",
          "2022_15",
          "2022_16",
          "2022_17",
          "2022_18",
          "2022_19",
          "2022_20",
          "2022_21",
          "2022_22",
          "2022_23",
          "2022_24",
          "2022_25",
          "2022_26",
          "2022_27",
          "2022_28",
          "2022_29",
          "2022_30",
          "2022_31",
          "2022_32",
          "2022_33",
          "2022_34",
          "2022_35",
          "2022_36",
          "2022_37",
          "2022_38",
          "2022_39",
          "2022_40",
          "2022_41",
          "2022_52"
         ],
         "xaxis": "x",
         "y": [
          135,
          172,
          27699,
          36070,
          488,
          504,
          92,
          62800,
          6506,
          3073,
          9869,
          5723,
          15063,
          12231,
          10071,
          21326,
          10587,
          15860,
          8775,
          14278,
          51778,
          7934,
          12108,
          16612,
          399,
          15022,
          10057,
          45496,
          10790,
          17302,
          5208,
          16968,
          7626,
          66,
          4243,
          16581,
          79813,
          21082,
          11404,
          16165,
          42477,
          34359,
          12392,
          17688,
          1637,
          18284,
          13778,
          7080,
          783,
          148,
          1982,
          6754,
          3481
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=favorites<br>week=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "favorites",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "favorites",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2021_41",
          "2021_42",
          "2021_43",
          "2021_44",
          "2021_46",
          "2021_47",
          "2021_48",
          "2021_49",
          "2021_50",
          "2021_51",
          "2021_52",
          "2022_1",
          "2022_2",
          "2022_3",
          "2022_4",
          "2022_5",
          "2022_6",
          "2022_7",
          "2022_8",
          "2022_9",
          "2022_10",
          "2022_11",
          "2022_12",
          "2022_13",
          "2022_14",
          "2022_15",
          "2022_16",
          "2022_17",
          "2022_18",
          "2022_19",
          "2022_20",
          "2022_21",
          "2022_22",
          "2022_23",
          "2022_24",
          "2022_25",
          "2022_26",
          "2022_27",
          "2022_28",
          "2022_29",
          "2022_30",
          "2022_31",
          "2022_32",
          "2022_33",
          "2022_34",
          "2022_35",
          "2022_36",
          "2022_37",
          "2022_38",
          "2022_39",
          "2022_40",
          "2022_41",
          "2022_52"
         ],
         "xaxis": "x",
         "y": [
          1368,
          622,
          221909,
          242310,
          1617,
          1651,
          612,
          516922,
          32790,
          15476,
          41891,
          22536,
          60010,
          45939,
          39573,
          73124,
          66028,
          109857,
          45466,
          138557,
          389645,
          27876,
          40562,
          51314,
          1344,
          65560,
          62873,
          346581,
          45388,
          73536,
          26562,
          91075,
          31759,
          371,
          17448,
          69097,
          557139,
          90070,
          45134,
          72289,
          330652,
          341616,
          48573,
          73022,
          7821,
          84196,
          66986,
          28147,
          2172,
          366,
          9176,
          22002,
          15252
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Retweets vs favorites",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Week</b>"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Amount</b>"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_weekly(\"vegan\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbcc798",
   "metadata": {},
   "source": [
    "# The level of social media activity vs level of engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b8746",
   "metadata": {},
   "source": [
    "Engagement on Twitter is measured by the number of retweets, follows, replies, favorites, and other people’s reactions to your tweets, including the clicks on the links and hashtags in those tweets. Your Twitter engagement rate is your engagement figure divided by the number of impressions on the tweet.\n",
    "\n",
    "According to our 2022 Social Media Industry Benchmark Report, the overall median engagement rate on Twitter is 0.037%. This is the rate for brands across all industries, from fashion to nonprofits. On average, these brands are also posting about 5 times per week.\n",
    "\n",
    "https://www.rivaliq.com/blog/good-engagement-rate-twitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95189cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_rate(keyword, df):\n",
    "    df = df.filter(df.full_text.contains(keyword))\n",
    "\n",
    "    eng_rate = df.withColumn('eng_rate', ((df['favorite_count'] + df['retweet_count'])/df['followers_count']))\n",
    "\n",
    "    eng_rate = eng_rate.withColumn('year', year(df[\"post_created_at\"]))\n",
    "    eng_rate = eng_rate.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    eng_rate_weekly = eng_rate.groupBy(\"year\", \"week\") \\\n",
    "                        .agg(avg(\"eng_rate\").alias(\"eng_rate\")) \\\n",
    "                        .sort(\"year\",\"week\", ascending = True)\n",
    "    eng_rate_weekly = eng_rate_weekly.select(concat_ws('_',eng_rate_weekly.year, eng_rate_weekly.week)\\\n",
    "                            .alias('week'), 'eng_rate').toPandas()\n",
    "\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select(concat_ws('_',freq_week.year, freq_week.week)\\\n",
    "                            .alias('week'), 'freq').toPandas()\n",
    "\n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Add traces\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x = freq_week[\"week\"],y = freq_week[\"freq\"], name=\"Amount of tweets\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x = eng_rate_weekly[\"week\"], y = eng_rate_weekly[\"eng_rate\"], name=\"Engagement rate\"),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    # Add figure title\n",
    "    fig.update_layout(\n",
    "        title_text=\"Amount of tweets compared to engagement rate\",\n",
    "        title_x = 0.5,\n",
    "        autosize = True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"<b>week</b>\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"<b>Amount</b> of tweets\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Engagement</b> rate\", secondary_y=True)\n",
    "\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "name": "Amount of tweets",
         "type": "scatter",
         "x": [
          "2021_41",
          "2021_42",
          "2021_43",
          "2021_44",
          "2021_46",
          "2021_47",
          "2021_48",
          "2021_49",
          "2021_50",
          "2021_51",
          "2021_52",
          "2022_1",
          "2022_2",
          "2022_3",
          "2022_4",
          "2022_5",
          "2022_6",
          "2022_7",
          "2022_8",
          "2022_9",
          "2022_10",
          "2022_11",
          "2022_12",
          "2022_13",
          "2022_14",
          "2022_15",
          "2022_16",
          "2022_17",
          "2022_18",
          "2022_19",
          "2022_20",
          "2022_21",
          "2022_22",
          "2022_23",
          "2022_24",
          "2022_25",
          "2022_26",
          "2022_27",
          "2022_28",
          "2022_29",
          "2022_30",
          "2022_31",
          "2022_32",
          "2022_33",
          "2022_34",
          "2022_35",
          "2022_36",
          "2022_37",
          "2022_38",
          "2022_39",
          "2022_40",
          "2022_41",
          "2022_52"
         ],
         "xaxis": "x",
         "y": [
          244,
          456,
          31989,
          36214,
          1142,
          489,
          299,
          53100,
          6253,
          1487,
          6801,
          3200,
          9993,
          7101,
          7174,
          9506,
          7442,
          16608,
          5261,
          11351,
          46742,
          4134,
          3695,
          6378,
          548,
          6684,
          9091,
          36307,
          4805,
          7328,
          3957,
          12433,
          5092,
          114,
          3776,
          8530,
          52683,
          11807,
          5655,
          8213,
          15834,
          47748,
          6512,
          8155,
          1227,
          8976,
          7147,
          3693,
          658,
          242,
          889,
          2283,
          2518
         ],
         "yaxis": "y"
        },
        {
         "name": "Engagement rate",
         "type": "scatter",
         "x": [
          "2021_41",
          "2021_42",
          "2021_43",
          "2021_44",
          "2021_46",
          "2021_47",
          "2021_48",
          "2021_49",
          "2021_50",
          "2021_51",
          "2021_52",
          "2022_1",
          "2022_2",
          "2022_3",
          "2022_4",
          "2022_5",
          "2022_6",
          "2022_7",
          "2022_8",
          "2022_9",
          "2022_10",
          "2022_11",
          "2022_12",
          "2022_13",
          "2022_14",
          "2022_15",
          "2022_16",
          "2022_17",
          "2022_18",
          "2022_19",
          "2022_20",
          "2022_21",
          "2022_22",
          "2022_23",
          "2022_24",
          "2022_25",
          "2022_26",
          "2022_27",
          "2022_28",
          "2022_29",
          "2022_30",
          "2022_31",
          "2022_32",
          "2022_33",
          "2022_34",
          "2022_35",
          "2022_36",
          "2022_37",
          "2022_38",
          "2022_39",
          "2022_40",
          "2022_41",
          "2022_52"
         ],
         "xaxis": "x",
         "y": [
          0.019666033358323863,
          0.03135884692827253,
          0.03638277776515163,
          0.03049681646095263,
          0.018730988115928975,
          0.05217945797300487,
          0.04967689995168227,
          0.03195847369086433,
          0.03278955813495828,
          0.0373548762209969,
          0.033076834652983916,
          0.025870389669968553,
          0.02855492160122165,
          0.030023919612383317,
          0.03632583417372137,
          0.029475478538697385,
          0.028869159505786342,
          0.04555288693685604,
          0.03350929928211469,
          0.030400579740222963,
          0.03521601178623151,
          0.02905221393948036,
          0.042477446890751745,
          0.038870180042103956,
          0.025925005987217375,
          0.045614113447693505,
          0.06613919814797253,
          0.04077643151368939,
          0.04731776353664208,
          0.04539335491462106,
          0.03858614927917626,
          0.04891292530695506,
          0.04548790709913085,
          0.03486182200001785,
          0.03356463482092408,
          0.038042708874347166,
          0.04906797592403247,
          0.04188343829405353,
          0.04742374687740593,
          0.03778452449435931,
          0.061324712317287464,
          0.0460841999799049,
          0.05212795328006782,
          0.03989292151296687,
          0.04210892995470061,
          0.04285885854532912,
          0.05099544089361031,
          0.037693414013733904,
          0.04681762740542449,
          0.026514430346946026,
          0.04066841874398489,
          0.04418326219989687,
          0.02023882898101348
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Amount of tweets compared to engagement rate",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.94
         ],
         "title": {
          "text": "<b>week</b>"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Amount</b> of tweets"
         }
        },
        "yaxis2": {
         "anchor": "x",
         "overlaying": "y",
         "side": "right",
         "title": {
          "text": "<b>Engagement</b> rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_rate(\"vegan\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae8f6",
   "metadata": {},
   "source": [
    "# The volume of influencer activity vs level of engagement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03936e76",
   "metadata": {},
   "source": [
    "We define an influencer as an account with the following characteristics:\n",
    "\n",
    "- a lot of followers => follower_count > 10000\n",
    "- there is a high engagement rate on their tweets which shows their influence => er > 0.05\n",
    "- tweet frequency is high enough => freq_weekly > 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influencers(follower_count_tresh, eng_rate_tresh, freq_week_tresh):\n",
    "\n",
    "    # get all users with their amount of followers\n",
    "    influencers = df.groupBy(\"screen_name\") \\\n",
    "                    .agg(first(\"followers_count\").alias(\"followers_count\"))\n",
    "\n",
    "    # average engagement rate for each user\n",
    "    eng_rate = df.withColumn('eng_rate', ((df['favorite_count'] + df['retweet_count'])/df['followers_count']))\n",
    "\n",
    "    eng_rate_user = eng_rate.groupBy(\"screen_name\") \\\n",
    "                            .agg(avg(\"eng_rate\").alias(\"eng_rate\"))\n",
    "\n",
    "    # average freq_weekly per user\n",
    "    freq_week = df.withColumn(\"year\", year(df[\"post_created_at\"]))\n",
    "    freq_week = freq_week.withColumn('week', weekofyear('post_created_at'))\n",
    "\n",
    "    freq_week = freq_week.groupBy('screen_name', 'year', 'week').agg(countDistinct(\"full_text\"))\\\n",
    "                    .withColumnRenamed(\"count(full_text)\", \"freq\") \\\n",
    "                        .sort('screen_name', 'year', 'week', ascending = True)\n",
    "    freq_week = freq_week.select('screen_name', 'freq')\n",
    "\n",
    "    freq_week = freq_week.groupby(\"screen_name\").agg(avg(freq_week.freq).alias('freq'))\n",
    "\n",
    "    # put the data together\n",
    "    data_joined = eng_rate_user.join(influencers, \"screen_name\").join(freq_week, \"screen_name\")\n",
    "\n",
    "    # filter the data\n",
    "    data_joined = data_joined.filter((data_joined.followers_count > follower_count_tresh) & (data_joined.eng_rate > eng_rate_tresh) & (data_joined.freq > freq_week_tresh))\n",
    "    \n",
    "    # show the data\n",
    "    data_joined.show()\n",
    "    return data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc674f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 394:====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 397:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 17:40:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:40:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------+----+\n",
      "|  screen_name|           eng_rate|followers_count|freq|\n",
      "+-------------+-------------------+---------------+----+\n",
      "|  ateenyalien|0.09717012618187412|          32730| 2.5|\n",
      "|MissAudreyBoo|0.10823612246832817|          13885| 4.0|\n",
      "|       itsTFC|0.04170045922053734|          12135| 7.0|\n",
      "|  teonawrites|0.16173973342805512|          29310| 4.0|\n",
      "+-------------+-------------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "influencers = get_influencers(10000, 0.04, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.alias('df')\n",
    "influencers = influencers.alias('influencers')\n",
    "influencers_tweets = df.join(influencers, \"screen_name\").select(\"df.*\")\n",
    "influencers_tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 427:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 430:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:41:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 475:===================================================> (195 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 478:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/25 17:43:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "name": "Amount of tweets",
         "type": "scatter",
         "x": [
          "2022_17",
          "2022_26",
          "2022_30"
         ],
         "xaxis": "x",
         "y": [
          4,
          4,
          4
         ],
         "yaxis": "y"
        },
        {
         "name": "Engagement rate",
         "type": "scatter",
         "x": [
          "2022_17",
          "2022_26",
          "2022_30"
         ],
         "xaxis": "x",
         "y": [
          0.10823612246832817,
          0.118379401809093,
          0.16173973342805512
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "autosize": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Amount of tweets compared to engagement rate",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.94
         ],
         "title": {
          "text": "<b>week</b>"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "<b>Amount</b> of tweets"
         }
        },
        "yaxis2": {
         "anchor": "x",
         "overlaying": "y",
         "side": "right",
         "title": {
          "text": "<b>Engagement</b> rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_rate(\"vegan\", influencers_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83bb92",
   "metadata": {},
   "source": [
    "# Dependent variable: Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea2e30ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends-async in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (0.3.2)\n",
      "Collecting tenacity==6.0.0\n",
      "  Using cached tenacity-6.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: lxml in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from pytrends-async) (4.6.3)\n",
      "Collecting httpx==0.9.5\n",
      "  Using cached httpx-0.9.5-py2.py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from pytrends-async) (1.3.4)\n",
      "Requirement already satisfied: certifi in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (2021.10.8)\n",
      "Requirement already satisfied: h2==3.* in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (3.2.0)\n",
      "Requirement already satisfied: h11==0.8.* in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (0.8.1)\n",
      "Requirement already satisfied: idna==2.* in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (1.5.0)\n",
      "Requirement already satisfied: hstspreload in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (2022.11.1)\n",
      "Requirement already satisfied: sniffio==1.* in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from httpx==0.9.5->pytrends-async) (3.0.4)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from h2==3.*->httpx==0.9.5->pytrends-async) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from h2==3.*->httpx==0.9.5->pytrends-async) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from tenacity==6.0.0->pytrends-async) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->pytrends-async) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->pytrends-async) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/wouterdewitte/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->pytrends-async) (1.20.3)\n",
      "Installing collected packages: tenacity, httpx\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.1.0\n",
      "    Uninstalling tenacity-8.1.0:\n",
      "      Successfully uninstalled tenacity-8.1.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.13.3\n",
      "    Uninstalling httpx-0.13.3:\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "plotly 5.7.0 requires tenacity>=6.2.0, but you have tenacity 6.0.0 which is incompatible.\n",
      "googletrans 3.1.0a0 requires httpx==0.13.3, but you have httpx 0.9.5 which is incompatible.\u001b[0m\n",
      "Successfully installed httpx-0.9.5 tenacity-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytrends-async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81ecfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrendsasync\n",
    "from pytrendsasync.request import TrendReq\n",
    "pytrends = TrendReq(hl='en-US', tz=360, timeout=10, proxies=['https://34.203.233.13:80',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0938757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependent(keyword):\n",
    "    # import again to avoid Google error\n",
    "    from pytrendsasync.request import TrendReq\n",
    "    from pytrends import dailydata\n",
    "\n",
    "    # get the data\n",
    "    df = dailydata.get_daily_data(keyword, 2021, 10, 2022, 10)\n",
    "\n",
    "    # only get relevant keyword column\n",
    "    df = df[keyword].reset_index()\n",
    "\n",
    "    # create the binary dependent variable\n",
    "    df[keyword + \"_yest\"] = df[keyword].shift(1)\n",
    "    df[\"dependent_\" + keyword] = df.apply(lambda x:  1 if x[keyword] > x[keyword + \"_yest\"] else 0, axis=1) # 1 if popularity rose compared to yesterday, otherwise 0\n",
    "    \n",
    "    df = df[[\"date\", \"dependent_\" + keyword]]\n",
    "    df = df.iloc[3: , :] # drop first 2 rows because insufficient data\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "292eaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegan:2021-10-01 2021-10-31\n",
      "vegan:2021-11-01 2021-11-30\n",
      "vegan:2021-12-01 2021-12-31\n",
      "vegan:2022-01-01 2022-01-31\n",
      "vegan:2022-02-01 2022-02-28\n",
      "vegan:2022-03-01 2022-03-31\n",
      "vegan:2022-04-01 2022-04-30\n",
      "vegan:2022-05-01 2022-05-31\n",
      "vegan:2022-06-01 2022-06-30\n",
      "vegan:2022-07-01 2022-07-31\n",
      "vegan:2022-08-01 2022-08-31\n",
      "vegan:2022-09-01 2022-09-30\n",
      "vegan:2022-10-01 2022-10-31\n"
     ]
    }
   ],
   "source": [
    "df = get_dependent(\"vegan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1629143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\".././../data/Google_trends/daily_trends.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd51236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    207\n",
       "0    186\n",
       "Name: dependent_vegan, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"dependent_vegan\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e35d98e8198887147a5837b6820e4bf8d41831f6222e06e86b8679b6549872f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
