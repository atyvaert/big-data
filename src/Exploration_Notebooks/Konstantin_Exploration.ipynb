{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83badc2c-af24-49a7-860c-0b72e9411521",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing pyspark and the json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87226276-9d83-48bc-9bc3-9219a428de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:17:53 WARN Utils: Your hostname, Konstantins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.45 instead (on interface en0)\n",
      "22/11/21 13:17:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "#import findspark\n",
    "\n",
    "# initialize findspark with spark directory\n",
    "\n",
    "#ALWAYS HAVE TO BE CHANGED \n",
    "#path = \"/Users/konstantinlazarov/Desktop/Big_Data/PySpark/Week_5/spark\"\n",
    "#findspark.init(path) \n",
    "\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session \n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fd03ea-50af-4201-96b3-5ead4b26470d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.45:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x103f6e110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289cba5c-ff9a-4055-84a2-024b7a67f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to your path, for some reason I have an error \n",
    "#reading in all the files\n",
    "import os \n",
    "path_json = \".././../data/Topic_vegan/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd1fd47-245e-4ddb-a7df-e7c715ee78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:18:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.option(\"multiline\",\"true\").json(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f889059e-c5d2-40e9-83d6-f39f11cd4085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3428559"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42586f68-df4e-4f92-8673-1fc3889128e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Question 1 The number of followers of the brand "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cee5a-59b0-4cae-91d0-89bcba123894",
   "metadata": {},
   "source": [
    "Brand defined as vegan? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6dc20a0-5ccc-4fb5-b1d9-f23610061c28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>urls</th>\n",
       "      <th>symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>„ÅÆ„Çä/Nori</td>\n",
       "      <td>nori_k_629</td>\n",
       "      <td>Mon Apr 04 10:09:55 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>[]</td>\n",
       "      <td>th</td>\n",
       "      <td>0</td>\n",
       "      <td>3582</td>\n",
       "      <td>139</td>\n",
       "      <td>346</td>\n",
       "      <td>63382</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...</td>\n",
       "      <td>[]</td>\n",
       "      <td>th</td>\n",
       "      <td>0</td>\n",
       "      <td>3837</td>\n",
       "      <td>655</td>\n",
       "      <td>194</td>\n",
       "      <td>8726</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karen Reed üå∏</td>\n",
       "      <td>kandk670</td>\n",
       "      <td>Mon Apr 04 10:09:54 +0000 2022</td>\n",
       "      <td>@trudiebakescake Organic coconut oil in a jar ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>711</td>\n",
       "      <td>429</td>\n",
       "      <td>3420</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>„Éè„É´):)</td>\n",
       "      <td>patlnwza55</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>[]</td>\n",
       "      <td>th</td>\n",
       "      <td>0</td>\n",
       "      <td>3582</td>\n",
       "      <td>236</td>\n",
       "      <td>187</td>\n",
       "      <td>15655</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alice</td>\n",
       "      <td>myn4meizalize</td>\n",
       "      <td>Mon Apr 04 10:09:52 +0000 2022</td>\n",
       "      <td>RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...</td>\n",
       "      <td>[]</td>\n",
       "      <td>th</td>\n",
       "      <td>0</td>\n",
       "      <td>3582</td>\n",
       "      <td>655</td>\n",
       "      <td>194</td>\n",
       "      <td>8726</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name    screen_name                      created_at  \\\n",
       "0       „ÅÆ„Çä/Nori     nori_k_629  Mon Apr 04 10:09:55 +0000 2022   \n",
       "1         alice  myn4meizalize  Mon Apr 04 10:09:54 +0000 2022   \n",
       "2  Karen Reed üå∏       kandk670  Mon Apr 04 10:09:54 +0000 2022   \n",
       "3         „Éè„É´):)     patlnwza55  Mon Apr 04 10:09:52 +0000 2022   \n",
       "4         alice  myn4meizalize  Mon Apr 04 10:09:52 +0000 2022   \n",
       "\n",
       "                                           full_text hashtags lang  \\\n",
       "0  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...       []   th   \n",
       "1  RT @mynameisnanon: ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏ß ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á...       []   th   \n",
       "2  @trudiebakescake Organic coconut oil in a jar ...       []   en   \n",
       "3  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...       []   th   \n",
       "4  RT @ohmpawatt: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏°‡∏°‡∏±‡πâ‡∏¢‡∏¢‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á...       []   th   \n",
       "\n",
       "   favorite_count  retweet_count  followers_count  friends_count  \\\n",
       "0               0           3582              139            346   \n",
       "1               0           3837              655            194   \n",
       "2               0              0              711            429   \n",
       "3               0           3582              236            187   \n",
       "4               0           3582              655            194   \n",
       "\n",
       "   favourites_count urls symbols  \n",
       "0             63382   []      []  \n",
       "1              8726   []      []  \n",
       "2              3420   []      []  \n",
       "3             15655   []      []  \n",
       "4              8726   []      []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select interesting features\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_json_sub = df_json.select(F.col(\"user.name\"),\n",
    "                                F.col(\"user.screen_name\"),\n",
    "                                F.col(\"created_at\"), \n",
    "                                F.col(\"full_text\"),\n",
    "                                F.col(\"entities.hashtags\"),\n",
    "                                F.col(\"lang\"),\n",
    "                                F.col(\"favorite_count\"),\n",
    "                                F.col(\"retweet_count\"),\n",
    "                                F.col(\"user.followers_count\"),\n",
    "                                F.col(\"user.friends_count\"),\n",
    "                                F.col(\"user.favourites_count\"),\n",
    "                                F.col(\"entities.urls\"),\n",
    "                                F.col(\"entities.symbols\"))\n",
    "\n",
    "df_json_sub.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30615ae4-fd8b-405e-a282-267b9b895080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing basic pre-processing \n",
    "\n",
    "import pandas as pd \n",
    "#drop duplicates  \n",
    "df_json_sub = df_json_sub.filter(~F.col(\"full_text\").startswith(\"RT\"))\\\n",
    "                        .drop_duplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af0f976-0848-4a92-bf40-8ed2d62b7118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Oct 18 09:24:09 +0000 2021</td>\n",
       "      <td>Sponsor #1: https://t.co/ixCQVKJZG5 After the campaign, their organic traffic rose from 29.5k to 46k per month. 35.9% https://t.co/3G4iPPmO7Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 04 10:47:40 +0000 2022</td>\n",
       "      <td>@tewfootprints #GoGreen #Recycle #Organic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 04 10:47:03 +0000 2022</td>\n",
       "      <td>@GillwallyO @irish_organic @peatyGHG This + if you can get some hay locally that is full of natural seeds.\\n\\nWe have done this on a roughly 50 √ó 50m plot with 20 layers &amp;amp; a bunch of broilers. Cracking resukts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 04 10:43:32 +0000 2022</td>\n",
       "      <td>Great River Organic Milling Great River Organic Whole Corn 25 Pounds Pack of https://t.co/aahZzoPJ6C eBay https://t.co/az0HFosgSi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Sep 13 19:13:49 +0000 2022</td>\n",
       "      <td>ü•ïNEW Vegin' Out September 19th Vegan Delivery Menu!\\n\\nü•ë#Vegan Meal plan arrives with 3 entrees, 4 sides, soup &amp;amp; dessert - about 5 meals for two! \\n\\nüòçTake a break from cooking!\\n\\nOrder Today!\\nüëâhttps://t.co/4LkIUjjA3F\\n\\n#govegan #fitness #goals #„Éì„Éº„Ç¨„É≥ #ÎπÑÍ±¥ #organic #weightloss https://t.co/mJvDVOKfZu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Mon Oct 18 09:24:09 +0000 2021   \n",
       "1  Mon Apr 04 10:47:40 +0000 2022   \n",
       "2  Mon Apr 04 10:47:03 +0000 2022   \n",
       "3  Mon Apr 04 10:43:32 +0000 2022   \n",
       "4  Tue Sep 13 19:13:49 +0000 2022   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            full_text  \n",
       "0                                                                                                                                                                       Sponsor #1: https://t.co/ixCQVKJZG5 After the campaign, their organic traffic rose from 29.5k to 46k per month. 35.9% https://t.co/3G4iPPmO7Z  \n",
       "1                                                                                                                                                                                                                                                                           @tewfootprints #GoGreen #Recycle #Organic  \n",
       "2                                                                                               @GillwallyO @irish_organic @peatyGHG This + if you can get some hay locally that is full of natural seeds.\\n\\nWe have done this on a roughly 50 √ó 50m plot with 20 layers &amp; a bunch of broilers. Cracking resukts  \n",
       "3                                                                                                                                                                                   Great River Organic Milling Great River Organic Whole Corn 25 Pounds Pack of https://t.co/aahZzoPJ6C eBay https://t.co/az0HFosgSi  \n",
       "4  ü•ïNEW Vegin' Out September 19th Vegan Delivery Menu!\\n\\nü•ë#Vegan Meal plan arrives with 3 entrees, 4 sides, soup &amp; dessert - about 5 meals for two! \\n\\nüòçTake a break from cooking!\\n\\nOrder Today!\\nüëâhttps://t.co/4LkIUjjA3F\\n\\n#govegan #fitness #goals #„Éì„Éº„Ç¨„É≥ #ÎπÑÍ±¥ #organic #weightloss https://t.co/mJvDVOKfZu  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_json_sub.select(\"created_at\", \"full_text\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fac8d8a-cb38-4c30-8dc7-33eaf2611388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f69abdba-1d28-42d8-91ba-e938b2d12c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_users1 = df_json_sub.groupBy('name')\\\n",
    "                .agg(countDistinct('created_at'))\\\n",
    "                .withColumnRenamed('count(created_at)', 'count_tweets')\\\n",
    "                .sort('count_tweets', descending = False)\\\n",
    "                .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11b6ddd-a8a7-4b9b-85f6-ceae061045a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- count_tweets: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3f6749-f325-4f49-83c2-6c3df6e9fdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===========================>                            (97 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:08 WARN MemoryStore: Not enough space to cache rdd_24_99 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:08 WARN BlockManager: Persisting block rdd_24_99 to disk instead.\n",
      "22/11/21 13:19:08 WARN MemoryStore: Not enough space to cache rdd_24_100 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:08 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_105 in memory.\n",
      "22/11/21 13:19:08 WARN BlockManager: Persisting block rdd_24_100 to disk instead.\n",
      "22/11/21 13:19:08 WARN MemoryStore: Not enough space to cache rdd_24_105 in memory! (computed 384.0 B so far)\n",
      "22/11/21 13:19:08 WARN BlockManager: Persisting block rdd_24_105 to disk instead.\n",
      "22/11/21 13:19:08 WARN MemoryStore: Not enough space to cache rdd_24_99 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================>                          (103 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:08 WARN MemoryStore: Not enough space to cache rdd_24_107 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:08 WARN BlockManager: Persisting block rdd_24_107 to disk instead.\n",
      "22/11/21 13:19:08 WARN MemoryStore: Not enough space to cache rdd_24_109 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:08 WARN BlockManager: Persisting block rdd_24_109 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===============================>                       (114 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:09 WARN MemoryStore: Not enough space to cache rdd_24_117 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:09 WARN BlockManager: Persisting block rdd_24_117 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=================================>                     (121 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:09 WARN MemoryStore: Not enough space to cache rdd_24_125 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:09 WARN BlockManager: Persisting block rdd_24_125 to disk instead.\n",
      "22/11/21 13:19:09 WARN MemoryStore: Not enough space to cache rdd_24_125 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================>                   (129 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:09 WARN MemoryStore: Not enough space to cache rdd_24_130 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:09 WARN BlockManager: Persisting block rdd_24_130 to disk instead.\n",
      "22/11/21 13:19:09 WARN MemoryStore: Not enough space to cache rdd_24_131 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:09 WARN BlockManager: Persisting block rdd_24_131 to disk instead.\n",
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_130 in memory! (computed 3.9 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================>                 (138 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:10 WARN BlockManager: Persisting block rdd_24_138 to disk instead.\n",
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_138 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_139 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:10 WARN BlockManager: Persisting block rdd_24_139 to disk instead.\n",
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_140 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:10 WARN BlockManager: Persisting block rdd_24_140 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================>                (141 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_147 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:10 WARN BlockManager: Persisting block rdd_24_147 to disk instead.\n",
      "22/11/21 13:19:10 WARN MemoryStore: Not enough space to cache rdd_24_149 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:10 WARN BlockManager: Persisting block rdd_24_149 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:========================================>              (146 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:10 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_156 in memory.\n",
      "22/11/21 13:19:10 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_24_159 in memory.\n",
      "22/11/21 13:19:11 WARN MemoryStore: Not enough space to cache rdd_24_156 in memory! (computed 384.0 B so far)\n",
      "22/11/21 13:19:11 WARN BlockManager: Persisting block rdd_24_156 to disk instead.\n",
      "22/11/21 13:19:11 WARN MemoryStore: Not enough space to cache rdd_24_155 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:11 WARN BlockManager: Persisting block rdd_24_155 to disk instead.\n",
      "22/11/21 13:19:11 WARN MemoryStore: Not enough space to cache rdd_24_159 in memory! (computed 384.0 B so far)\n",
      "22/11/21 13:19:11 WARN BlockManager: Persisting block rdd_24_159 to disk instead.\n",
      "22/11/21 13:19:11 WARN MemoryStore: Not enough space to cache rdd_24_157 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:11 WARN BlockManager: Persisting block rdd_24_157 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=========================================>             (152 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 13:19:11 WARN MemoryStore: Not enough space to cache rdd_24_159 in memory! (computed 3.9 MiB so far)\n",
      "[78.796s][warning][gc,alloc] Executor task launch worker for task 160.0 in stage 9.0 (TID 4510): Retried waiting for GCLocker too often allocating 30642 words\n",
      "[78.798s][warning][gc,alloc] Executor task launch worker for task 158.0 in stage 9.0 (TID 4508): Retried waiting for GCLocker too often allocating 256 words\n",
      "[78.798s][warning][gc,alloc] Executor task launch worker for task 162.0 in stage 9.0 (TID 4512): Retried waiting for GCLocker too often allocating 16952 words\n",
      "[78.798s][warning][gc,alloc] Executor task launch worker for task 161.0 in stage 9.0 (TID 4511): Retried waiting for GCLocker too often allocating 256 words\n",
      "[78.812s][warning][gc,alloc] Executor task launch worker for task 162.0 in stage 9.0 (TID 4512): Retried waiting for GCLocker too often allocating 140002 words\n",
      "22/11/21 13:19:11 WARN BlockManager: Block rdd_24_162 could not be removed as it was not found on disk or in memory\n",
      "[78.833s][warning][gc,alloc] Executor task launch worker for task 164.0 in stage 9.0 (TID 4514): Retried waiting for GCLocker too often allocating 4098 words\n",
      "[78.833s][warning][gc,alloc] task-result-getter-3: Retried waiting for GCLocker too often allocating 256 words\n",
      "22/11/21 13:19:11 ERROR Executor: Exception in task 162.0 in stage 9.0 (TID 4512)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:161)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:72)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:92)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:92)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:104)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1339/0x000000080146ed10.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "22/11/21 13:19:11 WARN MemoryStore: Not enough space to cache rdd_24_161 in memory! (computed 3.9 MiB so far)\n",
      "22/11/21 13:19:11 WARN BlockManager: Persisting block rdd_24_161 to disk instead.\n",
      "22/11/21 13:19:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 162.0 in stage 9.0 (TID 4512),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:161)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:72)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:92)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:92)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:104)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1339/0x000000080146ed10.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute("
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Executor task launch worker for task 164.0 in stage 9.0 (TID 4514)\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.<init>(TaskMemoryManager.java:93)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:486)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "[Stage 9:===========================================>           (159 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "22/11/21 13:19:11 WARN TaskSetManager: Lost task 162.0 in stage 9.0 (TID 4512) (192.168.1.45 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:161)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:72)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:92)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:92)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:104)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1339/0x000000080146ed10.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\n",
      "22/11/21 13:19:11 ERROR TaskSetManager: Task 162 in stage 9.0 failed 1 times; aborting job\n",
      "22/11/21 13:19:11 WARN BlockManager: Putting block rdd_24_166 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/11/21 13:19:11 WARN BlockManager: Block rdd_24_166 could not be removed as it was not found on disk or in memory\n",
      "22/11/21 13:19:11 WARN TaskSetManager: Lost task 161.0 in stage 9.0 (TID 4511) (192.168.1.45 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/21 13:19:11 WARN TaskSetManager: Lost task 158.0 in stage 9.0 (TID 4508) (192.168.1.45 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/21 13:19:11 WARN TaskSetManager: Lost task 166.0 in stage 9.0 (TID 4516) (192.168.1.45 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/21 13:19:11 WARN TaskSetManager: Lost task 156.0 in stage 9.0 (TID 4506) (192.168.1.45 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/21 13:19:11 WARN TaskSetManager: Lost task 165.0 in stage 9.0 (TID 4515) (192.168.1.45 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/ipython/libexec/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/2p/5fk5g87n00g45wqtch4dnp5h0000gn/T/ipykernel_2215/1658280536.py\", line 1, in <module>\n",
      "    df_users1.show(5)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/dataframe.py\", line 606, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_users1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/opt/ipython/libexec/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2063\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2065\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages/ipykernel/zmqshell.py:538\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    532\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    533\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    535\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    539\u001b[0m }\n\u001b[1;32m    541\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "df_users1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092251ab-f4f9-42b7-bd84-a3702eff47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df_users1.toPandas()\n",
    "df_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c5f4-1772-4c71-970f-aa976b291b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_screen_name = df_json_sub.groupBy('screen_name')\\\n",
    "                .agg(countDistinct('created_at'))\\\n",
    "                .withColumnRenamed('count(created_at)', 'count_tweets')\\\n",
    "                .sort('count_tweets', descending = False)\\\n",
    "                .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a235e45-4074-401d-91a8-7b620702badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_screen_name.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebabb3a-59e4-4792-bc6a-42f00ce015b6",
   "metadata": {},
   "source": [
    "# Question 2: The number of likes for a brand post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd420e35-ba0a-4ce0-ab64-b8ec3fcdd88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14535a-fbdf-4105-b616-b0f8ac73ff35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a637c-c5b7-443c-be44-0c9cbcc7588f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a99097-37ec-49f2-9354-469703062b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "877b48d9-c5a7-4c82-9871-9ab8ad5fb360",
   "metadata": {},
   "source": [
    "# Question 3: The number and impact of influencers for each brand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c342e1d-0a50-44c5-b8ff-4cf05a774cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2901f-8abc-4218-9eb4-43a8279dee65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da8fd5-deba-434a-8566-8ee83dbc5548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a24f7-4b9d-4ffb-a95c-4521cf0d013d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e72d6-e97f-453d-ba03-5b6116a5f238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c354cab-9778-421b-bc1e-c0de99aefd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25227b96-d74b-4bdc-b5ed-42073fec63aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
